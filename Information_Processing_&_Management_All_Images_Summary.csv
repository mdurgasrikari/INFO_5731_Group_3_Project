index,Author Summary,Journal,Research Paper Name,Research Paper Link,Image Link,Image Caption,Diagram(Flowdiagram/ Table),Team Member
0,"Studies based on bibliometric analysis generally follow a common process (Börner et al., 2003, Cobo et al., 2011b), as shown in Fig. 1. The first step involves selecting a bibliographic database and defining a query to delimit the corpus to be analyzed. The data must be exported and added to existing bibliometric analysis tools (Moral-Muñoz et al., 2020), which is typically performed through the exporting capabilities of the databases’ websites (e.g., Web of Science, Scopus). Before analysis, the data must be preprocessed to avoid errors and normalize the unit of analysis through a de-duplication process (Cobo, López-Herrera, Herrera-Viedma, & Herrera, 2011a). There are three main types of analysis: (i) science mapping analysis, which is based on bibliographic networks (Batagelj & Cerinšek, 2013); (ii) bibliometric evaluation, which can be done at the article or journal level and based on citations or altmetrics; and (iii) other analyses such as economic, leadership, and collaboration. Finally, the results can be visualized using different options and interpreted by the analyst.As was mentioned, science of science analyses use to be more complex, facing and needing a large amount of data. For instance, if a researcher wants to analyze all the scientific output of Spain during the last decade, he/she will need to retrieve over 1 million documents (i.e., according to a query made on the Web of Science). Hence, manual access to that information through web forms and web pages is a tedious, complex, and low-performance process. In this sense, most bibliometric databases currently offer access to their information by using REST APIs (Torres-Salinas & Arroyo-Machado, 2022), which is a technique that allows us to automatically access and retrieve information between different machines. For instance, on Twitter, there are operations in their API that allow us to retrieve the tweets of a specific user or hashtag. Hence, as multiple APIs exist and they do not follow a common standard, there is a big variation in the features they offer to their users.",Information Processing & Management,New trends in bibliometric APIs: A comparative analysis,https://www.sciencedirect.com/science/article/pii/S030645732300122X,https://drive.google.com/file/d/1VuXbnOJISyoGFE8aHET0gae1voLwdmdT/view?usp=sharing,Fig. 1.Main process of bibliometric analysis.,Flowdiagram,Durga Srikari Maguluri
1,"In Fig. 3, the available extension options are shown. Thus, on the left side, there are the source APIs, so that using the identifier of the central part, one can extend the metadata using the target APIs on the right side of the figure. For instance, if we have a document of Scopus Papers, we can use the DOI to get the altmetrics of the document. Hence, in view of Fig. 3, the DOI, PubMedID, and the PMCID are the identifiers that allow most extensions in the APIs. Other identifiers cover a good number of APIs but are internal to some companies (i.e., WOSID for Clarivate-related APIs, EID, PII, PUI and LUI for Elsevier-related APIs). Regarding the APIs that are more extensible, we must highlight Semantic Scholar, Dimensions, ORCID and Embase.",Information Processing & Management,New trends in bibliometric APIs: A comparative analysis,https://www.sciencedirect.com/science/article/pii/S030645732300122X,https://drive.google.com/file/d/1sf1PXV5FpvL7a9OUT3RRE_77h1yAPTTe/view?usp=sharing,Fig. 3. Diagram of extensions through identifiers.,Diagram ,Durga Srikari Maguluri
2,"As shown in Fig. 1, we take a two-step approach, involving a crowdsourcing study and an analysis of search results. First, we aimed to collect, in a natural manner, queries that Google users would pose to search images relevant to the Covid-19 pandemic. Collecting image search queries through crowdsourcing allows us to exploit the wisdom of the crowd (Brabham, 2008, Ranard et al., 2014, Zook et al., 2010), having access to a diverse set of people with “web literacy”. In other words, we make no claim that our sample of workers from each country represents the general population of that country.The first step involved crowdsourcing a simulated search task (Borlund, 2000) to participants in four European locations (Great Britain (GB), Germany (DE), Spain (ES) and Italy (IT)). Queries were cleaned and aggregated, and a content analysis was performed, resulting in a taxonomy of themes characterising participants’ information needs. In the second step, we used the aggregated queries to study the similarity of results retrieved by Google. Images were collected, and then content analysed by an image tagging tool. Finally, we conducted three analyses to understand the similarities and differences in the “view” on the pandemic portrayed: (i) we compared the image overlap in terms of the URLs retrieved, (ii) we analysed the content of the images retrieved (i.e., the respective content tags), (iii) we performed a localisation analysis, to understand the proportion of images presented to a user, which are sourced from a domain located in the user’s respective country. We also investigate how aspects (such as the thematic category of query or query language) influence the similarity of results.",Information Processing & Management,Do you see what I see? Images of the COVID-19 pandemic through the lens of Google,https://www.sciencedirect.com/science/article/pii/S0306457321001424,https://drive.google.com/file/d/1dDh19kgXigFOErN4MNFXkV7DOQwqj_ZJ/view?usp=sharing,Fig. 1. Overview of the data collection and analyses in the two-step approach.,Flowdiagram,Durga Srikari Maguluri
3,"We first analyzed how similar our datasets are in terms of word composition. After pre-processing our texts, we construct a Bag-of-Words vector representation for each dataset. Then we perform pair-wise cosine similarity between each vector representation to provide an idea of how similar the text content are in each dataset pair. Fig. 1 represents a heatmap of similarities between datasets. The datasets in this study have an average similarity of 0.22 ± 0.20. Only one pair stands out: rumoreval2019 and phemerumors with a similarity value of 0.97. The reason for the extremely high dataset similarity between phemerumors and rumoreval2019 is because phemerumors is a dataset that combines two publicly available datasets, one of which is rumoreval2019. The dataset is constructed as such to encompass a larger set of newsworthy events that sparked rumors.With this backdrop of the relationships between datasets, this study investigates the generalizability of models built across both similar and non-similar datasets. The general dissimilarity of the datasets serves to mimic a general usage of stance classification models, where the model is built based on data from one dataset and applied to data collected from Twitter that may not be in the same domain",Information Processing & Management,Is my stance the same as your stance? A cross validation study of stance detection datasets,https://www.sciencedirect.com/science/article/pii/S0306457322001728,https://drive.google.com/file/d/1Ww4A9RYbwTE4n7TDDu0rj7UjsCswYNq2/view?usp=drive_link,Fig. 1. Similarity across dataset through bag-of-words representation,Diagram,Durga Srikari Maguluri
4,"Information anxiety, being a cluster of negative emotions, may result in information avoidance. Information avoidance is also known as non-seeking behavior. Over the last fifty years, information avoidance behavior has been primarily studied in the context of health information, as it tends to be conceptualized as a coping mechanism for dealing with potentially unwanted information (Manheim, 2014). It has already been proved that individuals ignore information and try to be selective in consulting information (Bawden, 2001; Sairanen & Savolainen, 2010). Golman et al., 2017) considered anxiety as one of the seven distinct psychological mechanisms that can produce information avoidance. Similarly, Swar et al. (2017) reported that psychological ill-being constructs (negative affect, depressive symptoms, and trait anger) created by online health-related information overload negatively impact individuals' online health information search behavior. Lately, Dai et al. (2020) reported that fatigue (tiredness, disappointment, loss of interest, or decreased need/ motivation) has a positive relationship with individuals' inactive social networking websites usage intention. Hence, literature provides evidence of an association between information anxiety and information avoidance behavior, and therefore, the following hypothesis is proposed in the context of COVID-19:",Information Processing & Management,From information seeking to information avoidance: Understanding the health information behavior during a global health crisis,https://www.sciencedirect.com/science/article/pii/S030645732030933X,https://drive.google.com/file/d/1qT4mcyKvdEhVd50og30kZTtwPrd6m8VN/view?usp=drive_link,Fig. 3. Proposed Research Model based on the S-O-R Framework.,Flow Diagram,Durga Srikari Maguluri
5,"Overall, our approach provides a comprehensive workflow which integrates crisis detection, time-based tweet selection, sentiment, and emotion MTL classification with attention mechanism, subject detection, and subject cluster assignments with intent prediction.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,https://www.sciencedirect.com/science/article/pii/S0306457324000554,https://drive.google.com/file/d/1K53xFNW4nb9zcns41iq89GP92FY2F2rH/view?usp=drive_link,Fig. 1. Workflow diagram of the proposed approach.,Flowchart,Durga Srikari Maguluri
6,"We observe that the model exhibits the capability to effectively capture sentiment and emotion related words. In the first example, the model assigns the highest attention weights to the words “stranded” and “lost” in sentiment classification and these words truly convey a negative sentiment. For emotion classification, the model relies heavily on the words “stranded” and “lost” to predict the emotion of “Fear”. These words are emotional indicators signifying the emotional response of fear. In the second example, the attention of the model is on “Haiyan”, “Typhoon”, “faces” when predicting “Neutral” and “No Emotion” class. These words do not carry any sentiment or emotional state but simply provide factual information about the typhoon. The model's selection of these words shows its ability to recognize the absence of sentiment and emotion content in the tweet. In the third example, the model identifies “respect”, “firemen”, “manage” in predicting “Positive” and “Love”. These words convey a sense of appreciation and acknowledgement which also suggests a “Positive” sentiment. This shows the ability of the model to detect positive and joyful emotions by recognizing words associated with admiration and appreciation.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,https://www.sciencedirect.com/science/article/pii/S0306457324000554,https://drive.google.com/file/d/1vpPr6w67B07k_1-4ZCuCJML6wOGDO9AU/view?usp=drive_link,Table 9. Visualization of attention words from BERTweet MTL model on testing split data.,Table,Durga Srikari Maguluri
7,"To reveal what thematic fields are highly commented on, we then conduct the MeSH terms analysis using the MeSH metadata of PubMed datasets. MeSH, developed and maintained by NLM, is a controlled and hierarchically compiled vocabulary used in MEDLINE/PubMed and other NLM databases for indexing, cataloging, and searching biomedical information (NLM, 2023). We establish a co-occurrence network with MeSH major topic terms as nodes and the co-occurrence relationships of every pair of major topic terms as edges, leading to an increment in the weight of the connecting edge between the respective nodes",Information Processing & Management,Scientific commentaries are dealing with uncertainty and complexity in science,https://www.sciencedirect.com/science/article/pii/S0306457324000670,https://drive.google.com/file/d/10Dl-krQH3J7y7f13THskYrqD18JUywhF/view?usp=drive_link,Workflow of constructing the co-occurrence graphs of MeSH main heading terms.,Flowchart,Durga Srikari Maguluri
8,"The review by Hertwig et al. (2019) concluded that what is called risk tolerance here was a moderately stable psychological trait with both general and domain-specific components when measured through self-reports but not behavioral tests. Sahm (2012) pointed to the relatively stable risk preference according to a panel of 12,003 individuals over a decade. More previous studies show that risk tolerance was a stable personality trait and was unlikely to change substantially over life (Van de Venter et al., 2012), which supported the theory of Nicoletta Marinelli and Palmucci (2017) that risk tolerance was a genetic, predispositional, and stable personality trait. To summarize, it is reasonable to model and predict risk tolerance at an individual level since it does not change drastically over time.",Information Processing & Management,Financial risk tolerance profiling from text,https://www.sciencedirect.com/science/article/pii/S0306457324000645,https://drive.google.com/file/d/1B8vdO7Skcn4gW4ShjCM0gQcUaN0X3oFk/view?usp=drive_link,Risk-related terminologies and their relations,Flowchart,Durga Srikari Maguluri
9,"A CNN model is built based on the architecture described by Majumder et al. (2017) and several useful model features are experimented with to test for their effectiveness. Fig. 2 illustrates the model architecture. In detail, the following features may improve the model performance according to the literature:1.Richness of representations: Using multiple text representations is a key factor that influences the model performance. Recent studies, e.g., Yang et al. (2023) have shown that psychologically inspired lexicons and middle layers from large language models provide additional useful information to the network input. The network input in Fig. 2 is a concatenation from sentence embeddings, including Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), and BERT (Devlin et al., 2019), to preserve semantic information as much as possible.2.
Text augmentation: This is often useful when the model training phase underfits or overfits because of limited data size. Yang et al. (2023) reported SPDFiT (Self-Taught Personality Detection Fine-Tuning), which uses Bayesian learning to assign possible pseudo labels for new texts. In this study, the textaugment Python library2 is used to substitute words and create semantic equivalents of existing texts. Synonymous substitution is a common method in NLP, which increases the amount of data in the dataset. The method is dedicated to providing more training data, thus improving the classification effect of short texts through global augmentation methods.Multi-task learning: Previous studies documented that personality detection may be learned with closely related tasks, such as internet use behaviors (Mark & Ganzach, 2014) and emotion detection (Li et al., 2022). The multi-task fashion is thus experimented, i.e., combines the 5 personality traits and risk tolerance as outputs for the same network, so that parameters can be shared between the two tasks. Cross entropy loss function is used, where personality traits remain in 2 categories (‘y’ and ‘n’), and risk tolerance was divided into 4 categories.For the BERT embeddings, “bert-base-uncased”3 with 10% dropout is used. Each contributing representation has an output dimension of 100 after batch normalization. These together with the Mairesse features form a final in-feature size of 3 × 100 84 384 for the fully connected layer (see Fig. 2). The representations are not frozen and will also be trained. Model parameters are empirically set: training batch size =16, and maximum epoch =4. A standard Adam optimizer (learning rate =0.001 and weight decay =0) from the PyTorch package is used.",Information Processing & Management,Financial risk tolerance profiling from text,https://www.sciencedirect.com/science/article/pii/S0306457324000645,https://drive.google.com/file/d/1B8vdO7Skcn4gW4ShjCM0gQcUaN0X3oFk/view?usp=drive_link,Risk-related terminologies and their relations.,Flowchart,Durga Srikari Maguluri
10,"Participants in the control group make decisions with AI Assistant I which provides only prediction results without explanation (denoted as AI, Group 3). Another group makes decisions with AI Assistant II (Group 1) which provides both prediction results and rational explanations generated by SHAP. The third group make decisions with AI Assistant III which provides prediction results and randomly generated explanations (denoted as FXAI, Group 2). Fig. 3–1 shows the framework of our study design and the overview of our experimental procedure.",Information Processing & Management,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-AI trust and decision performance,https://www.sciencedirect.com/science/article/pii/S030645732400092X,https://drive.google.com/file/d/1zcGp7iVPSFtNs41HTfx0gE3eoZJX3cTY/view?usp=drive_link,Fig. 3–1. The framework of our study design.,flowchart,Durga Srikari Maguluri
11,"For the first two groups, participants were assigned randomly to Group 1 and Group 2, as depicted in Table 3–1. To improve the internal validity of our experiment, a third group of participants were recruited. After the experiment, we screened data for survey questions asking about their trust and reliance on AI. Those with all answers the same were excluded. The final valid participants are 36 in Group 1, 33 in Group 2, and 40 in Group 3. Participants in all three groups are required to make decisions with AI Assistant I for the first ten products (Step 3 in Fig. 3–2), then AI Assistant II is provided to participants in Group 1 and help them make decisions on the other ten products (Step 4 in Fig. 3–2). Still, participants in Group 2 need to make decisions with the help of AI Assistant III for the last ten products. Group 3 is the control group with only AI for assistant, thus, participants in Group 3 are required to make all the 20 prediction tasks with Assistant I.",Information Processing & Management,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-AI trust and decision performance,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-AI trust and decision performance - ScienceDirect,https://drive.google.com/file/d/1UXungZ6VgBgoLWmwMpSG0v3fy0-aQ6Pl/view?usp=drive_link,Table 3–1. Method for hypotheses testing.,Table,Durga Srikari Maguluri
12,"To distinguish the data analysis capacity among participants, we conducted a post-experiment survey, and 7 “True or False” questions were asked based on their understanding of the statistical description of the sales from our experiment platform (refer to the screenshot in Fig. 3–2?), which depicted the relationships between sales performance and product attributes (refer to Appendix A for quiz details). All the questions are designed to test participants’ data analysis capacity. From the plots depicting the relationships between product attributes and sales performance, some rules were included. For instance, there are no clothes with sales of more than 10,000 pieces if the size is “3XXS” nor a price higher than 15$, as depicted in Fig. 3–4. Participants' data analysis capability can be differentiated based on whether they can identify the rules successfully. The reason for a post-experiment test rather than a pre-experiment test for the capability is to avoid the contextual cueing effect, i.e., reminding or guiding the participants to analyze data via the way included in the True/False quiz question. The participants were divided into two groups: the high task-related capacity with a True or False quiz score between 4 and 7 and the low task-related capacity with a score between 0 and 3.",Information Processing & Management,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-AI trust and decision performance,https://www.sciencedirect.com/science/article/pii/S030645732400092X,https://drive.google.com/file/d/1UXungZ6VgBgoLWmwMpSG0v3fy0-aQ6Pl/view?usp=drive_link,Fig. 3–4. : A screenshot of the sales plot.,Diagram,Durga Srikari Maguluri
13,"Before the data analysis for checking the effect of treatments, the difference in their trust in AI, reliance on AI, average time cost for the 1st ten trials, accuracy for the first ten trials, between Group 3 and Group 1, Group 3 and Group 1 before treatments were tested (Campbell & Stanley, 2015; Cook et al., 2002). No significant differences were found, and the details of the test are presented in Table 3–2.",Information Processing & Management,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-AI trust and decision performance,https://www.sciencedirect.com/science/article/pii/S030645732400092X,https://drive.google.com/file/d/1ferxa9PrgMIkTXycixue-kvRiFSE_79Y/view?usp=drive_link,"Table 3–2. Differences in trust, reliance, time, accuracy, and consistency for the 1st ten trials.",Table,Durga Srikari Maguluri
14,"As seen from Fig. 1, the proposed AGLSOFS framework is based on OR and utilizes adaptive graph learning to explore more geometric structural information about the data. In sparse OR, the regression target consists of two forms, one in the form of hard labels for known labels and the other in soft labels for unknown labels, since it is not guaranteed that the unlabeled samples belong to the class. Then, the label matrix is scaled by adding a scaling factor to the label space and by adjusting the scaling factor to learn a label matrix suitable for different data distributions. In dynamic graph learning, this study utilizes both the original high-dimensional and projection space information to learn a more accurate and reliable similarity graph. This is because if only the high-dimensional data is considered to construct the graph, it is more affected by noise. In contrast, only the low-dimensional data is considered to construct the graph. Unreliable results may be obtained due to the inaccuracy of the projection matrix . Meanwhile, two constraints, the Frobenius norm, and the maximum information entropy are employed to constrain the similarity graph. As a result, two specific methods were derived: (1) AGLSOFS_norm and (2) AGLSOFS_entropy. In the later experimental section, the effects of these two constraints on the construction of similarity graphs and the performance of FS will be compared. In addition, the out-of-sample problem (new sample classification) can also be solved by the learned optimal  since this model utilizes transformations rather than embeddings in inferring pseudo-label. Given that the proposed algorithm in this paper necessitates the construction of a similarity graph, its complexity escalates with the data size. Consequently, the proposed algorithm is not viable for large-scale datasets. The algorithm implicitly assumes a linear mapping relationship between the sample and label spaces. Hence, it is most suitable for non-large-scale datasets with such a linear mapping relationship",Information Processing & Management,Adaptive orthogonal semi-supervised feature selection with reliable label matrix learning,Adaptive orthogonal semi-supervised feature selection with reliable label matrix learning - ScienceDirect,https://drive.google.com/file/d/1306o-Ao1Zc9eVEDN1t25p6OytTsFVC_U/view?usp=drive_link,Fig. 1. The framework of the AGLSOFS.,Flow diagram ,Durga Srikari Maguluri
15,"is a matrix. , , , and  denote its th row, ()th element, the transpose, and trace, respectively.The Frobenius norm of  is defined as , and the -norm of is defined as table 2 shows the notations used in this paper.",Information Processing & Management,Adaptive orthogonal semi-supervised feature selection with reliable label matrix learning,https://www.sciencedirect.com/science/article/pii/S0306457324000876,https://drive.google.com/file/d/1YkTXX9ZpDD7nT9n7V5tL1ikaEKu71gyy/view?usp=drive_link,Table 2 shows the notations used in this paper,Table ,Durga Srikari Maguluri
16,"LSR is sensitive to outliers, which can significantly impair the performance of the model when there are outliers in the training samples. In contrast, OR may be more suitable for this case. When the projection matrix  satisfies orthogonality (), more statistical and structural information can be retained in the projected subspace, as shown in Fig. 2. It is beneficial for FS in many scenarios (Xu et al., 2023, Xu et al., 2022). Therefore, OR is introduced into the model of this paper, i.e., the orthogonal constraints are imposed on  based on LSR. In later sections, the superiority of OR will also be demonstrated experimentally.n supervised and semi-supervised FS, most regression-based methods use a label matrix, and the known label matrix is usually given in the form of one-hot encoding. There is no guarantee that the inferred labels will be accurate for unlabeled samples, so they are learned as soft labels, i.e., probabilistic labels, which infer the probability that the sample belongs to each class. It can also satisfy the normalization and non-negativity constraints, as in Chen et al., 2017, Chen et al., 2020. The objective function for unlabeled sample label inference is as follows:observing Eq. (1), it is necessary to have an accurate projection matrix to infer an accurate . Finding an accurate and reliable is the main purpose of FS.",Information Processing & Management,Adaptive orthogonal semi-supervised feature selection with reliable label matrix learning,https://www.sciencedirect.com/science/article/pii/S0306457324000876,https://drive.google.com/file/d/1l-vh9askIIS0DhPrBq53rYZ7z1vRZHnL/view?usp=drive_link,Fig. 2. Least square regression and orthogonal regression .,Figure,Durga Srikari Maguluri
17,"In the CAS, the general processing framework to eliminate inconsistencies is shown in Fig. 2. Firstly, context information is obtained by deploying multiple sensors. After processing, context information is converted into the same data type. Then, the system detects the context information from different sources to determine whether there are contradictions between them. If inconsistencies are found, an inconsistency elimination algorithm is used to process the context to obtain relatively accurate results. Based on these results, the CAS makes corresponding adjustments, such as adjusting the temperature, adjusting the lighting, etc., to provide the best service for users. Fig. 3 depicts the framework of the proposed MDCIE algorithm, which will be elaborated below.",Information Processing & Management,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory - ScienceDirect,https://drive.google.com/file/d/1TWnS3JLP6oRIOe_aY8rG-kmJeUn0InYJ/view?usp=drive_link,Fig. 3. The framework of the proposed MDCIE algorithm.,Flow diagram ,Durga Srikari Maguluri
18,"In the CAS, the general processing framework to eliminate inconsistencies is shown in Fig. 2. Firstly, context information is obtained by deploying multiple sensors. After processing, context information is converted into the same data type. Then, the system detects the context information from different sources to determine whether there are contradictions between them. If inconsistencies are found, an inconsistency elimination algorithm is used to process the context to obtain relatively accurate results. Based on these results, the CAS makes corresponding adjustments, such as adjusting the temperature, adjusting the lighting, etc., to provide the best service for users. Fig. 3 depicts the framework of the proposed MDCIE algorithm, which will be elaborated below.",Information Processing & Management,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory - ScienceDirect,https://drive.google.com/file/d/1_fDyzZI8P3259p88CktIjE_JAl5z2yoY/view?usp=drive_link,Fig. 2. The general processing framework to eliminate inconsistency.,Flow diagram ,Durga Srikari Maguluri
19,"Step A: Set the uncertainty interval of the propositionsAssume that all propositions in Step B: Calculate the uncertainty interval of inconsistent context informationare true, and their uncertainty intervals are defined asThe belief function and plausibility function can be estimated using Eq. (4) and Eq. (5) in accordance with the BPA of the context. Then, the uncertainty interval is expressed as where represents the belief function of evidence sources The BPA modified again by CoP can be denoted byto proposition A, and  represents the plausibility function of evidence sources to proposition A.Step D: Correct BPA
",Information Processing & Management,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory - ScienceDirect,https://drive.google.com/file/d/1v5TPVhG7kLOzO0iLHwJMhioK4IpIxOGR/view?usp=drive_link,Algorithm 2. The CoP indicator calculation algorithm.,Table ,Durga Srikari Maguluri
20,"Smart home is one of the main application scenarios for Internet of Things (IoT) technology (Khemakhem, Rekik & Krichen, 2022). As a key link in smart home scenarios, a smart lighting system's adaptability is closely related to user experience and energy consumption (Srihari, Sai & Srinivas, 2022). This paper studies the inconsistency caused by multiple illumination sources in intelligent lighting systems. Fig. 6 shows the processing flowchart from context awareness to system response in an intelligent lighting scenario. Light intensity varies in different areas. To provide users with the best light, multiple light intensity sensors need to be placed in different positions so that the system can timely adjust curtains, light intensity, color temperature, etc., according to the light intensity, which also leads to the inconsistency of light intensity context information. In general, the best indoor light intensity is 300 Lx - 500 Lx. When the light intensity is not within this range, the system adjusts the light intensity in real time by adjusting the curtain, lighting, etc.In the experimental environment of this paper, four illuminance sensors are placed in different positions to collect indoor light intensity information. When the light intensity is within the range of [300,500], the light intensity is normal, represented by “1”, and the system does not need to adjust. Otherwise, if the light intensity is abnormal, which is represented by “0”, the system needs to adjust. To ensure consistent experimental results, each experiment is conducted 1000 times and the final output result is determined by averaging all results.",Information Processing & Management,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory,Inconsistency elimination of multi-source information fusion in smart home using the Dempster-Shafer evidence theory - ScienceDirect,https://drive.google.com/file/d/1enmc6LU_OONElGSMfoxwfoXWhQB4llEp/view?usp=drive_link,Fig. 6. Flowchart of processing light intensity inconsistencies in intelligent lighting scenario.,Flow diagram ,Durga Srikari Maguluri
21,"or the convenience of description, the main notations in our scenario are listed in Table 1. We consider a scenario where an organization has a trusted self-recommendation server with the goal to improve the recommendation quality by exchanging information with other untrusted third parties. MF will decompose the User-Item interaction matrix  into user matrix  and item matrix , and the recommendation performance can be effectively improved by sharing  with other recommendation servers. However, due to the correlation between  and , directly sharing  may result in the leakage of user privacy. To prevent user privacy leakage, we focuses on MF based on DP, allowing recommendation systems to exchange useful information while protecting data privacy. The model we proposed can exchange information among multiple companies. The main working steps of ADPMF is illustrated in Fig. 1, and the corresponding descriptions are as follows.",Information Processing & Management,Matrix factorization recommender based on adaptive Gaussian differential privacy for implicit feedback,Matrix factorization recommender based on adaptive Gaussian differential privacy for implicit feedback - ScienceDirect,https://drive.google.com/file/d/1J7Pwqhhgy6zEEBlL0U9wx7NLFXIuxyli/view?usp=drive_link,Fig. 1. The framework of Bayesian personalized ranking matrix factorization achieving differential privacy.,Flowdiagram,Durga Srikari Maguluri
22,"According to Table 5, we can get that ADPMF, Clip-MFBPR, Sigma-MFBPR take more time than MFBPR to training model once. Although using optimization operations increases the running time of ADPMF, the increased computation time is not much and is acceptable in practical applications. Here, we take the model training in the dataset ML-100K as an example. Since 495,285 data are used in the process of training model once, for each data, the difference of training time between ADPMF and MFBPR is about 12.14 ms. Thus, the difference is very tiny. Moreover, in the experiments here, we only analyze the running time from the perspective of absolute calculation amount and do not consider the impact of optimization operations on model convergence. If the impact of optimization operations on the convergence speed is considered, the efficiency difference between ADPMF and MFBPR will be further reduced. Therefore, it can be concluded that our ADPMF has good efficiency.",Information Processing & Management,Matrix factorization recommender based on adaptive Gaussian differential privacy for implicit feedback,Matrix factorization recommender based on adaptive Gaussian differential privacy for implicit feedback - ScienceDirect,https://drive.google.com/file/d/124aPuNV1bmkRDlDSbcPCmA00bvWb6eRZ/view?usp=drive_link,Table 5. The running time of ADPMF and other models.,Table,Durga Srikari Maguluri
23,"n this paper, we develop an invertible super-resolution steganography network (ISSN), which achieves image hiding with lossless extraction of secret images. As shown in Fig. 1(b), different from existing INN-based approaches, the proposed ISSN generates the stego image without lost information. We feed a low-resolution (LR) cover image and three secret images into an invertible neural network to construct a high-resolution (HR) stego image whose feature dimensions match those of the input, and thus there is no lost information. Consequently, our method achieves lossless information hiding. The primary contributions of this paper are listed in the followingn this paper, we develop an invertible super-resolution steganography network (ISSN), which achieves image hiding with lossless extraction of secret images. As shown in Fig. 1(b), different from existing INN-based approaches, the proposed ISSN generates the stego image without lost information. We feed a low-resolution (LR) cover image and three secret images into an invertible neural network to construct a high-resolution (HR) stego image whose feature dimensions match those of the input, and thus there is no lost information. Consequently, our method achieves lossless information hiding. The primary contributions of this paper are listed in the following:
We propose an INN-based image steganography framework, which regards image steganography as an image super-resolution task. The proposed scheme effectively eliminates lost information of previous INN-based steganography schemes to achieve lossless information hiding. Besides, the generated high-resolution stego images have high visual quality and undetectability.
We construct a bijective secret projection module that transforms the secret image into a latent variable, improving the invisibility of the secret image and the visual quality of the stego image.Substantial experiments are conducted to demonstrate that the proposed method has superior imperceptibility, security, and reversibility. Benefiting from the bidirectional training strategy, our model achieves competitive performance with regard to the visual quality of stego images while ensuring better reversibility and security compared to existing methods",Information Processing & Management,Lossless image steganography: Regard steganography as super-resolution,Lossless image steganography: Regard steganography as super-resolution - ScienceDirect,https://drive.google.com/file/d/14LSvBq97RUbltdolhu0wN6Xx_RYgsHxq/view?usp=drive_link,Fig. 1. The difference between the classic INN-based steganography method and the proposed method.,Flowdiagram,Durga Srikari Maguluri
24,"We build up our ISSN based on a series of flow steps (Chávez, 2022). Fig. 3 shows the architecture of the invertible flow step. A flow step represents a sequence of an activation normalization layer (Kingma & Dhariwal, 2018) and an invertible block (Xiao et al., 2020). The input and output of each component in the flow step can be converted to each other, which ensures that the concealment and recovery of secret information are realized in the forward and inverse process, respectively. The invertible block is a combination of an affine coupling layer and an additive coupling layer. Specifically, the input of invertible block in the th flow step are divided into two branches:  and , the corresponding outputs are denoted as  and . The forward transformation of the th invertible block is formulated in Eq. (1).where 
 and represent Hadamard product and exponential operator, respectively. can be any transformations.In the inverse process, the direction of data flow is opposite to that of the forward pass. The formulation of the th invertible block of the inverse process can be easily derived from Eq. (1) as follows:",Information Processing & Management,Lossless image steganography: Regard steganography as super-resolution,Lossless image steganography: Regard steganography as super-resolution - ScienceDirect,https://drive.google.com/file/d/11k8mmheFylx3j4t9W8eVVdGoHYwXUKqj/view?usp=drive_link,Fig. 3. The flow step is a sequence of activation normalization and invertible block.,Flowdiagram,Durga Srikari Maguluri
25,"The SPM is proposed to encode the secret image into a simple form that follows a fixed distribution. As shown in Table 4, the SPM improves the visual quality of the stego image and reduces the accuracy of steganalyzer. In addition, Fig. 6 also demonstrates that the model with SPM has better hiding performance, where the error map is less prone to exposing hidden information. There are two reasons behind it. Firstly, SPM converts the secret image into Gaussian noise, thereby hiding the texture of the secret image. Secondly, as part of the input of SSN, the secret image encoded with Gaussian noise has a more general form, making it easier to train SSN to generate high-quality stego images. It is worth noting that the PSNR of secret images and revealed secret images is slightly higher in the model without SPM. This is due to the fact that removing the SPM reduces the size of the model and consequently reduces the error propagated by the floating-point calculation.",Information Processing & Management,Lossless image steganography: Regard steganography as super-resolution,Lossless image steganography: Regard steganography as super-resolution - ScienceDirect,https://drive.google.com/file/d/1DOnhJnzWgPQl_R2MmA7uleA4-M3d9dS3/view?usp=drive_link,"Table 4. Ablation study on secret projection module, bidirectional training.",Table,Durga Srikari Maguluri
26,"In recommendations, collaborative filtering (CF) is a widely used approach. To make the predictions, it was assumed that users with similar behaviors would have similar preferences. Matrix factorization (MF) learns the embeddings of users and items on user–item interactions (e.g., purchase, click, add to cart). However, these approaches consider only low-order interactions between users and items. Powered by graph neural networks (GNNs), researches such as GCMC (Berg, Kipf, & Welling, 2017), NGCF (Wang, He, Wang, Feng, & Chua, 2019), LightGCN (He et al., 2020), etc. focus on leveraging GNNs to model higher-order user–item interactions. The above works mainly considered the structural data of the user–item interactions. However, as shown in Fig. 1(a), attributes (e.g., user gender, age, product type, and brand) are essential side-information (Ni et al., 2023), which can alleviate the cold-start problem. It has already been proven that adding attribute information to recommendations can enhance performance (Li et al., 2021, Luo et al., 2022). One of the key issues is how to use user and item attributes to enhance the recommendation performance. Fi-GNN (Li, Cui, Wu, Zhang, & Wang, 2019) represents the attributes in a graph structure, nodes in a graph correspond to attributes and edges correspond to attribute interactions. For example, in movies recommendations, Paul is an action movie’s director. (Paul, action movies) can be regarded as an attribute interaction. Recently, several approaches (Su, Zhang, Erfani, Xu, 2021, Su, Zhang, Erfani, Gan, 2021) model the attribute interactions by graph, as shown in Fig. 1(b), and use graph neural networks to capture such pairwise interactions for recommendation. Nevertheless, the graph structure has limits on high-order relations modeling, as only pairwise connections can be represented in a graph. Hypergraph (Gao et al., 2020) is a generalization of a graph that has significant strengths in modeling the high-order relations that can be more sophisticated than pairwise relations, as shown in Fig. 1(c).",Information Processing & Management,Cross-view hypergraph contrastive learning for attribute-aware recommendation,Cross-view hypergraph contrastive learning for attribute-aware recommendation - ScienceDirect,https://drive.google.com/file/d/1KxpgeTLs3GuizVg1E3YqVFo5e3Yf0cRZ/view?usp=drive_link,Fig. 1. Illustration of the attribute-aware recommendation scenario and the differences between existing work and CHCLA.,Flowdiagram,Durga Srikari Maguluri
27,"CHCLA is primarily composed of four parts: local collaborative interaction learning, global hypergraph interaction learning, aggregation and prediction, and cross-view contrastive learning. Fig. 2 shows an overview of CHCLA. CHCLA takes full advantage of user–item interaction and attribute interactions to improve the recommendation performance. To achieve this, we leverage a graph neural network to obtain information about local pairwise interactions between user and item (Fig. 2(a)). Next, we utilize the hypergraph structure to model higher-order attribute interactions and comprehensively capture global attribute-level collaboration information (Fig. 2(b)). A fusion operation is then utilized on the output of user/item representations to make predictions (Fig. 2(c)). To address the issue of sparse supervised signal, we propose cross-view contrastive learning that generates self-augmented contrastive views from local and global learning (Fig. 2(d)).",Information Processing & Management,Cross-view hypergraph contrastive learning for attribute-aware recommendation,Cross-view hypergraph contrastive learning for attribute-aware recommendation - ScienceDirect,https://drive.google.com/file/d/14qGJ6lveBUPCwdEgf22pA-g_tOeLKNOc/view?usp=drive_link,Fig. 2. An Overview of CHCLA.,Flowdiagram,Durga Srikari Maguluri
28,"HIRS, which also considers hypergraph to model attribute information, outperforms GMCF on two datasets. This demonstrates that hypergraph structures are useful for modeling high-order interactions. The CHCLA distinguishes the attributes of users and items, and uses hypergraph to capture high-order attribute interactions. In addition, cross-view contrastive learning preserves both user–item interaction and attribute interaction from two levels. Hence, CHCLA captures more comprehensive information than the other methods.
Graph-based models outperform traditional CF models on most datasets, suggesting the effectiveness of modeling pairwise relations using a graph architecture for recommendation. However, hypergraph-based approaches outperform graph-based methods, demonstrating that flexibility and effectiveness of modeling high-order interactions in a hypergraph architecture.",Information Processing & Management,Cross-view hypergraph contrastive learning for attribute-aware recommendation,Cross-view hypergraph contrastive learning for attribute-aware recommendation - ScienceDirect,https://drive.google.com/file/d/1GyQkWodnq5iWm_rKrZYJWotgwdv-cRDj/view?usp=drive_link,Table 2. Overall performance of CHCLA with the baselines.,Table,Durga Srikari Maguluri
29,"Crisis tweets are filtered and analyzed together at a predefined time interval, e.g., every 15 min. Each time interval is a point (or bin) in the sentiment/emotion analysis vs time. Therefore, the sentiments/emotions carried by the crisis tweets in a window represent the overall sentiments/emotions in that respective window.We employ a MTL model with attention mechanism for sentiment and emotion analysis. By using attention mechanism built within transformers, we can extract words with highest attention (weights) and use these attention words to provide insights during prediction. The outputs of the MTL model include sentiment and emotion classifications with their respective attention words.Subject detection extracts key subject entities (e.g., police, victims) from the crisis tweets. We then group similar subjects using cosine similarity and assign them to specific clusters. To leverage the COMET-ATOMIC 2020 model, crisis related tweets are pre-processed as required by the model to generate the intent associated with the subject entity. The resulting outputs include subject entities, intent, and the cluster numbers to which the subjects are assigned.Overall, our approach provides a comprehensive workflow which integrates crisis detection, time-based tweet selection, sentiment, and emotion MTL classification with attention mechanism, subject detection, and subject cluster assignments with intent prediction.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction - ScienceDirect,https://drive.google.com/file/d/1QoKkPuIbITy2bOszHakOgyi0ApWAy7zE/view?usp=drive_link,Fig. 1. Workflow diagram of the proposed approach,Flowdiagram,Durga Srikari Maguluri
30,"We adopt the Shaver Emotion Model (Table 1) which is based on six emotions such as fear, anger, sadness, surprise, love and joy (Shaver et al., 1987). Additionally, we add “No Emotion” category for tweets which contain factual information without emotional or evaluative content. As explained above, “he went to the mall yesterday” is annotated as “Neutral” and “No Emotion”.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction - ScienceDirect,https://drive.google.com/file/d/1FiCh_Y6dSWJAIleDfaO76UmA5uaFYwUm/view?usp=drive_link,Table 1. Emotion annotation rule.,Table,Durga Srikari Maguluri
31,"Since BERTweet achieves the best Macro-F1 score for both sentiment and emotion classification and computationally more efficient, we incorporate BERTweet as the final model in our subsequent study.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction - ScienceDirect,https://drive.google.com/file/d/1Dc9U1F_qCAOh2F9p-vCCokWbbKzT31F_/view?usp=drive_link,Table 8. Comparison of training and evaluation time for MTL models.,Table,Durga Srikari Maguluri
32,"We observe that the model exhibits the capability to effectively capture sentiment and emotion related words. In the first example, the model assigns the highest attention weights to the words “stranded” and “lost” in sentiment classification and these words truly convey a negative sentiment. For emotion classification, the model relies heavily on the words “stranded” and “lost” to predict the emotion of “Fear”. These words are emotional indicators signifying the emotional response of fear. In the second example, the attention of the model is on “Haiyan”, “Typhoon”, “faces” when predicting “Neutral” and “No Emotion” class. These words do not carry any sentiment or emotional state but simply provide factual information about the typhoon. The model's selection of these words shows its ability to recognize the absence of sentiment and emotion content in the tweet. In the third example, the model identifies “respect”, “firemen”, “manage” in predicting “Positive” and “Love”. These words convey a sense of appreciation and acknowledgement which also suggests a “Positive” sentiment. This shows the ability of the model to detect positive and joyful emotions by recognizing words associated with admiration and appreciation.",Information Processing & Management,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction,Unveiling the dynamics of crisis events: Sentiment and emotion analysis via multi-task learning with attention mechanism and subject-based intent prediction - ScienceDirect,https://drive.google.com/file/d/10dAUmE3I39YPhQ6awltcOYk253SNYq_A/view?usp=drive_link,Table 9. Visualization of attention words from BERTweet MTL model on testing split data.,Table,Durga Srikari Maguluri
33,"However, finding a pruned model that performs as well as the original one is not trivial. Previous studies pointed out that several sub-networks may have the same or better predictive performance as the complete model (Frankle & Carbin, 2019). Specifically, BERTweet, the base architecture we select to prune, comprises 144 heads distributed in 12 layers. This way, to avoid the combinatorial explosion of testing each one of the vast numbers of models that could be generated when pruning subsets of BERTweet heads, the proposed method establishes an order of importance for the heads and prunes them incrementally in the opposite order. In this context, the method works in two sequential steps. First, it defines a hierarchy of importance for all heads (called here as Step 1). Then, considering this hierarchy, it identifies the best-pruned model architecture to be finetuned among all pruned models generated during the pruning process (called here as Step 2). Fig. 1 illustrates the top-level procedure to execute the proposed method. Sections 3.1 Step 1: defining an importance order for the heads, 3.2 Step 2: Selecting the best-ranked pruned model provide detailed explanations of these steps.For robustness and to avoid bias, it is crucial to use a different and unique collection of datasets in each of these steps. In Fig. 1, we underscore this principle by utilizing two distinct labels (datasets_X and datasets_Y) to represent the two datasets groups associated with Steps 1 and 2, respectively. Moreover, previous literature observed that the finetuning procedure could produce substantially different results due to some random internal procedure (Dodge et al., 2020). This way, to mitigate the impact it could make on the results achieved by a unique execution, each finetuning procedure executed in the proposed method (Step 1 and Step 2) must be performed a reasonable amount of times using different seeds. Consequently, the model evaluation must be computed according to the mean results achieved by all the executions. We incorporate those experimental decisions into the proposed method to ensure the intended robustness.Although this study focuses on the BERTweet model and the sentiment classification task, the proposed method is general enough to be applied to other Transformer-based models and tasks.",Information Processing & Management,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis - ScienceDirect,https://drive.google.com/file/d/18D272t2iYxuI3PCEXH1ZK3cIeTZW98Jj/view?usp=drive_link,Fig. 1. Overall steps that compose the proposed pruning method.,Flowdiagram,Durga Srikari Maguluri
34,"We use a large set of twenty-two datasets of tweets that have been extensively used in the Twitter sentiment analysis literature (Barreto et al., 2023, Carvalho and Plastino, 2021). These datasets encompass a diverse range of domains, providing a comprehensive scenario in evaluating the performance of the pruned model architectures in the sentiment classification task. For example, generic sentiment datasets such as aisopos, sentiment140, Narr, STS-gold, SentiStrength, Vader, SemEval13, SemEval16, SemEval17, and SemEval18 provide a broad foundation for understating sentiments across various topics. On the other hand, the domain-specific datasets iphone6, sanders, and archeage capture sentiments related to consumer products, technology, and gaming, respectively. Similarly, while datasets movie and hobbit offer insights into audience reactions, datasets person and Target-dependent explore sentiments directed towards individuals, including celebrities. In the political domain, datasets OMD and HCR explore sentiments about political figures and issues. Lastly, although datasets irony, sarcasm, and SemEval-Fig contain tweets with linguistic nuances associated with irony, sarcasm, and metaphor language, they were manually labeled with the explicit intention of detecting the expression of sentiments in the context of these linguistic complexities.",Information Processing & Management,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis - ScienceDirect,https://drive.google.com/file/d/1UdXqtCgZlISmVL39BhedTj6bd39jMG4Q/view?usp=drive_link,Fig. 2. Methodology adopted to evaluate the proposed pruning method.,flowdiagram,Durga Srikari Maguluri
35,"As can be observed in Table 9, mamh3 yielded the best model in six datasets, whereas cm won in the other one. When comparing moih3 to cm exclusively, the former won in four datasets, but with statistical significance only in s18. The latter won in the other three with statistical significance in s17. Moreover, when comparing mamh3 to cm, we realize that mamh3 won in six datasets with statistical significance in s18 and per datasets, whereas cm won in one. Further, when comparing moih3 to mamh3, the latter overcomes the former in all datasets but without statistical significance.",Information Processing & Management,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis,Less is more: Pruning BERTweet architecture in Twitter sentiment analysis - ScienceDirect,https://drive.google.com/file/d/1kTvyEVpHYt5GTASHFSnA9J3FLL05m9Qp/view?usp=drive_link,"Table 9. Comparison among the results achieved by mamh3, moih3, and cm over the Step 2 datasets (g2).",Table,Durga Srikari Maguluri
36,"In our experiments we found that the network with satisfactory performance and stable training consists of a single hidden layer with 50 neurons, each using ReLU activation function. To minimize the aforementioned loss function, the Adam optimizer is used, with the initial learning rate set to 0.001. All models are trained with batch size 4 and 100 or 200 epochs. We attempted to combine other activation functions for hidden neurons and learning parameters in the multilayer perceptron (MLP), but the statistical performance was either similar or worse than the previous results. Parameters and training process for each model are provided in Table 5. The available dataset was split into five folds by using data stratification technique (Botev et Ridder, 2017). During cross-validation, the folds are constructed in a way that maintains the proportion of samples for each class. The main objective of this process is to assess the model's capacity to make predictions for novel data that were not used to train it, thereby providing insight into the model's generalization ability and potential overfitting issues. In the results section, we present the outcomes obtained from five independent cross-validation experiments, where the performance measures of accuracy, precision, recall, and AUC are summarized.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1AcY2WBfllHKfnUtJelhPUOPkgpCXjiMu/view?usp=drive_link,Table 5. Parameters and training process for each model.,Table,Durga Srikari Maguluri
37,"In regard to the RQ2, we have found out the mechanisms that are key drivers of knowledge collection and transformation, as presented in Table 9.NGOs collect the missing knowledge most of the time through trainings, conferences, and consultations with experts and partners. These are already well known and quite traditional channels which through organizations try to learn, exchange and transfer. What does seem worrying is the very small percentage of NGOs who recognize consultations with beneficiaries as an important mechanism for learning about their needs to which NGO projects should provide meaningful solutions. This fact may explain the palliative rather than transformative effects of NGO work, the reason why developmental issues still remain wicked with complex contexts hardly influenced by NGOs as they rather opt to work for their beneficiaries instead with them. Also, mentoring and internships or job shadowing still have not been recognized as one of the most useful learning methods, massively advocated by the professional and scientific community in recent years. Learning by doing or on job learning and intensive mentoring and coaching from more experienced professionals are the golden standard for most of the top world companies today. Although NGOs could claim lack of financial resources as a key limitation factor for those incentives, it is still something reachable because the project ecosystems in which NGOs operate can provide that resource. NGOs do have plenty of opportunities in that regard, it is just the matter of recognizing it is needed first of all.NGOs do transform tacit knowledge into explicit and in most of the cases they use manuals, minutes and their databases as key repositories for storage of written data, information, knowledge about certain processes, systems, rules, etc. Still, our analysis suggests that only small number of NGOs actually do that, which means that the majority of experiences, observations, experiments, practical knowledge and skills remain with individuals instead of being transferred and integrated into the organizational systems, processes and repositories. Thus, institutional memory of the organization remains cut from a chance to generate specific knowledge in the form of its intellectual capital, creating an added value to the organizational performance and impact.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1qRCFtQaP_maRgb_bYyiHJew6vS_MTQbp/view?usp=drive_link,Table 9. The most common used mechanisms for knowledge acquisition.,Table,Durga Srikari Maguluri
38,"To identify the key drivers of knowledge acquisition in NGOs, a series of statistical analyses were conducted. The first step was to check the internal consistency of all the social capital (SC) and knowledge acquisition (KA) variables examined. For external social capital, a scale of 21 questions demonstrated a satisfactory level of internal consistency with a Cronbach alpha of ?=0.815, split-half (Spearman-Brown coefficient) reliability at 0.816, and an average item correlation with the overall score of r = 0.58. For internal social capital, a 23-question scale also demonstrated a satisfactory level of internal consistency with a Cronbach alpha of ?=0.925, split-half (Spearman-Brown coefficient) reliability at 0.883, and an average correlation of items with the overall score of r = 0.59. The knowledge acquisition scale consisted of 4 questions (two quantitative and two qualitative) and showed a high level of internal consistency with a Cronbach alpha of ?=0.916, split-half (Spearman-Brown coefficient) reliability at 0.842, and an average correlation of items with the overall score of r = 0.61. The surveyed NGOs evaluated their external, internal social capital, and knowledge acquisition, as specified in Table 2, Table 3, Table 4.The skewness values indicate that some of the variables are not normally distributed, with negative values indicating a longer left tail and positive values indicating a longer right tail. For example, the variables related to network openness (S1) and network closeness (S2) have negative skewness values, suggesting that there may be a larger proportion of individuals with fewer ties in the network. In contrast, the variable related to power through influence (N2c) has a negative skewness value, indicating that there may be a larger proportion of individuals with less influence. The kurtosis values indicate whether the distribution is more or less peaked than a normal distribution. For example, the variable related to network openness (S1) has a kurtosis value of 2.452, indicating a very peaked distribution. In contrast, the variable related to network position (S4) has a kurtosis value of ?0.418, indicating a relatively flat distribution. Similar interpretation could be done for other two tables. The dataset is described in more detail in our previous papers (Mikovic et al., 2020, Mikovic et al. 2019a, Mikovic et al., 2019b).",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1TRgLwWIv-4LHlj-f7Y3Jcv99_YzpKBGV/view?usp=drive_link,Table 4. Knowledge acquisition of the organization – descriptive data for key KA features.,Table,Durga Srikari Maguluri
39,"In our experiments we found that the network with satisfactory performance and stable training consists of a single hidden layer with 50 neurons, each using ReLU activation function. To minimize the aforementioned loss function, the Adam optimizer is used, with the initial learning rate set to 0.001. All models are trained with batch size 4 and 100 or 200 epochs. We attempted to combine other activation functions for hidden neurons and learning parameters in the multilayer perceptron (MLP), but the statistical performance was either similar or worse than the previous results. Parameters and training process for each model are provided in Table 5. The available dataset was split into five folds by using data stratification technique (Botev et Ridder, 2017). During cross-validation, the folds are constructed in a way that maintains the proportion of samples for each class. The main objective of this process is to assess the model's capacity to make predictions for novel data that were not used to train it, thereby providing insight into the model's generalization ability and potential overfitting issues. In the results section, we present the outcomes obtained from five independent cross-validation experiments, where the performance measures of accuracy, precision, recall, and AUC are summarized.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/15pPhw6wWa2pan1Qr7fZhTUL0CAHL95Dn/view?usp=drive_link,Table 5. Parameters and training process for each model.,Table,Durga Srikari Maguluri
40,"Here, we focused our attention primarily on the KA3 model, a more complex neural network architecture we used, compared to KA1 and KA1 + KA3 models which are simple. The reason for this selective presentation is twofold: First, KA3 is a multiplex process and far more challenging to manage than KA1. KA3 model's complexity allows it to capture intricate data relationships that simpler models struggle to discern, making it the most promising candidate for our research objectives. Second, presenting detailed results for all models could potentially inundate the paper, compromising readability and clarity. It is essential to note that the results and conclusions obtained for the KA3 model closely align with those of other models explored in our study. The results obtained using the SHAP approach and a comparison with the results obtained using DeepLIFT are presented below",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1NpILeSc7LakI5lKQqYYT286GpJpSK8HP/view?usp=drive_link,Table 8. The most and least influences of social capital on low and high level of knowledge acquisition – detailed presentation of results.,Table,Durga Srikari Maguluri
41,"As presented in Fig. 1, the social capital embedded in project ecosystems of nongovernmental organizations is one of the relevant factors in building a successful model for effective and efficient predicting of the level of knowledge acquisition. The main objective of our study is to forecast and comprehend the relationship between social capital and the knowledge gained in nonprofit organizations, utilizing advancements in the explainability of machine learning models (Angelov et al., 2021; Linardatos et al., 2021). To achieve this, we have formulated two distinct research questions:1.To evaluate which factors of social capital affect knowledge acquisition, distinguishing between high and low levels of both positive and negative impacts on organizations that function with varying degrees of knowledge acquisition (Research Question 1). It is vital for resource-constrained organizations to enhance the knowledge acquisition process through social capital embedded within their project ecosystems. NGOs, inherently limited in resources, are constantly seeking innovative solutions but often face shortages in finances, time, personnel, and know-how to support their development efforts and bridge existing gaps. Consequently, they must depend on their connections, networks, and individuals both within and outside their organizational boundaries.2.To pinpoint the key mechanisms driving the collection and transformation of knowledge (Research Question 2). NGOs take pride in their diverse partnerships and recognize their varied benefits. However, they often struggle with effectively leveraging the social capital accessible to them for the purpose of gathering necessary knowledge and converting it into impactful outcomes, such as innovative solutions for those in need. This transformation of knowledge into practical impact is the ultimate aim NGOs strive for through their international development projects. Therefore, it is crucial to illuminate the dynamics of power and cooperation within these organizations.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1boxE-vTY2XIwNUEiXtUpgTDxlGTksawp/view?usp=drive_link,"Fig. 1. Conceptual framework of the research. Surveys and interviews are conducted with an NGO, and information collected is categorized into informative social capital variables that are used by the proposed system to both estimate the level of knowledge acquisition and to extract highlights and lowlights of measured data thus providing the NGO the necessary insights into their knowledge acquisition performance and directions to improve it.",Flowdiagram,Durga Srikari Maguluri
42,"In our experiments we found that the network with satisfactory performance and stable training consists of a single hidden layer with 50 neurons, each using ReLU activation function. To minimize the aforementioned loss function, the Adam optimizer is used, with the initial learning rate set to 0.001. All models are trained with batch size 4 and 100 or 200 epochs. We attempted to combine other activation functions for hidden neurons and learning parameters in the multilayer perceptron (MLP), but the statistical performance was either similar or worse than the previous results. Parameters and training process for each model are provided in Table 5. The available dataset was split into five folds by using data stratification technique (Botev et Ridder, 2017). During cross-validation, the folds are constructed in a way that maintains the proportion of samples for each class. The main objective of this process is to assess the model's capacity to make predictions for novel data that were not used to train it, thereby providing insight into the model's generalization ability and potential overfitting issues. In the results section, we present the outcomes obtained from five independent cross-validation experiments, where the performance measures of accuracy, precision, recall, and AUC are summarized.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1iHy1XI1NzqBRqScs988UyHr8bfouTiPp/view?usp=drive_link,Table 5. Parameters and training process for each model.,Table,Durga Srikari Maguluri
43,"Table 8 is a narrative interpretation of data, scores and features codified in Table 7, created primarily for the audience less acquainted with ANN and DeepLIFT methodology, with aim to ease their understanding of the results obtained. To avoid confusion with the combination of, for example, positive class and positive features, in some places the negative class is called ZERO, and the positive class is called ONE.In addition to our investigation involving DeepLIFT, we also incorporated the SHAP (Shapley Additive Explanations) (Lundberg et Lee, 2017; Lundberg et al., 2018) model into our analytical framework. The inclusion of SHAP was aimed at enhancing our understanding of feature importance and model interpretability and to compare the results obtained using both methods. SHAP provides valuable insights into the contributions of individual features towards model predictions, shedding light on the black-box nature of complex neural networks. The SHAP methodology was chosen, because in comparing the DeeLIFT and SHAP methodologies, there are notable differences in their respective approaches to feature attribution and interpretability. While DeepLIFT focuses on attributing contributions to input features for a specific prediction, SHAP employs a more holistic approach by considering feature contributions across all possible permutations of feature values. This distinction in methodology led to nuanced disparities in our interpretation of feature importance and model behavior, which was the goal, i.e. to check if there is and how much overlap there is in the results obtained.Here, we focused our attention primarily on the KA3 model, a more complex neural network architecture we used, compared to KA1 and KA1 + KA3 models which are simple. The reason for this selective presentation is twofold: First, KA3 is a multiplex process and far more challenging to manage than KA1. KA3 model's complexity allows it to capture intricate data relationships that simpler models struggle to discern, making it the most promising candidate for our research objectives. Second, presenting detailed results for all models could potentially inundate the paper, compromising readability and clarity. It is essential to note that the results and conclusions obtained for the KA3 model closely align with those of other models explored in our study. The results obtained using the SHAP approach and a comparison with the results obtained using DeepLIFT are presented below.Our findings reveal that both DeepLIFT and SHAP offer valuable insights into model interpretability, each with its unique strengths and considerations. The choice between these methods may depend on the specific research question, the level of interpretability required, and the nature of the dataset. What we got is that although the approaches were completely different, the results and the selected features and their importance in the neural network matched to a large extent.",Information Processing & Management,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects,Importance of social capital for knowledge acquisition– DeepLIFT learning from international development projects - ScienceDirect,https://drive.google.com/file/d/1LJ8g9l3fXxfCAB0vR4Yu4eYrGgARmdhL/view?usp=drive_link,Table 8. The most and least influences of social capital on low and high level of knowledge acquisition – detailed presentation of results.,Table,Durga Srikari Maguluri
44,"To reveal what thematic fields are highly commented on, we then conduct the MeSH terms analysis using the MeSH metadata of PubMed datasets. MeSH, developed and maintained by NLM, is a controlled and hierarchically compiled vocabulary used in MEDLINE/PubMed and other NLM databases for indexing, cataloging, and searching biomedical information (NLM, 2023). We establish a co-occurrence network with MeSH major topic terms as nodes and the co-occurrence relationships of every pair of major topic terms as edges, leading to an increment in the weight of the connecting edge between the respective nodes (Fig. 2).Next, we apply the Louvain algorithm (Blondel et al., 2008) to partition the network into communities. The core idea of this algorithm is to assign nodes in the network to different communities in order to maximize the connectivity within communities and minimize the connectivity between communities. The Louvain algorithm uses a modularity-based optimization criterion that measures the relative density of connections within the network and between communities. The algorithm consists of two main steps. In the first step, each node is initially assigned to its own community, and then, for each node, it attempts to find the maximum positive modularity gain by moving the node to each of its neighboring communities. If no positive gain is achieved, the node remains in its original community. The modularity gain obtained by moving isolated node i to community C is calculated as follows (Blondel et al., 2008) (Supplementary Note Part 5.3):",Information Processing & Management,Scientific commentaries are dealing with uncertainty and complexity in science,Scientific commentaries are dealing with uncertainty and complexity in science - ScienceDirect,https://drive.google.com/file/d/1oH_IkioK0Afqh2rJl9uKhC6Z1hKWLf21/view?usp=drive_link,Fig. 2. Workflow of constructing the co-occurrence graphs of MeSH main heading terms.,Flow Diagram,Durga Srikari Maguluri
45,"On the basis of existing literature and theory, we develop our research hypotheses and model. We first assume the main effect of performance-contingent incentivized reviews on subsequent supplementary reviews. Then, since the contribution motivations of reviewers are contingent on the product-related contextual factors, we make two hypotheses on moderating effects by including two contingent factors (i.e., product type and product description type). Fig. 1 shows the relationships between the proposed hypotheses.",Information Processing & Management,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews - ScienceDirect,https://drive.google.com/file/d/1KwjS31VG3Z93UJRDGBZcM-scoPrgefa1/view?usp=drive_link,Fig. 1. The research framework,flow Diagram,Durga Srikari Maguluri
46,"138 participants (Mage = 27.03; 72.5 % female) were recruited for the main experiment. Each participant was randomly invited to the experimental group or the controlled group. Participants were asked to imagine that they were present in an online shopping situation that sold a digital camera with consumer reviews (The experimental materials were screenshots from a flagship store on JD.com selling Canon cameras). After they viewed the screenshots with the digital camera reviews, we asked them to complete a questionnaire measuring the reciprocity, self-efficacy, and contribution to supplementary reviews (1= strongly disagree; 7= strongly agree), see detailed measure scales in Table D1.",Information Processing & Management,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews - ScienceDirect,https://drive.google.com/file/d/1OE5ssTS7Q4m2RBJlVKXY7cid10qGxl2E/view?usp=drive_link,Table D1. The measurement scales.,Table,Durga Srikari Maguluri
47,"The measurement model was evaluated by the indicators of reliability, convergent validity, and discriminant validity of the latent variables. As shown in Table D2, the reliability of constructs was good because all Cronbach's alpha (?) and composite reliability (CR) values were up to the criteria of 0.7 (Nunnally, 1994). Convergent validity was supported for all the average variance extracted (AVE) and factor loadings exceeded the recommended value of 0.5 (Fornell & Larcker, 1981).",Information Processing & Management,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews - ScienceDirect,https://drive.google.com/file/d/1vxZ81y0xhCxI0QMGWPuurMcJIuyKh5Gg/view?usp=drive_link,Table D2. Convergent validity and reliability statistics.,Table,Durga Srikari Maguluri
48,"We further conducted a mediation test to check the mediation effects of self-efficacy on the relationship between PIRs and participants’ contribution to supplementary reviews by using the bootstrapping method. As shown in Table D3, the effects of PIRs on participants’ contribution to supplementary reviews were mediated by their self-efficacy (?PIR?SE?SR = 0.128, Confidence Interval at 95 % is [0.048, 0.206]). These results are consistent with the conclusions in the observational study and provide a theoretical interpretation for the PIRs-to-SRs relationship that could be partially attributed to self-efficacy, in support of H1.",Information Processing & Management,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews,Unveiling the impacts of performance-contingent incentivized reviews on subsequent supplementary reviews - ScienceDirect,https://drive.google.com/file/d/1tv4qn--5t4hEhI3_szqX1JTPc57jx5z2/view?usp=drive_link,Table D3. Mediation analysis results.,Table,Durga Srikari Maguluri
49,"Step 5: Conduct a comparative analysis and robustness test. To verify the effectiveness of the proposed model, we introduced several feature selection methods (LASSO, RFE, MIC) and used the selected feature sets as input to the benchmark models (CNN-LSTM, BiLSTM, LSTM, CNN, ANN, and SARIMAX). In addition, prediction experiments on the tourist flow of adjacent tourist attractions were used to verify the robustness of the forecasting framework.",Information Processing & Management,Forecasting tourism demand with search engine data: A hybrid CNN-BiLSTM model based on Boruta feature selection,https://www.sciencedirect.com/science/article/pii/S0306457324000591,https://drive.google.com/file/d/1pTjwDr9IEhDLU_jjQkiKquvtbXCnucQ5/view?usp=drive_link,Fig. 1. Research framework of tourist flow forecasting.,Flow Diagram,Durga Srikari Maguluri
50,"A Baidu Index data crawler was designed using Python 3.6 to obtain the daily SED of all keywords in the lexicon from January 1, 2011 to May 31, 2021. As the tourist flow of Hongcun Village is described by monthly data, the daily SED collected are aggregated into monthly data to maintain the consistency of the data. The descriptive statistics of tourist flow and search volume are summarized in Table 1.",Information Processing & Management,Forecasting tourism demand with search engine data: A hybrid CNN-BiLSTM model based on Boruta feature selection,https://www.sciencedirect.com/science/article/pii/S0306457324000591,https://drive.google.com/file/d/1D0Q84Pp1QC-Wg_zpSr8RsNzDisf5dJPy/view?usp=drive_link,Table 1. Descriptive statistics.,Table,Durga Srikari Maguluri
51,"During the training process, several hyperparameters were used to adjust the convergence effect. The epoch was set to 150, indicating that the model was trained 150 times. The loss function is used to monitor the convergence of the model, and popular options include mean absolute error (MAE), mean squared error (MSE), and Huber loss. The training gradients of MAE are usually equal, which is not conducive to functional convergence and model learning. Although Huber loss is robust and converges faster than MAE, its effect depends on additional hyperparameters, which requires more resources for training and debugging. MSE is a common loss function in deep learning that has the advantages of a dynamic training gradient and fast convergence. Therefore, MAE was chosen as the loss function in this study. Adaptive moment estimation (Adam) is an optimizer that updates the weights of the network. The learning rate for Adam is 0.001. To improve memory utilization, we set the number of samples (i.e., batch size) used for each update of the weights to 10. Table 4 lists the specific hyperparameters.",Information Processing & Management,Forecasting tourism demand with search engine data: A hybrid CNN-BiLSTM model based on Boruta feature selection,https://www.sciencedirect.com/science/article/pii/S0306457324000591,https://drive.google.com/file/d/1stciUXKKi-Qmd114zb2vV_gve8RTfKRJ/view?usp=drive_link,Table 4. The hyperparameter settings of CNN-BiLSTM.,Table,Durga Srikari Maguluri
52,"The statistics for NED dataset are shown in Table 1. Specifically, NED includes 17,366 samples from 2011 to 2022, annotated with 40 real-world news events. These events cover a broad range of topics, including political events, sports events, natural disasters, social and cultural events, and violent and terror events. To enhance the dataset’s complexity and utility, each thematic category includes numerous closely related events, thereby increasing the detection task’s difficulty, e.g., “Super Bowl LI” and “Super Bowl LII” in the American football events, “Hurricane Maria” and “Hurricane Dorian” in hurricane events.",Information Processing & Management,Multi-modal news event detection with external knowledge,https://www.sciencedirect.com/science/article/pii/S0306457324000578,https://drive.google.com/file/d/1OZe_eH4OJzNZpLJDPHodtlHr7M8wU4ss/view?usp=drive_link,Table 1. The statistics of NED dataset. (“#” represents the number of samples).,Table,Durga Srikari Maguluri
53,"Compared to other datasets, the NED dataset is collected based on hashtags, which avoids the disadvantages of keyword searches and is more aligned with real-world scenarios. Additionally, the data we have gathered is fine-grained and multi-modal, which benefits researchers in their further studies. Furthermore, we have open-sourced our dataset, with the hope of fostering the development of the field of news event detection.",Information Processing & Management,Multi-modal news event detection with external knowledge,https://www.sciencedirect.com/science/article/pii/S0306457324000578,https://drive.google.com/file/d/15UeGTFsYdD6IVAE21HgaDJAIZK83bk18/view?usp=drive_link,Table 2. Comparison of existing datasets. (“#” represents the number of samples.),Table,Durga Srikari Maguluri
54,"In this section, we introduce our proposed Multi-modal Fusion with External Knowledge (MFEK) method. Fig. 5 shows a framework of our method. Specifically, MFEK consists of three main modules: a text enhancement module, a knowledge extraction module, and a knowledge-aware feature fusion module. The text enhancement module enriches the original text by extracting the semantic features of the image, while the knowledge extraction model utilizes Wikipedia and large language models (LLM) to extract external knowledge from text and images. After that, the knowledge-aware feature fusion module uses the attention mechanism to merge knowledge, text, and images into a multi-modal fusion to predict news events. In the following, we first give the definition of multi-modal news event detection in Section 4.1. Then, we explain the components of the MFEK method in more detail in Sections 4.2, 4.3, and 4.4.",Information Processing & Management,Multi-modal news event detection with external knowledge,https://www.sciencedirect.com/science/article/pii/S0306457324000578,https://drive.google.com/file/d/1ky2imG_hUfxHvj1w2_foX1Q9-OclFXql/view?usp=drive_link,Fig. 5. The framework of MFEK.,Flow Diagram,Durga Srikari Maguluri
55,"The performance of our model decreases significantly in the absence of text or image inputs. This underscores that multi-modal data has a complementary function in news event detection, which plays a crucial role in supplementing the news event elements.
For the text enrichment module, the semantic information represented by captions or OCR text is beneficial for news event detection. This demonstrates the advantage of using visual semantic representations to enrich text.
The integration of external knowledge, including both implicit and explicit knowledge, contributes to the performance. This highlights the benefit of leveraging external knowledge for news event detection, which can help the model mitigate the OOD problem.After replacing the co-attention Transformer module with a concatenation method, the performance of the model decreases. This proves that our designed knowledge-aware feature fusion module can effectively integrate the obtained knowledge with the input, filtering out some irrelevant information to the task.",Information Processing & Management,Multi-modal news event detection with external knowledge,https://www.sciencedirect.com/science/article/pii/S0306457324000578,https://drive.google.com/file/d/1vMougTLTj5tuPIF3O-qVq5wKkbSqj3mA/view?usp=drive_link,Table 5. Classification performance on the test set for different variants of the MFEK model.,Table,Durga Srikari Maguluri
56,"Semantic-based image segmentation. MLRN employs atrous convolution to obtain a high-dimensional visual field of images. The image is then segmented using non-iterative clustering. By performing segmentation at multiple levels, the model can extract multi-view features that aid in identifying noisy image blocks. Reconstruction of noisy image blocks. MLRN applies fast Fourier convolution to reconstruct the defaced image in accordance with the mask image. Subsequently, an attention mechanism is adopted to synthesize the images, ensuring that the reconstructed image attains optimal performance from both global and local perspectivesRecognition of noisy image blocks. In this part, MLRN fuses multi-view features to achieve accurate identification of noise. The model then generates mask images with varying levels of granularity.Recognition of noisy image blocks. In this part, MLRN fuses multi-view features to achieve accurate identification of noise. The model then generates mask images with varying levels of granularity.",Information Processing & Management,MLRN: A multi-view local reconstruction network for single image restoration,https://www.sciencedirect.com/science/article/pii/S0306457324000608,https://drive.google.com/file/d/113NaLVGrRaKBWeWvnJyKL2SUQPD2ZQfb/view?usp=drive_link,"The overall architecture of MLRN. This model is composed of three main components: (i) semantic-based image segmentation, (ii) recognition of noisy image blocks, and (iii) reconstruction of noisy image blocks. MLRN eliminates noise from the defaced image by identifying and reconstructing the corrupted image blocks.",Flow Diagram,Durga Srikari Maguluri
57,"To comprehend the overall structure of the image, a large and efficient receptive field is essential. While multi-level stacked convolution can expand the receptive field, it may lead to parameter explosion. To prevent this, we employ atrous spatial pyramid pooling (ASPP) and a three-layer convolution to implement the spatial mapping, as depicted in Fig. 3(i). To be specific, we extract the R, G, and B components, the row and column position indices of each pixel to create the matrix , where  and  represent the height and width of , respectively. Subsequently, we apply ASPP to sample  three times, resulting in the feature matrix , as illustrated in Eq. (4).",Information Processing & Management,MLRN: A multi-view local reconstruction network for single image restoration,https://www.sciencedirect.com/science/article/pii/S0306457324000608,https://drive.google.com/file/d/1qASSEvJRHEl7QzEfrig2VIPl-8RGQJci/view?usp=drive_link,"The network structure of semantic-based image segmentation. It mainly involves three components: feature extraction, gradient calculation and label aggregation. To rapidly expand the receptive field, we utilize dilated convolution, while non-iterative clustering is applied to decrease the computational load.",Flow Diagram,Durga Srikari Maguluri
58,"During the reconstruction process, it is essential to take into account the global contextual information, rather than just focusing on local features. This necessitates that MLRN encompasses wide sensory units as early as possible. In traditional convolutional networks, the sensory units grow at a slower pace, thus rendering the reconstruction process inefficient. To tackle this issue, we adopt fast Fourier convolution (FFC) for image restoration, as it can perceive the entire image features, as demonstrated in Fig. 4(b). FFC comprises two primary components: global and local parts. In the former, MLRN utilizes a 3 × 3 convolution kernel to execute convolution operations; in the latter, MLRN leverages real FFT to capture the global contextual context. The specific steps of FFC are as follows",Information Processing & Management,MLRN: A multi-view local reconstruction network for single image restoration,https://www.sciencedirect.com/science/article/pii/S0306457324000608,https://drive.google.com/file/d/1BA32eGLbnr8NByl1gEjstcplH1qU6W-I/view?usp=drive_link,Fig. 4. The network structure for the reconstruction of noisy blocks. (a) MLRN adopts a framework that leverages mask images and fast Fourier convolution (FFC) for reconstructing the defaced image blocks. (b) The construction of the FFC module is presented in detail.,Flow Diagram,Durga Srikari Maguluri
59,"In the MLRN training process, we employ the Adam (Liu, Lin et al., 2021, Suvorov et al., 2022) optimizer with an initial learning rate of 0.0004. Additionally, we halve the learning rate at  and  iterations during training, for a total of  iterations. It is important to note that during the training of deep networks, overfitting (poor generalization) can easily occur due to the complexity of the mapping function and limited training data. To alleviate this problem, we adopt the L2 regularization strategy to prevent overfitting. The hyperparameters are determined using grid search, which is widely used in many deep models (Liu, Dang et al., 2022, Yu et al., 2021). In our experiments, we initialize the hyperparameters according to the values listed in Table 4.",Information Processing & Management,MLRN: A multi-view local reconstruction network for single image restoration,https://www.sciencedirect.com/science/article/pii/S0306457324000608,https://drive.google.com/file/d/10_Fpi3cstIdEwiw390loqww_ioXA8RRF/view?usp=drive_link,Table 4. Hyperparameter setting of MLRN.,Table,Durga Srikari Maguluri
60,"In order to provide a visual representation of the performance, we use Fig. 6 to display the data presented in Table 5. It is apparent from the figure that all algorithms can restore damaged images (input images) to some extent. In regards to PSNR and SSIM, TDTN, GFP-GAN, and EDGE exhibit superior performance relative to DIP and OWA. Moreover, these three methods can produce images with higher quality. Among all baseline algorithms, the restored images produced by GFP-GAN exhibit the highest similarity to the original versions (FID). Concerning unstructured noises (LPIPS), the performance of GFP-GAN and TDTN surpasses that of other baseline methods. Overall, our method achieves the best results across all four quantitative metrics. The restored images generated by MLRN are more similar to the original ones and possess superior visual quality.",Information Processing & Management,MLRN: A multi-view local reconstruction network for single image restoration,https://www.sciencedirect.com/science/article/pii/S0306457324000608,https://drive.google.com/file/d/1mDlaJgC1st5p0TUoLWz4YTEOxz6pa-5I/view?usp=drive_link,"Table 5. Quantitative results of different methods on DIV2K-R and CelebA-R datasets.“” indicates that a higher value denotes better performance, while “” means that a lower value reflects superior performance. We bold the best results for each metric. It can be observed that MLRN achieves the best performance on both DIV2K-R and CelebA-R datasets",Table,Durga Srikari Maguluri
61,"The main objective of CMFD is to find a pair of similar target areas composed of the source and the tampered. Focus on the task, this paper proposes the DRnet, shown in Fig. 2, for CMFD, which includes the CD module and the SD module. First, the feature extractor is leveraged to extract features from the copy-move tampered images as inputs of the CD module and SD module. Then, the coarse object area is obtained through the CD module. In addition, the coarse location obtained in CD is used as the prior condition to locate the target area in the multiple shallow layers, and the layer-by-layer decoupling method is used to mine the suppressed non-redundant details in the shallow layer. The output represented coarse detection generated from the CD module and the supplementary area features detected by the SD module are respectively extracted by ASPP and decoded. Finally, the two features are fused adaptively to output the final detection mask. Each module is described in detail in the following parts.",Information Processing & Management,Strong robust copy-move forgery detection network based on layer-by-layer decoupling refinement,Strong robust copy-move forgery detection network based on layer-by-layer decoupling refinement - ScienceDirect,https://drive.google.com/file/d/13EYH8bijRBYUD9fSaJXz1Q8r3qYTnmRi/view?usp=drive_link,"Fig. 2. The overall structure of the proposed DRNet. The DRNet consists of the CD module and the SD module, the former is utilized to extract the coarse similar areas by the novel HS scheme. The high-level feature generated from the CD module is set as a prior condition to obtain the initial location of the target area in the shallow feature. The latter extracts the suppressed similar targets from the shallow layers with initial location by a layer-by-layer decoupling mechanism that works as a supplement for the coarse detection from the CD module to further refine similar target areas.",Flow Diagram,Durga Srikari Maguluri
62,"We leverage the USC-ISI CMFD, a synthetic dataset provided by Wu et al. (2018a) to train the model, which contains a total of 100,000 samples. We divide it into 8:1:1 for model training, validation, and performance testing. Usually, copy-move forgery occurs in the new scene environment, therefore the generalization of the model puts forward higher requirements. Aiming at testing the generalization ability of the DRNet we proposed, in addition to USC-ISI CMFD, we conduct relevant experiments on two other standard datasets, i.e., the CASIA v2.0 CMFD dataset (Dong, Wang, & Tan, 2013), and the CoMoFoD dataset (Tralic, Zupancic, Grgic, & Grgic, 2013). The details of the public datasets we used are shown in Table 1.",Information Processing & Management,Strong robust copy-move forgery detection network based on layer-by-layer decoupling refinement,Strong robust copy-move forgery detection network based on layer-by-layer decoupling refinement - ScienceDirect,https://drive.google.com/file/d/1oMTXizCMGvjY-Qo4OkWZOeadgg4_POzN/view?usp=drive_link,Table 1. Details of three forgery datasets.,Table,Durga Srikari Maguluri
63,"From our literature review, we developed our research model, presented in Fig. 3. First, we expected the knowledge differentiation inherent in the text and audio of paid knowledge products to prominently affect consumers’ purchase decisions. Second, we expected such effect to be influenced by the temporal distance between the textual knowledge differentiation and the audio knowledge differentiation. Third, we investigated the moderating role of relational capital and persuasion knowledge in the effect of knowledge differentiation on customers’ purchase decisions.",Information Processing & Management,Near-future vs. distant-future: Unraveling the effect of knowledge differentiation on customers’ decision to purchase paid knowledge from the temporal distance perspective,https://www.sciencedirect.com/science/article/pii/S0306457324000505,https://drive.google.com/file/d/1tzKdMLPYZ3vvGdo6fNCea367PQA1VAr2/view?usp=drive_link,Fig. 3. Research model.,Flow Diagram,Durga Srikari Maguluri
64,"Drawing on the above discussion, we visualize our experimental method framework, as shown in Fig. 5. The method framework mainly consists of two parts: (1) processing the preview audio and the text description data and (2) the empirical analysis.",Information Processing & Management,Near-future vs. distant-future: Unraveling the effect of knowledge differentiation on customers’ decision to purchase paid knowledge from the temporal distance perspective,Near-future vs. distant-future: Unraveling the effect of knowledge differentiation on customers’ decision to purchase paid knowledge from the temporal distance perspective - ScienceDirect,https://drive.google.com/file/d/10QPZGf5shy9r_unG_7p2RytvrRp7QerA/view?usp=drive_link,Fig. 5. Experimental method framework.,Flow Diagram,Durga Srikari Maguluri
65,"Drawing on the main analysis, the three robustness checks, and the post-hoc analysis, this study confirms the four hypotheses on the positive effects of knowledge differentiation. Table 11 summarizes the study results.",Information Processing & Management,Near-future vs. distant-future: Unraveling the effect of knowledge differentiation on customers’ decision to purchase paid knowledge from the temporal distance perspective,Near-future vs. distant-future: Unraveling the effect of knowledge differentiation on customers’ decision to purchase paid knowledge from the temporal distance perspective - ScienceDirect,https://drive.google.com/file/d/1Gthpar2Wcz0gmR7_fJ3v9ylUF9mF-yeT/view?usp=drive_link,Table 11. Summary of empirical results.,Table,Durga Srikari Maguluri
66,"The following sub-sections provide a detailed description of the above process. Specifically, the coarse-grained stage contains the following primary components: (1) pre-processing (Section 5.1), (2) multi-granularity embedding (Section 5.2), (3) heterogeneous graph construction (Section 5.3), and (4) multi-granularity relationship-based extractor (Section 5.4), while the fine-grained stage only contains abstractive summarizer (Section 5.5).",Information Processing & Management,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor - ScienceDirect,https://drive.google.com/file/d/1KUDRYYms_Ig7HZwL_VK96795rUjaRSpI/view?usp=drive_link,Fig. 3. General framework of the proposed coarse-to-fine pipeline for abstractive multi-document summarization.,Flow Diagram,Durga Srikari Maguluri
67,"The first module of the MGRExtractor is to remove noisy documents using the constructed heterogeneous graph. In particular, a K-Nearest-Neighbors (KNN)-based outlier detection algorithm is used to discover potentially noisy documents. As demonstrated in Algorithm 1, the noise removal module traverses each document node  in graph  to calculate the average score of the document edges  of its nearest two neighbors. If the score is greater than a preset threshold, the graph is pruned by removing the noisy node and its connecting edges. This algorithm can effectively detect outliers (such as noisy nodes) that are far away from most normal nodes, ensuring that documents with irrelevant topics will not appear in the summary. However, it cannot make sense when there are only two documents in a cluster.",Information Processing & Management,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor - ScienceDirect,https://drive.google.com/file/d/1n3wrxNez07uQCkEhDlfcr8866hckKzhn/view?usp=drive_link,Fig. 5. Flowchart of the proposed MGRExtractor.,Flow Diagram,Durga Srikari Maguluri
68,"From this table, we have the following observations. Among single-granularity representations, sentence-sets exhibit better performance than finer-grained sentences and coarser-grained paragraphs and documents. Specifically, MDS-SGRE achieves an average performance of 28.55 on Multi-News. This finding emphasizes the necessity of selecting sentence-sets to generate high-quality meta-documents for MDS tasks. Compared to these single-granularity methods, our multi-granularity proposal shows significant improvements in term of ROUGE-1/2/L/SU. These results indicate the superiority of multi-granularity methods over single-granularity methods. Lastly, there is an improvement of 0.17 between our proposed MDS-MGRE and MDS-SGRE. It is worth noting that our proposal not only improves evaluation performance but also provides faster response time. These results further demonstrate the effectiveness of our top-down multi-granularity relationship-based extractor.",Information Processing & Management,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor,From coarse to fine: Enhancing multi-document summarization with multi-granularity relationship-based extractor - ScienceDirect,https://drive.google.com/file/d/1Bcp-fkwT5PLJiYMbqL4-C_dFlZdAqPSZ/view?usp=drive_link,"Table 6. Experimental results of various single-granularity methods and our multi-granularity proposal on Multi-News. Bold highlights the best performance in each column, and  indicates statistical significance.",Table,Durga Srikari Maguluri
69,"To address the problem of insufficient accuracy in heuristic methods, we propose the HCCKshell algorithm based on heterogeneous similarity and comprehensively consider the importance of users from two perspectives, including the social history and social relationship features of users. The framework of the HCCKshell algorithm is shown in Fig. 1, which includes two main modules: user representation learning and cross-comparison Kshell algorithm based on heterogeneous similarity entropy. The specific process covers Transformer Encoder and GCN technology to learn the content feature vector and the structure feature vector of the user, respectively, to obtain a more accurate low-dimensional latent vector representation. Firstly, the Transformer Encoder method and social history content data are used to model the content features of users. This process can encode the user’s historical behaviors in a continuous vector space to capture more semantic content feature vectors. Secondly, a Graph Convolutional Network (GCN) was used to model social relationships. GCN can consider direct and indirect relationships between users and their social networks to generate a structural feature vector for each user. This vector can better characterize the user’s position and influence in the social network. Next, to quantify the similarity between users, the concepts of heterogeneous similarity and heterogeneous information entropy are introduced. These concepts could measure how similar users are in terms of content and structural features, and better mine potential connections between users. Finally, the heterogeneous information entropy was applied to the improved cross-comparison Kshell algorithm to find the critical seed nodes in the network dynamically. By comprehensively considering multiple attributes of users and combining the concept of similarity and information entropy, the HCCKshell method can select the critical seeds more effectively. Table 1 shows the meaning of all notations listed in this paper, and specific implementations of the proposed algorithm are given as follows.",Information Processing & Management,HCCKshell: A heterogeneous cross-comparison improved Kshell algorithm for Influence Maximization,HCCKshell: A heterogeneous cross-comparison improved Kshell algorithm for Influence Maximization - ScienceDirect,https://drive.google.com/file/d/1csBobAD1UQhgn3JReVuwFFAOeHylJWl3/view?usp=drive_link,Fig. 1. The framework of HCCKshell algorithm,Flow Diagram,Durga Srikari Maguluri
70,"The experimental data sources contain two kinds, and the first kind of three datasets is real social network datasets (Li et al., 2022). They are all from Sina Weibo1, a popular social platform application in China. Although the number of nodes is not large, it contains the historical posting content of users and the relationship between users. The three datasets in the second category are wiki-RfA2, cit-HepTh3 and soc-sign-Slashdot4. Wiki-RfA is a voting data set of selected administrators of the Wikipedia community. Here, the positive or negative comments expressed by users are treated as the historical content of users. Cit-HepTh is derived from the high energy physics theory citation graph, where the paper’s title is taken as the historical content of the node. Soc-sign-Slashdot is derived from Slashdot, which revolves around technology-related news and applies to specific communities. Since this dataset does not contain the user’s historical content text, we use ChatGPT (Ray, 2023) to generate historical text content of any length for each user, and the length of the text is limited to no more than 100 when generating. The specific information of each data set is shown in Table 2. In the history content column, “y” represents that the data set contains historical text, and “n” means that the data set does not have historical text.",Information Processing & Management,hCCKshell: A heterogeneous cross-comparison improved Kshell algorithm for Influence Maximization,HCCKshell: A heterogeneous cross-comparison improved Kshell algorithm for Influence Maximization - ScienceDirect,https://drive.google.com/file/d/1q9qxuanW2Bc_S2lWr6G9n0njN6YgURQ7/view?usp=drive_link,Table 2. Datasets details.,Table,Durga Srikari Maguluri
71,"From the above discussion, conventional methodologies that rely heavily on historical user-news interactions for personalized suggestions face the cold-start problem. To the best of our knowledge, the challenge of a cold start in NR, particularly in the context of zero-shot, has not yet been explored. To address the cold-start predicament specifically for entirely new users, we introduce a novel zero-shot methodology aimed at mitigating the cold-start problem (CSP) in a news-recommender system. To accomplish this, we present a novel framework, BCE4ZSR, which employs a student–teacher architecture featuring a combination of bi-encoders and fine-tuned cross-encoders. The bi-encoder acts as the student undergoes knowledge distillation from the cross-encoder and teacher to enhance its capacity to handle the cold-start problem. This design choice stems from the inherent strengths of both models in capturing nuanced user preferences and complex features of news articles. Furthermore, the incorporation of a zero-shot setting further aligns with the cold-start problem, enabling our model to provide meaningful recommendations even for users with no historical interactions. Our proposed BCE4ZSR is applicable to any neural NR model based on user news embeddings. Moreover, BCE4ZSR effectively increases the informational bandwidth available for user preference modeling by representing user embeddings through various news articles. As a result of this innovative approach, user profiles are not only enhanced in depth, but the recommendation accuracy is also improved. Using this novel framework in our research, we aim to demonstrate its efficacy in addressing the cold-start news recommendation problem, as well as in regular cases.",Information Processing & Management,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation - ScienceDirect,https://drive.google.com/file/d/1PPDuLdng0RPG_9LZwa0QBoLBw-q9fhy3/view?usp=drive_link,"Fig. 1. ZSL and CSR: The principles of ZSL and CSR are similar. In CSR, the behavior of new users is missing, just as unseen classes are missing in ZSL.",flow Diagram,Durga Srikari Maguluri
72,"In personalized NR, recall and ranking are two critical steps. An example of a personalized NR workflow is shown in Fig. 2. As shown in Fig. 2, when a user accesses a news platform, it retrieves a small set of candidate news articles from a large pool of articles, and according to user profiles, the personalized news recommender ranks these articles. Subsequently, the personalized news recommender employs user profile insights to prioritize and rank these candidates based on user interests. The top K-ranked news articles are then presented to the user, and the platform records user interactions with these articles. These interactions are crucial for updating the user profile, ensuring that future services are tailored to evolving user preferences and behaviors.However, new news stories are consistently published, and occasionally new users visit online news platforms. These typical models are insufficient to generate a good recommendation because there are no user-news interactions. This paper introduces a new framework named, BCE4ZSR, that mitigates the cold start dilemma in news recommender systems. The presented methodology consists of two main steps: a fine-tuned sentence transformer (Bi-Encoder), and a bi-encoder enforced with a fine-tuned transformer (Cross-Encoder). The bi-encoders are utilized for inference because of the efficiency they offer when tackling large datasets, such as MIND. Secondly, we take a novel route by synergizing the bi-encoder and cross-encoder models. During the training phase, we employ the cross-encoder model as a teacher to further refine the bi-encoder’s capabilities. This setup allows the bi-encoder to learn from the insights and knowledge provided by the cross-encoder. Subsequently, during the inference phase, we exclusively employ the bi-encoder model. This strategic choice optimizes faster inference times while still benefiting from the enhancements learned from the teacher cross-encoder.",Information Processing & Management,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation - ScienceDirect,https://drive.google.com/file/d/1rkRccXP7duKgpphW0-eAiOaJPzC-zTYQ/view?usp=drive_link,Fig. 2. Typical pipeline of news recommendation.,flow Diagram,Durga Srikari Maguluri
73,"where,  is the ground truth label (1 for positive, 0 for negative). p is the predicted click probability. The total loss combines the individual losses, and by optimizing this loss, the model learns to predict accurate recommendations for regular and cold-start cases. The total loss for the recommendation task is depicted in Eq. (10):",Information Processing & Management,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation,BCE4ZSR: Bi-encoder empowered by teacher cross-encoder for zero-shot cold-start news recommendation - ScienceDirect,https://drive.google.com/file/d/18kEeoTyrQERPfmztpjA4N56NBT7qiYtS/view?usp=drive_link,Fig. 4. The overall structure of our BCE4ZSR approach.,flow Diagram,Durga Srikari Maguluri
74,"ig. 1. The workflow of our model. It starts from the first stage—Pattern-RG. The encoder converts the post  to hidden representations. Simultaneously, the pattern model generates a vector  of the pattern distribution, where each dimension denotes one pattern. Then the concatenation of the hidden vectors and the pattern distribution is fed into the decoder to control the pattern of the response. Next, the output response of the decoder is fed into the second stage—Content-RM where keywords of content are firstly added to the response. After that, the response is iteratively modified using the MH sampling algorithm. Finally, the output response  includes both the pattern and the content.",Information Processing & Management,Pattern and content controlled response generation,Pattern and content controlled response generation - ScienceDirect,https://drive.google.com/file/d/1cfY5MX3bYsKGTSjCaWSt2HZ_D9R4OTTo/view?usp=drive_link,Fig. 1 shows the workflow of our model.,Flow Diagram,Durga Srikari Maguluri
75,"Each dataset is divided into three subsets: 80%, 10%, and 10%, for training, validation, and testing respectively. All the parameters of the pattern model and the encoder–decoder model are tuned with the validation set. After the tuning process, the dimension number for the pattern embedding is 64. The hidden size of the encoder and decoder is 256. Our encoder–decoder model is simply the sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015).",Information Processing & Management,Pattern and content controlled response generation,Pattern and content controlled response generation - ScienceDirect,https://drive.google.com/file/d/18wHVohIomK17r0AoZLhbcIZuHRu3RQqf/view?usp=drive_link,"Table 2. Overview of the datasets, |V| is the size of |V|.",Table,Durga Srikari Maguluri
76,"Table 6 shows the results. We observe that when there is only one keyword, our Fluency score is lower than SongNet and the same with CGMH. But with more than one keywords, our method outperforms both SongNet and CGMH. This is because in SongNet, positions of keywords are fixed and in CGMH, the order of keywords is fixed, which may limit the quality when the set of keywords gets larger. For Content-RM, neither the positions nor the order is limited. According to the results of Distinct and Dives, our methods have the best performance in generating diverse sentences. From the results of Content, MultiSource cannot ensure mandatory inclusion of the content during the left-to-right generation in RNN.",Information Processing & Management,Pattern and content controlled response generation,Pattern and content controlled response generation - ScienceDirect,https://drive.google.com/file/d/104eNf1Ms_fZlDbUeS9I2u6Hc9NKvJwBS/view?usp=drive_link,"Table 6. Results of human and automatic evaluation on sentences with 1 to 3 keywords. Sentences with higher Fluency, higher Distinct, lower Self-BLEU and higher Content scores are better. Boldface indicates the best performance in each row.",Table ,Durga Srikari Maguluri
77,"We also compare our method with PPLM (Dathathri et al., 2020) which is able to generate pattern-controlled text with pre-specified prefix or suffix content; the pattern is extracted from input keywords. The experiment is conducted on 10k randomly sampled post-user-content pairs from the test sets. PPLM is not a response generator, but to keep variables the same, we feed it both the post and the user response history as the keywords of pattern. We evaluate the generated texts in the metrics of C-score, Self-BLEU and Distinct; the results are shown in Table 8. Our Method outperforms PPLM in generating diverse responses consistent with the user response history.",Information Processing & Management,Pattern and content controlled response generation,Pattern and content controlled response generation - ScienceDirect,https://drive.google.com/file/d/1uPXOCTsSbv0XAXnjfAJb2rvzlENUH2GK/view?usp=drive_link,Table 8. Metric-based evaluation results of Our Method and PPLM. Boldface indicates the best performance in each column.,Table ,Durga Srikari Maguluri
78,"Fogg’s (2002b) framework suggests that three main credibility aspects are operator, content, and design. Based on the web credibility criteria mentioned in Section 2.1, we deleted the criteria that are unlikely to occur in mobile searches or good abandonment. For example, we omitted “privacy control” because it is difficult to identify privacy control functions on an SERP. We deleted “content length” because in a mobile SERP, all the results’ content is the same length. Consequently, we proposed a model of good abandonment credibility measures in mobile searching in terms of operator, content, and design (see Fig. 1). Table 1 presents an explanation of each criterion in the model.",Information Processing & Management,Credibility assessment of good abandonment results in mobile search,Credibility assessment of good abandonment results in mobile search - ScienceDirect,https://drive.google.com/file/d/1j5cxqBIxb-rCVWwapWA-dVJMidAWN09C/view?usp=drive_link,Fig. 1. Model of good abandonment credibility assessment in mobile search.,Table ,Durga Srikari Maguluri
79,"Since the research goal is to explore the credibility assessment of good abandonment results, it is necessary to design tasks that can lead to good abandonment. Li et al. (2009) suggest query categories that can potentially lead to good abandonment: Weather, Translation, Stock, Currency and Celebrities. We designed tasks according to these categories. These tasks are fact-finding tasks which are in line with our daily search habits. All the tasks are related to the daily life of a college student. Five search tasks are shown in Table 2.",Information Processing & Management,Credibility assessment of good abandonment results in mobile search,Credibility assessment of good abandonment results in mobile search - ScienceDirect,https://drive.google.com/file/d/1CCnJXWajVDivrZTfQHn8D_de8Mmj6Ejx/view?usp=drive_link,Table 2. Mobile search task description.,Table ,Durga Srikari Maguluri
80,"As a result, we found that six factors are essential for users’ credibility assessment: result consistency, content neutrality, authority of the search engine, the search engine's ranking, interaction design and usability, and information design. According to these factors, we improved the model of good abandonment credibility assessment in mobile searching",Information Processing & Management,Credibility assessment of good abandonment results in mobile search,Credibility assessment of good abandonment results in mobile search - ScienceDirect,https://drive.google.com/file/d/1sQ-17ygXwLE6FMBtSxf1okpNik3fVxBG/view?usp=drive_link,Table 4. Rotated component matrix for credibility assessment criteria factor analysis.,Table ,Durga Srikari Maguluri
81,"For the content aspect, result consistency is the core of credibility judgment. Users spend the most time paying attention to the content of the search result, judging whether the retrieved content meets their information need. Through interview analysis, we can see that when assessing credibility, users will first judge whether the search result is desired. If the result meets the need, users will determine that this result is reliable to some degree. Users also utilize additional means to verify the content's consistency. Users apply their previous experience or knowledge to verify if the information they are currently seeing is consistent with their information need. It is seen that the result consistency refers to relevance, which measures how close the result is to the information need. Therefore, result consistency being included in the credibility assessment model indicates relevance and credibility have a connection, which is mentioned in Section 3. The consistency of the results includes consistency of both content and context. Contextual factors such as time and place will increase the credibility of the results. As seen in T1, users tend to find the results of a weather search more credible when the location information or temperature is more accurate or when the latest time is updated. Search engines can improve the credibility of the search results by providing more accurate and time-sensitive search results that meet users’ information needs.",Information Processing & Management,Credibility assessment of good abandonment results in mobile search,Credibility assessment of good abandonment results in mobile search - ScienceDirect,https://drive.google.com/file/d/1aZCIazWMvjsaa1Gmula-B4Yw1RicC5sA/view?usp=drive_link,Fig. 6. Model of credibility assessment of good abandonment results in mobile search.,Flow Diagram,Durga Srikari Maguluri
82,"Based on the results of RQ1 and RQ2, we identified the attributes of cross-device search tasks. Additionally, we proposed RQ3 to understand cross-device search tasks in greater depth by revealing relationships between these attributes. Relationships between topic, task type, and task complexity have been investigated in web search on a single device, but not across devices (Liu et al., 2013, 2010). In addition, compared with general web search, cross-device search depends heavily on search context. Therefore, we proposed the hypotheses shown in Fig. 1. First, we tested the correlation between attributes separately in terms of search task and search context. Second, we studied the correlation between search task and search context by examining which context attribute has an impact on which task attribute",Information Processing & Management,Exploratory study of cross-device search tasks,Exploratory study of cross-device search tasks - ScienceDirect,https://drive.google.com/file/d/15Vqels9lRByXdamaHcqpqTC--k01mG46/view?usp=drive_link,Fig. 1. Hypotheses of relationship between attributes of cross-device search tasks.,Flow Diagram,Durga Srikari Maguluri
83,"Coding of task types resulted from respondents’ own choices. The options provided were factual task (specific question and specific answer), interpretive task (specific question and general answer), and exploratory task (general question and general answer). Table 3 shows that nearly half of the cross-device searches reported in the survey were factual tasks, and interpretive and exploratory tasks accounted for almost equal proportions of the other half. Compared to interpretive and exploratory tasks, the information needs of a factual task with a specific answer should be less complex. Searching factual tasks across different devices is likely to be driven less by complex information needs and more by other driving forces. Table 3 also shows that task types with specific questions, namely factual and interpretive tasks, predominate. This indicates that cross-device search is generated from the clear motivation of solving a problem.",Information Processing & Management,Exploratory study of cross-device search tasks,Exploratory study of cross-device search tasks - ScienceDirect,https://drive.google.com/file/d/1CmKUa0mcbH_44a3cEwylJgpUAXPhbqU3/view?usp=drive_link,Table 3. Task types of cross-device search tasks.,Table,Durga Srikari Maguluri
84,"As shown in Table 7, there are 12 ways of switching device among the 342 replies (one outlier switched from PS4 to laptop). Search devices used in cross-device search were mainly smartphones, desktops, and laptops. For the pre-switch device, there is no doubt that smartphones are in a dominant position. Desktops and laptops have nearly equal shares. For the post-switch device, the gap between smartphones and desktops/laptops narrows. We found that users prefer smartphones as the initial device for cross-device search. There are two frequent ways in which device switch occurs: smartphone to laptop and smartphone to desktop. This demonstrates a frequent device switch direction, which is mobile to desktop in cross-device search. Users prefer the smartphone as the initial search device because of its portability, accessibility, and privacy. In the age of the 4 G network, people and their smartphones are inseparable. In addition, in many companies, employees are not allowed to use work computers to deal with private issues. Therefore, smartphones become the first choice for searching.",Information Processing & Management,Exploratory study of cross-device search tasks,Exploratory study of cross-device search tasks - ScienceDirect,https://drive.google.com/file/d/1cOBLgEBhy55Eu9Iplow0kPfnQJX0PCht/view?usp=drive_link,Table 7. Device switch of cross-device search tasks.,Table,Durga Srikari Maguluri
85,"As shown in Table 8, there is significant correlation between Topic and task complexity, no matter which dimension. In-depth analysis of the co-occurrence indicates that the topics occurring most frequently in complexity of factual knowledge are Health and Reference. For complexity of conceptual knowledge, the top topic is Arts. The topic of Computers occurs the most in the complexity of procedural knowledge. As for the complexity of metacognitive knowledge, the occurrence of different topics is low and similar, the top topic being Arts. From the point of view of cognitive dimension, Arts-related searches occur the most in the complexity of ‘remember.’ Reference is likely associated with the complexity of ‘understand.’ Shopping is the most frequent topic in the complexity of ‘apply.’ Business co-occurs the most with the complexity of ‘analyze.’ For the complexity of ‘evaluate,’ Recreation is the prominent topic, while Home is the top topic in the complexity of ‘create.’ There is significant correlation between task type and the complexity of knowledge dimension. It can be expected that the factual task occurs the most in the complexity of factual knowledge. Interpretive tasks are the most in the complexity of conceptual knowledge. Exploratory tasks are prominent in the complexity of metacognitive knowledge.",Information Processing & Management,Exploratory study of cross-device search tasks,Exploratory study of cross-device search tasks - ScienceDirect,https://drive.google.com/file/d/1kVtg23lZXFLz4jIuueddnwZ3iG4UUL1T/view?usp=drive_link,Table 8. Contingency coefficient among task attributes.,Table ,Durga Srikari Maguluri
86,"We were alternatively motivated by the finding that some headline formats follow similar language patterns inherently: the stem and syntax of the sentence remain the same, but some crucial words change to convey the content, as shown in Table 1. Based on the observation, we frame the headline style transfer task as distinct content-feature extraction and style modeling via a dual-encoder and a shared decoder to generate target-styled texts. To conquer the problems of lacking parallel content and style training data, we synthesize adequate training pairs to accommodate the content extraction and style modeling with reference to the target-style texts. Given a target stylistic text, we separately create a content-similar input and a style-consistent input to feed the dual encoders.",Information Processing & Management,Latent representation discretization for unsupervised text style generation,Latent representation discretization for unsupervised text style generation - ScienceDirect,https://drive.google.com/file/d/1ycn5qg_BltpWNvEYHxAW_y6UOwaWi2mp/view?usp=sharing,Table 1. Examples of two kinds of syntactic structures. Sentences sharing the same syntactic structure can be treated as a style.,Table ,Durga Srikari Maguluri
87,"The model structure is described in Section 3.3. One of the inputs is content input  encoded by a content encoder, then fed into a content pooling to extract its sentence-level feature, denoted as . Similarly, the other input is style input  encoded by a style encoder, then fed into a style pooling to get style representation . The hypothesis is that the pooling serves as a bottleneck which can disentangle the representation of content and style with help of proper loss function (Section 3.4). The overall model architecture is shown in Fig. 1.",Information Processing & Management,Latent representation discretization for unsupervised text style generation,Latent representation discretization for unsupervised text style generation - ScienceDirect,https://drive.google.com/file/d/1CFqhu7iDCDcDxefPmpEyB2lzEkaF7lxt/view?usp=drive_link,Fig. 1. The framework of the DST model. The training phase and inference phase are depicted in the figure.,Flow Diagram ,Durga Srikari Maguluri
88,"Baselines We compared the proposed model against the following three strong baseline approaches in text style transfer: BARTR (Lai, Toral, & Nissim, 2021) is trained by fine-tune BART model with an extra BLEU reward and style classification reward. This model uses a parallel dataset. In order to meet our requirement of an unsupervised style transfer setting, we used pseudo-parallel data  and  as input and target in the following experiment. StyIns (Yi et al., 2021) leverages the generative flow technique to extract stylistic properties from multiple instances to form a latent style space, and style representations are then sampled from this space. TSST (Xiao et al., 2021) proposes a retrieval-based context-aware style representation that uses an extra retriever module to alleviate the domain inconsistency in content and style.",Information Processing & Management,latent representation discretization for unsupervised text style generation,Latent representation discretization for unsupervised text style generation - ScienceDirect,https://drive.google.com/file/d/1RVv9PMRzT-vCQqysPYLsWGMzyF4QEl_7/view?usp=sharing,Table 4. Hyperparameters used in our experiments.,Table ,Durga Srikari Maguluri
89,"To tackle the challenges mentioned above, we propose Joint Inter-Word and Inter-Sentence Multi-Relation Modeling for the Summary-based Recommender System (MRSR), which removes noise data from the reviews and extracts concise and clear summary information to learn user and item feature representations. Examples in Fig. 1 show the summaries and the reviews of the same items. According to the rating, user Tom’s evaluation of the clothes is positive, and both the review and summary contain Tom’s preference for the clothes, such as material and season (bold part). However, the review also includes useless information (underlined part). Similarly, user Mike’s evaluation of the clothes is negative, and the review and summary contain the description for style, quality, and price. And there is also some noise in his review. Therefore, it can be seen that the user’s summary and review contain the same amount of information that represents user preference and item attributes. Compared to reviews, summaries are more concise and have less noise, making them more suitable for recommendation in today’s big data environment. To model the Summary information at different granularities, MRSR employs two multi-relationship modeling modules, the inter-word and inter-sentence modules, to learn the user and item features contained in the Summary information. The former utilizes Transformer (Vaswani et al., 2017) and a pooling layer to perceive the relative relationships between words in a single summary and learn representative features. At the same time, the latter employs a self-attention mechanism to differentiate the importance of different summaries in predicting the rating and constructing sentence-level feature representations. The multi-headed attention (MHA) fusion layer is then used to learn the correlation of user and item features, performing feature fusion with weighted importance. Finally, the attentional factorization machine (AFM) (Xiao et al., 2017) is utilized to realize deep interaction and computation between high-order nonlinear features of user–item fusion features for rating prediction.",Information Processing & Management,Joint inter-word and inter-sentence multi-relation modeling for summary-based recommender system,https://www.sciencedirect.com/science/article/pii/S0306457323003680,https://drive.google.com/file/d/1nt6eTKJ3jdMeMKBE6sE0EjUADkHtONlh/view?usp=sharing,"Fig. 1. Summary vs. Review. Both summaries and reviews contain equal information on the user’s tendencies and item’s attributes, but it is clear that the summaries are more concise and have less noise than the reviews.",Flow Diagram ,Durga Srikari Maguluri
90,"he paper proposes MRSR, as depicted in Fig. 2, which includes a summary data preprocessing module, an inter-word and inter-sentence multi-relation modeling module, and a fusion and prediction module. Firstly, to reduce the amount of noise in the data and simplify the model training, this study uses existing summary information in the dataset as the data source. All summaries from a user are used to represent the user’s preferences, and all summaries about a particular item are used to describe the item’s attributes. Secondly, to address the problem of insufficient expression ability of user and item features, an inter-word and inter-sentence multi-relation modeling module is designed to enrich the information contained in the features. Specifically, the summary is processed by an embedding layer and a positional encoding layer. A Transformer and a pooling layer are applied to achieve fine-grained inter-word relation modeling, and a self-attention mechanism is employed to achieve coarse-grained inter-sentence relation modeling. Finally, to deal with the issue of insufficient interaction between user and item features, MRSR employs the MHA-based fusion module to achieve shallow interaction fusion between user and item features, and then the AFM prediction module performs attentional high-order non-linear deep interaction calculation on the user–item feature vector to get the predicted rating.",Information Processing & Management,Joint inter-word and inter-sentence multi-relation modeling for summary-based recommender system,https://www.sciencedirect.com/science/article/pii/S0306457323003680,https://drive.google.com/file/d/18MQxMbHR0OoKlmxOJYEw6XATqU6O-9oh/view?usp=sharing,Fig. 2. The architecture of MRSR.,Flow Diagram ,Durga Srikari Maguluri
91,"To fully utilize the information from different levels of the fused feature vector  during prediction, an AFM-based prediction module is used to calculate the first and attentional second orders of , achieving deep interaction computation and ultimately obtaining the prediction result. The structure of the module is illustrated in Fig. 3.Firstly, the prediction module based on AFM takes the fusion feature vector, denoted as , as input to a fully connected layer to obtain the AFM’s first-order linear feature. To address the problem of insufficient model learning due to data sparsity, an auxiliary matrix is introduced to assign a dimension of k to each associated feature element  Then, the auxiliary matrix is combined with the interaction feature , and both are simultaneously fed into the Pair-wise Interaction layer to obtain the AFM’s second-order interaction feature. Finally, the attention mechanism is applied to optimize the second-order feature by learning the importance of each part of the feature for the prediction result.During the rating prediction stage, this module sums up the results of the high-order non-linear deep interactions, i.e., the first-order linear features and the second-order interaction features based on the attention mechanism. Then, it linearly adds the sum to obtain the predicted rating value, denoted as . The complete formula is as follows:",Information Processing & Management,Joint inter-word and inter-sentence multi-relation modeling for summary-based recommender system,https://www.sciencedirect.com/science/article/pii/S0306457323003680,https://drive.google.com/file/d/1IA_gtWKRMNZ5e7nyC53Fw5KgeakOa9tP/view?usp=sharing,Fig. 3. The architecture of AFM-based prediction Module.,Flow Diagram ,Durga Srikari Maguluri
92,"Although intuitively, reviews are richer in corpus information than summaries, summaries contain the same amount of information about user tendencies and item characteristics as reviews. Instead, excessively long reviews introduce noisy information that affects the modeling of users items. Therefore, our modeling approach based on comment summaries is more advantageous regarding conciseness and amount of noise. The proposed MRSR achieved the best performance on all datasets, with an average improvement of 5.94% over the best baseline methods. The results demonstrate that incorporating summaries to the inter-word and inter-sentence multi-relation modeling module is able to capture user preferences and item attributes more effectively, while the deep interaction of features through fusion and prediction modules significantly improves prediction accuracy. Summaries contain more concise information than the comments, enhancing the model’s modeling ability. The multi-relation modeling module can effectively extract user and item features from two levels. Additionally, the fusion and prediction layer based on MHA and AFM can fully integrate features, distinguish the degree of correlation between feature information, and effectively select information for prediction while reducing the interference of noisy information.",Information Processing & Management,Joint inter-word and inter-sentence multi-relation modeling for summary-based recommender system,https://www.sciencedirect.com/science/article/pii/S0306457323003680,https://drive.google.com/file/d/1q5UpKwQ3Vc-l4iRBU0EjE3WZhqq5uZiB/view?usp=sharing,Table 3. Comparative results of RMSE on five datasets.,Table ,Durga Srikari Maguluri
93,"The experiments indicate that GS positively affects the model’s predictive capability compared to the original review, but it slightly lags behind the provided summary. We believe this could be due to GS extracting concise critical information from reviews. However, the clarity and brevity of the information are not as pronounced as those provided in the summaries. The experiments also show that the model’s predictive performance is influenced slightly by the quality of the generative summary. Therefore, we believe that in situations where datasets lack summaries while requiring more concise textual data than reviews, particularly when the model’s predictive results are not excessively sensitive, generative summaries can serve as a viable compromise.",Information Processing & Management,Joint inter-word and inter-sentence multi-relation modeling for summary-based recommender system,https://www.sciencedirect.com/science/article/pii/S0306457323003680,https://drive.google.com/file/d/1svZav9QOeHIidy1iN565lwrMuyZoBA6T/view?usp=sharing,"Table 7. Comparison of RMSE values for review, summary, and generative summary.",Table ,Durga Srikari Maguluri
94,"Although these approaches have been successful, the model is like a black box that can generate incorrect responses, leading to an uncomfortable experience for users. As shown in Fig. 1, the task-oriented dialogue model generates inconsistent content with the knowledge base (the distance of panda_express is 1_miles, while the distance generated by the dialogue system is 2_miles). To address this issue, Qin et al. (2021) first tried to solve the problem of consistency in the task-oriented dialogue domain by introducing a novel dataset for the consistency identification task. The dataset defines three types of inconsistency in task-oriented dialogue and formulates the task as a multi-label classification problem. This task aims to examine the consistency of response with facts to determine the response’s usefulness, which needs the task-oriented dialogue model to interact with a large knowledge base frequently. As shown in Fig. 2(a), they flatten the knowledge base into sequences and concatenate them with current dialogues as inputs to the language model. Notably, this method introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base, where the redundant information becomes noise during model learning",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1R0A0N0Hg6__7MpZ1i7OMrLpZHZLrOcHf/view?usp=sharing, Fig. 1. Example of an error response. The red font is the knowledge base information related to the dialogue. The blue font in the dialogue is an inconsistent response generated by the dialogue system (the entity of the response is inconsistent with the information in the knowledge base).,Flow Diagram ,Durga Srikari Maguluri
95,"To address the issue of introducing noise, previous works (Guan et al., 2020, Madotto et al., 2020) embedded the knowledge base directly into the model’s parameters. As shown in Fig. 2(b), they trained the model on the knowledge base and then performed subsequent tasks using the trained model. Given that their method needs to be trained on the knowledge base, it is only suitable for static or pseudo-dynamic knowledge bases with periodic updates (e.g., hotel names, phone numbers, and addresses). In reality, some knowledge bases require in real time updates, such as weather and vehicle conditions. Such a method needs to retrain the model to update the knowledge base, which results in significant training costs. Moreover, this method trains the model on new dialogue data after training on the knowledge base, which makes it easy to suffer catastrophic forgetting and the loss of part of the previously memorized knowledge base information (Luo et al., 2023).",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1znZMEoqtm7P-K_7WCn0JzbkmdiCh8B-X/view?usp=sharing,Fig. 2. Different ways of injecting knowledge into the method. Figure (a) directly flattens the knowledge base into a sequence and concatenates it with the dialogue history (Direct Concatenation Method). Figure (b) trains the model on the knowledge base first and then uses the trained model to perform the next task (Embedding the Knowledge Base into the Model Parameters Method). The method we proposed in Figure (c) incorporates the knowledge base with the PLM using an adapter and fusion layer.,Flow Diagram ,Durga Srikari Maguluri
96,"In this section, we first introduce the consistency identification task formulation. Then, we present the adapter module and describe how to design the fusion layer to avoid noise injection, and finally insert the adapter module into the language model to complete the consistency identification task (see Fig. 3).",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1MoZkbNsm2VrwOMbcE4rPGg7VsMn7C4Pd/view?usp=sharing,"Fig. 3. Model architecture. Firstly, we transform the knowledge base into the relational triple form, then encode the knowledge base using the adapter. Meanwhile, we input the dialogue into the language model and inject the encoded knowledge base information into the model using the fusion layer. Finally, we output whether there is a consistency issue in the current dialogue.",Flow Diagram ,Durga Srikari Maguluri
97,"While incorporating the knowledge base, directly injecting the adapter’s output into the language model causes noise, which leads to a decrease in model performance. Therefore, we design a new fusion mechanism that can select the appropriate knowledge base information based on the current utterance, allowing the model to only focus on relevant information from the knowledge base. Specifically, after obtaining the representations of the conversation history and the knowledge base, we apply a cross-attention mechanism to associate the conversation history with the knowledge base, where the query (), key (), and value () are , , and , respectively. To filter the irrelevant knowledge base information, we propose a sparse method using topk to select  knowledge base information that is most appropriate to the conversation. The formula of the fusion layer is as follows",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1FjocY1hJJlkONiYlLvsh-Tu3qgOCKyWb/view?usp=sharing,Fig. 4. Structure of the adapter layer (left). The adapter layer consists of two linear layers and  transformer layers. The transformer layer is shown on the right.,Flow Diagram ,Durga Srikari Maguluri
98,"Table 1 shows the experimental results on CI-ToD. We use BERT as the base language model to compare with the baselines. The results indicate a great improvement in performance achieved by our method in comparison to the previous baseline methods. Our method outperforms the CGIM model with a significant improvement in F1 score of 5.7% for the HI category, 1.0% for the KBI category, and 2.9% for the overall accuracy. Our model outperforms the direct concatenation methods. For example, compared with BERT, our model achieves a significant increase in F1 score of 6.9% for the HI category, 8.1% for the QI category, 4.1% for the KBI category, and 9.2% for the overall accuracy. It is noteworthy that the previous models only transform the knowledge base into a sequence and concatenate the sequence with the utterance, which often introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base. The redundant information may become noise during model learning, and simply flattening the knowledge base into sequences cannot effectively model relevant information. Our model outperforms the methods that embed the knowledge base into the model parameters. For example, compared with BERT+KE, our model achieves a significant increase in F1 score of 8.0% for the HI category, 1.4% for the QI category, 3.4% for the KBI category, and 4.7% for the overall accuracy. The reason is that these methods train the model on new dialogue data after training on the knowledge base. This tends to suffer catastrophic forgetting and the loss of part of the previously memorized knowledge base information. Whereas our model injects the knowledge base into the model and then only selects relevant knowledge, which avoids this problem and thus improves performance. In addition, we can see that the PLM can perform much better than the non-pretrained model, which is mainly due to the fact that the PLM can better use the knowledge learned in the training process.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1E2GtC4yrJvJnR1o8F5lMg90xr1AMgIy2/view?usp=sharing,Table 1. Comparison of different methods on the CI-ToD dataset. The best result is highlighted in bold.,Table ,Durga Srikari Maguluri
99,"w/o adapter: In this setup, we simply flatten the knowledge base into the sequence and then concatenate the sequence with the current conversation. From the results, we can see that in the CI-ToD dataset, the F1 score of this setup dropped by 7% on the HI category, 7.1% on the QI category, 2.6% on the KBI category, and 9% on the overall score. This shows that only the knowledge base is transformed into a sequence and concatenated with the utterance, which often introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base. This redundant information becomes noise during model learning, and simply flattening the knowledge base into sequences cannot effectively model relevant information.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1VaMzSSEwSVeMwzFaVbJWbgfjmW_Lmb7D/view?usp=sharing,Table 2. Ablation study.,Table ,Durga Srikari Maguluri
100,"We select the adapter with layers 1, 2, 3, and 4, as shown in Table 3. The results indicate that when the number of adapter layers is 1, the performance is the best. From Table 3, we observe that with each additional adapter layer, the parameter size increases by 7.09M. When the adapter layer number is 1, the parameter size of the adapter is 0.05 of the total parameter size. In comparison to the model with 2 adapter layers, our model improves the F1 score by 4.1% on the HI category and 1% on the overall acc. In comparison to the model with 3 adapter layers, our method improves the F1 score by 3.2% on the HI category, 0.6% on the QI category, 0.4% on the KBI category, and 0.7% on the overall acc. In comparison to the model with 4 adapter layers, our model improves the F1 score by 3.6% on the HI category, 0.3% on the QI category, 4% on the KBI category, and 2% on the overall acc. Our analysis indicates that as the number of adapter layers increases, the model’s parameter size and depth also increase. During backpropagation, this may lead to gradient diminishment, hindering the adapter layers from effectively capturing the features of the knowledge base and resulting in a decline in model performance. Additionally, through experiments, we find that no matter how many layers the adapter has, our method outperforms the methods whose knowledge base is flattened into sequence input, which also proves the advantages of our method.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1w3D1xxMhA8AgDz0yNPLbTuT8g3wuspgW/view?usp=sharing,Table 3. Effectiveness of the number of adapter layer.,Table ,Durga Srikari Maguluri
101,"We select hyperparameters of 3, 5, and 10, as illustrated in Table 4. The results indicate that when k  5, the model performs the best.  model outperforms the  model with a significant increase in F1 score of 7.9% for the HI category, 1.6% for the KBI category, and 1.7% for the overall accuracy.  model outperforms the  model with a significant increase in F1 score of 2.7% for the HI category, 1.3% for the QI category, and 1.3% for the overall accuracy. We analyse that when k is small, many knowledge bases related to the current utterance are ignored. When k is large, many knowledge bases unrelated to the current utterance cannot be filtered out. Furthermore, our experiments reveal that irrespective of the value of k, our proposed approach consistently outperforms the model employing a flattened knowledge base as sequential input, thereby highlighting the superiority of our method.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1xXGdtAjul6hdb1SawBD51XdhcBHt4WFC/view?usp=sharing,Table 4. Effectiveness of the topk.,Table ,Durga Srikari Maguluri
102,"Page load speed and usability are crucial in large-scale web applications, as longer load times significantly decrease user engagement and completion of tasks (Nurshuhada et al., 2019, Szalek and Borzemski, 2018). Research indicates that over 40 percent of visitors abandon a website if the page load time exceeds three seconds (Pourghassemi et al., 2019, Stringam and Gerdes, 2019). Therefore, the integration of web analytics tools must be precise to avoid further impacting these factors. Design and software development teams focus on enhancing application performance in response to these insights. The selection of a web analytics tool involves considering data collection methods, location, cost, tracking capacity, real-time data availability, and metrics monitoring. Factors such as data segmentation capabilities, dashboard customization, interoperability, and cost-effectiveness also play a role (Al-Otaibi et al., 2018, Almatrafi and Alharbi, 2023, Boufenneche et al., 2022, Kumar and Ogunmola, 2020). The selection process should align with organizational needs and web application objectives while maintaining user experience and performance standards.",Information Processing & Management,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments - ScienceDirect,https://drive.google.com/file/d/1MkwpTSOtaMVZ04cnD9h-kstpFDQMBvWQ/view?usp=sharing,Fig. 1. Commonly used data collection and user tracking methods for web analytics.,Flow Diagram ,Durga Srikari Maguluri
103,"CAWAL’s framework incorporates an ETL (Extract-Transform-Load) process (Al-Rahman et al., 2023) for data analytics, storing results in data marts within a data warehouse. This setup allows CAWAL to combine an OLTP (Online Transaction Processing) database (Li et al., 2022) for immediate user data processing with a data warehouse for long-term storage. This integration ensures real-time processing and preservation of data for future analysis, offering a dynamic platform suitable for modern web analytics demands. CAWAL employs object-oriented PHP and integrates with a MySQL relational database, creating a robust analytics solution. PHP manages server-side operations such as session management, while MySQL, optimized for high-speed data retrieval, serves as the main database for data storage and query execution (Alya & Ikhwan, 2022). MySQL’s ACID-compliant transactional features ensure reliable data persistence, and its SQL functions facilitate data processing, statistical analysis, and time series evaluation. The architecture also supports distributed capture agents via CAWAL’s API, resulting in a balanced and comprehensive web analytics solution",Information Processing & Management,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments - ScienceDirect,https://drive.google.com/file/d/1e1N4IRIe78cVxpZXY0ZN_SRuvtHJKoYI/view?usp=sharing,Fig. 2. Full model of multi-server supported web analytics framework architecture.,flow Diagram ,Durga Srikari Maguluri
104,"The “log_analytics” table within the framework is another critical component, storing complex analytics data processed for each day in a single tuple. This table includes around 50 fields, with the last six fields consisting of serialized multidimensional data. A daily record in this table amounts to approximately 68 kilobytes, reflecting the extensive scope of analytics extraction performed by the CAWAL framework. This level of detailed data storage underscores CAWAL’s commitment to delivering an efficient and accurate analytics solution, catering to the nuanced needs of web analytics in enterprise environments.",Information Processing & Management,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments - ScienceDirect,https://drive.google.com/file/d/1QYRt3Z_vjfY_z90k3wZz1IFTF-EUO92_/view?usp=sharing,Fig. 3. UML diagram of the logical data model.,Flow Diagram ,Durga Srikari Maguluri
105,"The data collection process involves gathering both qualitative and quantitative information. Qualitative dimensions, such as the client’s browser and operating system, IP address, preceding webpages, channel identification, geolocation, and user profiles, are derived from various sources, including HTTP requests and network protocols. Quantitative metrics, like pageviews and application-specific data, are also obtained from these sources. Integrating these qualitative and quantitative elements into CAWAL entails data cleansing, refinement, and analysis, enabling a comprehensive understanding of user interactions and behaviors. This integrated approach significantly enhances the precision of analytics within the CAWAL framework.",Information Processing & Management,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments - ScienceDirect,https://drive.google.com/file/d/1A-Kgxu0z6EOHQ97Ijg42B4XXHMcoy_4C/view?usp=sharing,Fig. 4. Sequence diagram of client-side (a) and server-side (b) user tracking.,Flow Diagram ,Durga Srikari Maguluri
106,"The data shows that in-house users have higher pageviews per session, and user reflects increased organizational engagement. The predominance of in-country users highlights the site’s significance within the local context, suggesting opportunities for increased localization and user engagement. Additionally, the differentiation between in-house and other categories provides valuable insights into internal operations and how users interact with the system based on location. This information guides managers in making informed decisions on resource allocation, user experience enhancement, and strategic planning tailored to geographic trends and specific organizational needs.",Information Processing & Management,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments - ScienceDirect,https://drive.google.com/file/d/1DogRTkG9NnAN0zlOGJkwX2MLF2yusuTp/view?usp=sharing,"Table 3. Number of sessions and pageviews by in-house, in-country, and out-country.",Table ,Durga Srikari Maguluri
107,"To summarize, our proposed cSEN addresses the challenges posed by noisy syntax parses in ancient Chinese through a novel two-stage method. Initially, cSEN adopts a Biaffine-attention-based mechanism to measure the confidence of derived syntax parses, which stands out from conventional techniques. These confidence scores actively influence syntax encoding by softly masking noisy arcs and illuminating previously undetected ones. In addition, our incorporation of the Left–Right Branch feature is inspired by the nuanced linguistic characteristics of ancient Chinese—attributes largely neglected by prior research. This feature not only extends the encoding scope of the syntax graph but also adeptly captures the local semantics of the language, enhancing the model’s interpretative abilities. Together, these innovations ensure a more robust and accurate representation of ancient Chinese syntax, redressing the prevalent complications tied to harnessing ancient Chinese syntax features.",Information Processing & Management,Confidence-based Syntax encoding network for better ancient Chinese understanding,Confidence-based Syntax encoding network for better ancient Chinese understanding - ScienceDirect,https://drive.google.com/file/d/1VYqawnlGtfozgZjv-a3k1uV-Tet4DkRL/view?usp=sharing,"Fig. 2. Architecture of the proposed cSEN. The symbol  represents the concatenation operation, while  denotes the gated mechanism. We depict  as a graph with arcs pointing from heads to dependencies. In , cells are colored to emphasize local dependencies; darker shades indicate stronger correlation.",Flow Diagram ,Durga Srikari Maguluri
108,"For the ancient–modern Chinese translation, we adopt the ancient–modern Chinese parallel corpus contributed by the open-source NiuTrans project.3 The corpus contains 967,255 sentence pairs extracted from ancient Chinese books. We divided the corpus into training, validation, and test sets with corresponding sizes of 900,000, 60,000, and 7255.",Information Processing & Management,Confidence-based Syntax encoding network for better ancient Chinese understanding,Confidence-based Syntax encoding network for better ancient Chinese understanding - ScienceDirect,https://drive.google.com/file/d/1iOFV8_0D4dfc7fr3dvXKtL1LrVFiBc1Y/view?usp=sharing,Table 1. Data statistics of the ancient Chinese poetry thematic classification dataset.,Table ,Durga Srikari Maguluri
109,"The baseline model without syntax parses achieves 37.14 in BLEU score and F-scores of 69.71, 46.24, 67.62 in ROUGE-1, ROUGE-2, and ROUGE-L respectively. With single syntactic parses incorporated, all models achieve better performance in all metrics, proving that syntax can effectively improve ancient–modern Chinese translation. LRB is relatively the weakest one, slightly increasing BLEU score by 0.28, and ROUGE f-scores by 0.15, 0.12, 0.10. This might be caused by that sentences from ancient books have more long-distance dependencies and more complicated syntactic structures that left–right branch cannot recover. Anchi-BERT-derived syntax parses have better performance with an improvement of 0.41 in BLEU score, and 0.19, 0.29, and 0.23 in ROUGE scores. BERT-wwm derived syntax trees and trees generated by Biaffine parser have similar results. In contrast to Anchi-BERT derived trees, their performance is inferior in BLEU scores but better in ROUGE F-scores. Feeding multiple syntactic parses into the GAT-based model simultaneously leads to a significant performance drop. While replacing GAT with the proposed cSEN increases performance in all matrices, with 37.73 in BLEU score and 70.27, 47.09, 68.23 in ROUGE F-scores. From the above results, we conclude that syntax parses from unsupervised derivation or modern Chinese syntax parsers introduce noise and degrade model performance. With our confidence learning, model is able to distinguish and separate informative syntactic information from noise, thus alleviating its negative effect.",Information Processing & Management,Confidence-based Syntax encoding network for better ancient Chinese understanding,Confidence-based Syntax encoding network for better ancient Chinese understanding - ScienceDirect,https://drive.google.com/file/d/1hGPfx9arLjKMN7mIA2wakfmnttCxeqMn/view?usp=sharing,Table 3. Experimental Results of the ancient and modern Chinese translation task.,Table ,Durga Srikari Maguluri
110,"Second, we experiment with different syntax tree combination setups. Table 6 lists the findings. Combining two syntax parsers onto a single dependency graph can provide richer syntactic information and produce higher performance than using alone syntax parses, no matter if it is from unsupervised derivation or a supervised parser. We also explore the incorporation position of LRB features. It can be observed that the model performance suffers significantly if LRB features and graph parses are directly merged together. This again indicates the necessity of our gated method for LRB feature integration.",Information Processing & Management,Confidence-based Syntax encoding network for better ancient Chinese understanding,Confidence-based Syntax encoding network for better ancient Chinese understanding - ScienceDirect,https://drive.google.com/file/d/1kEXt5wuDv_b4qwg50rlpZPYl8TXLlUMO/view?usp=sharing,Table 5. Ablation study results.,Table ,Durga Srikari Maguluri
111,"BiGraph Block. Denote  as the input of BiGraph Block in HWTIN. Denote  as the input of BiGraph Block in our baseline (without HBM and IHBM). Following Tang, Bai, Torr, and Sebe (2020), in this study, the Interaction-and-Aggregation (IA) and Bipartite Graph Reasoning blocks are used. IA blocks are used to interactively improve a person’s physical characteristics, while BGR blocks can explain the crossing of long-range relationships between  and . More details can be found in Tang, Bai, Torr, and Sebe (2020).",Information Processing & Management,Haar-wavelet based texture inpainting for human pose transfer,Haar-wavelet based texture inpainting for human pose transfer - ScienceDirect,https://drive.google.com/file/d/1RAKABWxxQZzlxmHTs8U3OxShA5Elmdyq/view?usp=sharing,Fig. 2. The structure of the proposed method.,Flow Diagram ,Durga Srikari Maguluri
112,"In DeepFashion, due to the lack of background, there were no evaluation indicators in this experiment, Mask SSIM and Mask IS. SSIM, IS, DS, and the PCKh value slightly decreases compared to the baseline network. All indicators of Full are somewhat higher than baseline and HWTIN- Improvement of. The above experiments further demonstrate that combining the HBM and IHBM modules can achieve attitude transition. This paper believes that this is due to the loss of high-frequency information in the down-sampling process of images, and the proposed IHBM can utilize this high-frequency information during the up-sampling process, thereby learning more comprehensive graphs slice features to generate more realistic human body images.",Information Processing & Management,Haar-wavelet based texture inpainting for human pose transfer,Haar-wavelet based texture inpainting for human pose transfer - ScienceDirect,https://drive.google.com/file/d/1-iLcq5Sks4IVYkUw7KW4Xphd2y3bBN3m/view?usp=sharing,Table 3. Quantitative comparison of the ablation study.,Table ,Durga Srikari Maguluri
113,"For the purpose of evaluating the efficacy of the proposed ESRFS approach, our experimental assessment involved the utilization of a comprehensive set of fifteen data sets sourced exclusively from the MULAN Library (Tsoumakas, Spyromitros-Xioufis, Vilcek, & Vlahavas, 2011). In addition, we adapt the segmentation scheme in the literature (Tsoumakas et al., 2011) and Zhang and Zhou (2010) to divide data sets. As an example, the Yeast data set comprises 2417 phylogenetic profiles of yeast genes, where each yeast gene is associated with a distinct subset of 14 annotations. the data set Emotions (also called Music) have 593 songs and 6 labels, such as relaxing-calm, sad-lonely and amazed, etc. A comprehensive overview of all employed multi-label data sets is presented in Table 3, providing detailed descriptions for each of them.",Information Processing & Management,Multi-label feature selection with high-sparse personalized and low-redundancy shared common features,Multi-label feature selection with high-sparse personalized and low-redundancy shared common features - ScienceDirect,https://drive.google.com/file/d/18Dad9VxF_oUg40EHUTCeRFoITlObWFuS/view?usp=sharing,Table 3. Elaborated information regarding the experimental data sets.,Table ,Durga Srikari Maguluri
114,"Next, Fig. 2 is employed to examine the classification performance of the compared methodologies. Fig. 2 demonstrates the relationship between the number of selected features (referred to as top-m% features in this study) and the classification performance of all compared methodologies. The horizontal -axis represents the number of selected features, while the vertical -axis represents the classification performance measured by Hamming Loss and Zero-One Loss (MLkNN), Micro- and Macro-(SVM), and Micro- and Macro-(3NN), respectively. Notably, Fig. 2 shows the classification performance on several representative data sets, namely Arts, Education, Flags, and Health. By these data sets, we can show and analyze experimental results, where the first two sub-graphs in each row indicate the results of HL (MLkNN) and ZOL (MLkNN), while the last four sub-graphs in each row indicate the results of Micro- and Macro- (SVM), Micro- and Macro- (3NN).",Information Processing & Management,Multi-label feature selection with high-sparse personalized and low-redundancy shared common features,Multi-label feature selection with high-sparse personalized and low-redundancy shared common features - ScienceDirect,https://drive.google.com/file/d/1rNK2NvFiw70g1i0aZ-X7HrQQrXpQvb5D/view?usp=sharing,Table 10. The Friedman  statistics and corresponding critical values.,Table ,Durga Srikari Maguluri
115,"Modeling interest trend and diversity together lead to performance degradation. Note that the user’s interests could be decomposed into the combination of (1) interest trend; and (2) interest diversity. In particular, the interest trend reveals the user’s long-term interest evolution process that will not shift sharply as time goes by and encourages similar recommendations, while interest diversity, tends to explore a user’s various point-of-interests and prompt more diverse recommendations. Therefore, modeling the objective-conflict factors and optimizing the orthogonal aims, i.e., the interest trend and interest diversity, simultaneously without disentanglement will lead to performance deterioration.Eliminating the diverse items results in significant information loss. To avoid the above performance degradation, existing methods treat the interest diversity as noise (Fan et al., 2022, Sun et al., 2021, Wang, Zhang et al., 2022) and remove the diverse and irrelevant items sloppily. However, we argue these items could be attributed to external environmental factors, e.g., emerging fashion trends, advertisement and marketing, and sharing or recommendation from friends, thus, they may also represent the user’s potential interests that could facilitate the point-of-interest exploration.",Information Processing & Management,Disentangle interest trend and diversity for sequential recommendation,Disentangle interest trend and diversity for sequential recommendation - ScienceDirect,https://drive.google.com/file/d/1UAok3IO3Mt1Xsn5TTsVSJy85lpLsh59E/view?usp=sharing,"Fig. 1. The user historical interactions can be split into two parts: (1) major interest trend (red bracket); (2) interest diversity (green bracket). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)",Flow Diagram ,Durga Srikari Maguluri
116,"where  is the final likelihood estimation vector of user  generated by the prediction aggregation function . In addition,  and  are the interest trend modeling module and the interest diversity modeling module respectively",Information Processing & Management,Disentangle interest trend and diversity for sequential recommendation,Disentangle interest trend and diversity for sequential recommendation - ScienceDirect,https://drive.google.com/file/d/1alVcXUD1b-Hi5XQ6vrZX8Gyl5PQOAxlJ/view?usp=sharing,Fig. 2. The network architecture of our proposed Teddy.,Flow Diagram ,Durga Srikari Maguluri
117,"We, thereby, utilize four public datasets commonly used in the relevant literature for the overall performance comparison.Amazon Beauty, Toys, Sports2 are collected from Amazon platform which contains numerous user–item interaction behaviors spanning from May 1996 to July 2014 (McAuley et al., 2015). Here, we choose the two most commonly used categories with different scales for model evaluation: Amazon Beauty, Amazon Toys and Amazon Sports.Steam3 is obtained from an online video game website. The dataset collectes reviews from 334,730 users towards 13,047 games spanning from October 2010 to January 2018 and other external information (e.g., media score, users’ play hours, games’ prices and categories, publisher and developer information).Following Kang and McAuley, 2018, Li et al., 2020 and Wang, Wang et al. (2022), we regard the record of a rating or review as the user’s implicit feedback, i.e., an interaction between item and user. Hence, for all the datasets, we filter inactive users and unpopular items whose interaction records are less than five. After that, for each user, we organize all of his or her historical interaction records chronologically as a sequence based on the item timestamp information associated with each interaction. Furthermore, we apply the leave-one-out protocol for all the datasets for performance evaluation. More concretely, given a sequence we obtain the last interaction for model testing, the penultimate interaction 
 for validation, and the remains for training. The statistics of the aforementioned four datasets are summarized in Table 1. We could observe that an interaction sequence has a shorter average length (8.53) on Amazon Beauty dataset, while Steam dataset has a longer length (12.40) instead.",Information Processing & Management,Disentangle interest trend and diversity for sequential recommendation,Disentangle interest trend and diversity for sequential recommendation - ScienceDirect,https://drive.google.com/file/d/1NhjYvgkEOquNynyOF1znzsCME1HGKr5Q/view?usp=sharing,Table 1. Statistics of four preprocessed datasets. Avg_len: the average length of an interaction sequence.,Table ,Durga Srikari Maguluri
118,"mpact of . As aforementioned described, we disentangle interest trend and interest diversity and model them in a holistic method. And the final prediction results are determined by these two modules (ref (10)). To instantiate the idea, we define a coupling factor  to arrange the allocation of proportions for each part. Therefore, we analyze the impact of  under the set of . It is noteworthy that when , the model degrades to w/o TRD and if  closes to , the model is equivalent to . Fig. 6 illustrates the variation of  to the performance of Teddy. We could observe that moderate values (i.e.,  or 0.6) are beneficial to the recommendation results, whereas, a small value will cause the model performance to have a sharp decrease, particularly on Amazon Beauty dataset. This phenomenon is also consistent with the experiment results of  (Fig. 5), i.e., the model performance will diminish significantly when the information from user’s interest",Information Processing & Management,Disentangle interest trend and diversity for sequential recommendation,Disentangle interest trend and diversity for sequential recommendation - ScienceDirect,https://drive.google.com/file/d/1tcjuZ1Nz_SgIMLi9iM-_6FQc2TuDS_T-/view?usp=sharing,"Table 4. Teddy performance over various . 2nd and 3rd means the second and third of the last item respectively. The best results are highlighted in boldface, and the second-best results are underlined.",Table ,Durga Srikari Maguluri
119,"Moreover, DAs can also help determine whether utterances are salient. Some DAs are naturally associated with salient utterances. For example, Turn 348 (marked as D and S) is salient, where Grad A disrupts (D) the dialogue and states (S) the important problem that “zero” can be pronounced as “zero” and “O”. This utterance leads to subsequent discussion, and it is more likely to be extracted. Conversely, some DAs are always associated with uninformative utterances. For example, Turn 349 (marked as B) only contains one word and is less informative. Therefore, we use DAs to help measure the salience of an utterance. DAs are also beneficial for the integration of extracted utterances. For example, when generating a summary, it is intuitive to merge a question utterance (like Turn 350) and its answer utterance (Turn 352) into a declarative sentence. Specifically, we use DAs as prefix prompt to guide how to integrate extracted utterances into a summary.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/13132hJSeYfvCBggzvfPBvkoWGjQXu0BB/view?usp=sharing,Fig. 1. An example of query-based meeting summarization with dialogue acts (DAs). The utterances with red DA labels are extracted to generate a summary. DA labels: Q links to questions; B is used for backchannel or responses; F gets involved in the floor; D marks disruption utterances; S is related to statement utterances.,Flow Diagram ,Durga Srikari Maguluri
120,"Considering limited annotations of DAs for meeting transcripts, we propose to predict DAs using the contextual representation of utterances. It allows us to predict DAs on unseen data. Besides, supervised by DAs the RoBERTa model can get aware of interactive signals in the conversation to better understand meeting transcripts and produce better contextual representation for utterances. We formulate DAs prediction as follows:4.2. Dialogue acts enhanced extract–abstract framework DAs are helpful for finding salient utterances from meeting transcripts (Di et al., 2020, Oya and Carenini, 2014), so we use predicted DAs distributions to enhance the extractor. We build the relationship between salient utterances and DAs by updating the salience score (in Eq. (2)) as follows:Please note that there is an information loss that the abstractor (BART) only takes one utterance as input to get vocabulary distribution and dynamic weight following Lewis, Perez, et al. (2020) and Mao et al. (2022). In this paper, we propose to use DAs to reduce information loss since DAs can provide interactive functions for each utterance. For example, it is natural to find feedback utterances after incorporating a question utterance into a summary.Inspired by the success of Li and Liang (2021) and Liu et al. (2022), we use DAs as prefix prompt to guide how to integrate extracted utterances in the abstractor. We proposed a prefix prompt attention mechanism to add the DAs information as illustrated in Fig. 3. DAs are used to calculate the extra self-attention keys and values, which will be concatenated with the original keys and values from the input utterance. Consequently, BART encoder will generate a hidden state aware of DAs information. Please note that different MLP is used to obtain keys and values for different layers and heads, so DAs can be expected to play different roles in different layers and heads. The key (or value) of a layer (or a head) can be calculated via a separate MLP as followswhere 
 is the prompt key (or value) for a layer (or a head), and  is the DAs embedding matrix. The architecture comprises three layers, wherein the ReLU activation function is employed across each layer. It should be noted that this process only imposes a slight impact on the original BART pre-trained parameters, so we argue that this method can well preserve the pre-trained knowledge.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1dKAhpj2mZjWtrvFa8laZNKGNO9KVXyvD/view?usp=sharing,"Fig. 2. Workflow of our DASum. A meeting transcript is divided into  chunks, each containing an optional query and consecutive utterances. Each chunk is independently encoded in the extractor by a shared RoBERTa model. The utterance representation  and DAs distribution  are used to calculate salience score , according to which the extractor extracts  utterances. In the abstractor, the optional query and each extracted utterance, with its DAs distribution as prefix prompt, are used in BART to get vocabulary distribution and dynamic weight. The final output distribution is the weighted sum of the generated vocabulary distributions with their dynamic weights.",Flow Diagram ,Durga Srikari Maguluri
121,"where  is the prompt key (or value) for a layer (or a head), and  is the DAs embedding matrix. The  architecture comprises three layers, wherein the ReLU activation function is employed across each layer. It should be noted that this process only imposes a slight impact on the original BART pre-trained parameters, so we argue that this method can well preserve the pre-trained knowledge.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1LSGubX3O5dyhIpM1Kt6uQKGpC7gSwQqW/view?usp=sharing,"Fig. 3. Transformer Attention v.s. Prefix Prompt Attention. The left part is the original attention of the transformer. The right part is our prefix prompt attention, where the output can contain the dialogue acts information via extra keys and values (red blocks or dialogue acts blocks). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)",Flow Diagram ,Durga Srikari Maguluri
122,"ICSI is a meeting summarization dataset comprising naturally-occurring academic meetings recorded at the International Computer Science Institute (Janin et al., 2003). For reproducibility, we split the train, validation, and test set consistent with the setup of Zhong, Liu, et al. (2021).1 For evaluation, we report Rouge scores and BERTScore.QMSum is a query-based meeting summarization dataset (Zhong, Yin, et al., 2021). It gets involved in meetings from three domains, including the product meetings of AMI (Carletta et al., 2006), the academic meetings of ICSI (the above dataset), and the committee meetings of the Welsh Parliament2 and Parliament of Canada.3 English experts write each query and answer. To ensure reproducibility, we employ train, validation, and test splits in our experimental setup following Mao et al. (2022).4 We evaluate the generated summary quality using Rouge scores and BERTScore.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1bkCpgU3-TifB-6yDBG2PKqE2ISjA3JUb/view?usp=sharing,Table 1. Statistics of datasets.,Table ,Durga Srikari Maguluri
123,"We want to know whether the limited performance stems from our model architecture. So we report the result (DASum - joint in Table 5), where our model is trained without using QMSum. It achieves powerful performance with a small margin (0.6) to the best method. It means that our model architecture can deal with DAs classification. And joint training with QMSum slightly damages the performance of DAs classification.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1Ap28unfH_1qQBYNL6WmvB3Q_9yD-RGro/view?usp=sharing,"Table 5. Accuracy on MRDA. Results with  come from He et al. (2021), and other results (DASum and DASum - joint) come from our implementation.",Table ,Durga Srikari Maguluri
124,"We share our interest in exploring the effects of dialogue acts (DAs) on the performance of the extractor and abstractor models, as we believe that DAs can benefit both components. In order to assess the impact of DAs on the extractor, we conducted an experiment to compare the ROUGE scores of extracted utterances. The results are presented in Table 6. The findings from our experiment indicate that our DASum model outperforms the backbone model (DYLE) in terms of ROUGE scores for extracted utterances. This outcome suggests that incorporating DAs can indeed contribute to the effective extraction of salient utterances. By leveraging the interaction information provided by DAs, the extractor model is better equipped to identify the most relevant and informative utterances from the dialogue.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1Eo2eOLLZ2DyMdBhRNNfjqU5yMMifUoh8/view?usp=sharing,Table 6. ROUGE scores of extracted utterances on different datasets compared to our backbone model (DYLE).,Table ,Durga Srikari Maguluri
125,"We conducted experiments to evaluate the performance of our models on different domains of the QMSum dataset. The results of these experiments are presented in Table 8. Comparing our DASum model with the backbone model (DYLE), we observe that DASum achieves the most significant improvement (approximately 3.9 on ROUGE-1) on the committee domain. At the same time, it only yields a gain of around 2.3 on the academic domain and achieves similar performance on the product domain. It can be attributed to the complexity of dialogue interactions. As indicated in Table 1, meetings in the committee, academic, and product domains involve an average of 34.1, 6.3, and 4.0 speakers, respectively. With a higher number of speakers, the committee domain exhibits more intricate and intricate interactions compared to the other domains. Given that DASum leverages dialogue interactions to improve its understanding and utilization, it is natural that the committee domain, with the most speakers and complex interactions, benefits the most from our approach, resulting in the largest improvement in performance.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1cf-4xlo5nYLAi88-8UcByWvX_9VjFGA3/view?usp=sharing,Table 8. ROUGE scores on different domains of QMSum compared with our backbone model (DYLE).,Table ,Durga Srikari Maguluri
126,"We are interested in the effect of prefix prompt length. We conducted experiments on the QMSum dataset, varying the prefix prompt length from 1 to 7, and evaluated the results in terms of ROUGE scores. The findings are summarized in Table 9. Based on the results, we observe that the ROUGE scores generally increase as the prefix prompt length increases up to a certain threshold (in this case, 5). This improvement can be attributed to the fact that a longer prefix prompt contains more trainable parameters, providing the model with increased expressive power. Consequently, the model is better able to capture the underlying patterns and nuances in the data, leading to improved performance. However, it is important to note that there is a slight drop in performance beyond the threshold of 5. This decline can be attributed to the phenomenon of overfitting, wherein a model becomes too specialized to the training data and fails to generalize well to unseen data. A longer prefix prompt increases the risk of overfitting, as the model may excessively memorize specific patterns in the training data, thereby hindering its ability to generalize to new examples. Therefore, there exists a trade-off between the benefits of a longer prefix prompt in terms of improved performance and the risk of overfitting. It is crucial to strike a balance and select an optimal prefix prompt length that maximizes performance without sacrificing generalization ability.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1Svy_vSLwjxKzQbCVwnFcxSIKNtwaAQ8O/view?usp=sharing,Table 9. ROUGE scores with different prefix prompt lengths on QMSum.,Table ,Durga Srikari Maguluri
127,"The results of the human evaluation, as presented in Table 10, clearly demonstrate that DASum outperforms DYLE in terms of Readability, Conciseness, and Coverage on both the ICSI and QMSum datasets. The higher scores achieved by DASum indicate that the generated summaries are more readable, concise, and effectively cover the important information compared to those produced by DYLE.",Information Processing & Management,Dialogue acts enhanced extract–abstract framework for meeting summarization,Dialogue acts enhanced extract–abstract framework for meeting summarization - ScienceDirect,https://drive.google.com/file/d/1JNCQt0zptjgoE0jgeZV_Q6wcmkbgRp23/view?usp=sharing,Table 10. Human evaluation scores.,Table ,Durga Srikari Maguluri
128,"In addition to accuracy, commonly used evaluation metrics in binary classification, such as precision, recall, and F1, can also be applied to multilabel classification problems by computing averages. The label averaging strategies of macro- and micro-averaging are commonly used. Macro-precision, -recall, and -F1 are obtained by computing and then averaging the corresponding metrics for each label. Micro-precision, -recall, and -F1 combine the prediction results of all single-label samples with the true labels before calculating the corresponding overall metrics.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/1ccrCsji5Tp7bVFNjnVhx2e7mu51aCdaf/view?usp=sharing,Fig. 1. Flowchart of cultural type identification of tourism resources based on PLMs.,Flow Diagram ,Durga Srikari Maguluri
129,"Accurate identification of cultural types embedded in cultural tourism resources is the bridge that connects culture and tourism resources. However, this is far from satisfying the needs of cultural tourists. A large amount of knowledge is needed to support pretour decision-making, tour experience, and posttour sharing. The construction of a KG of tourism resources from a cultural perspective is necessary, and the KG is stored using a graph database. Although relational databases can meet daily needs with a small amount of data, as the KG becomes more complex, the advantages of graph databases increase. Especially for second- or third-degree correlation queries, the efficiency of graph databases will be thousands or even millions of times that of relational databases. The construction of a cultural tourism KG includes ontology design, data collection, knowledge acquisition, knowledge fusion, and knowledge management, as shown in Fig. 3.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/17UGyEax9978GDMLVBZR8e35qE_YCtkEJ/view?usp=sharing,Fig. 3. Flowchart of CuPe-KG construction.,Flow Diagram ,Durga Srikari Maguluri
130,"The ontology is meticulously designed with the following details:(1) Name: Notable tourism resources typically have multiple names associated with them. Beyond the standard names, there are often popular or short names that are commonly used to facilitate easy dissemination and frequent appearance in the descriptive texts of tourism resources. For instance, the standard name ‘Xiamen Kulangsu Scenic Area’ is often colloquially referred to as ‘Kulangsu.’ Other descriptions of this resource have previously included names such as ‘Yuanshazhou’, ‘Yuanzhouzi’, and ‘Wulongyu.’ (2) Location: This encompasses a comprehensive array of geographical identifiers pertinent to the tourism resource, including the province, city, and county in which it is situated, as well as a detailed address. Additionally, the inclusion of longitude and latitude coordinates is indispensable to cater to the requisites of downstream applications that necessitate location-based services. (3) Information: This encompasses fundamental details pertaining to the tourism resource such as ticket prices and contact information, including email addresses, phone numbers, and social media accounts. Extended attribute information primarily involves resource attributes obtained from existing KGs, which are continuously updated with the iteration of the KG versions. (4) Suggestion: This includes suggestions such as the recommended season, recommended tour length, opening hours, travel hotness index, and key opinion words or phrases. These elements exert a significant influence on tourists’ choices of destinations and the planning of their itineraries. (5) Rating: Tourism ratings serve as a crucial metric for gauging the influence of a tourism resource. In China, national-level tourist sites are graded from 1A to 5A. Meanwhile, cultural heritage sites and intangible cultural heritages are primarily classified according to tiers, such as world-level, national-level, provincial-level, city-level, and county-level. (6) Culture: From a cultural perspective, the ontology design considers the cultural types of tourism resources, which can be identified using the method described in this paper. Cultural types can also be fine-grained, according to regional characteristics. (7) Category: Most studies on the design of KG ontologies for tourism are based on tangible cultural tourism resources. Intangible cultural heritage is considered an integral part of cultural tourism. We also outline the established classification of resources into 4 major categories and 35 subcategories. The four main categories are natural landscapes, human landscapes, artificial landscapes, and intangible cultural heritage. Tangible cultural heritage is categorized in accordance with the National Tourism Resource Classification Standards of China. Subcategories of intangible cultural heritage are structured following the national basic classification system. (8) Historical figure & inheritor: Cultural heritage is related to individual historical figures, who become the focus of cultural tourism. The inheritors associated with intangible cultural heritage are also elements to be considered in cultural tourism ontology. The relationship between figures and tourism resources is an important space-time link in cultural tourism. (9) Historical event: The historical events associated with tourism resources are also valuable elements of cultural tourism, as a basis for planning routes, the guiding direction of resource development, and an important driver of tourism marketing.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/1hiNXSR37jKYJv2THGMW0uzKusgJt38Nx/view?usp=sharing,Fig. 4. Ontology of tourism resources from a cultural perspective,Flow Diagram ,Durga Srikari Maguluri
131,"Primarily, this paper evaluates the performance of the FT-ERNIE and ChatGPT methods for cultural type identification, employing the tourism resources annotation dataset. For FT-ERNIE, we use the training set to fine-tune the ‘ernie-3.0-xbase-zh.’ The hyperparameters of the model were tuned using the validation set. The final fine-tuned pivotal hyperparameters used by the model are shown in Table 3.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/1E5RATyk8rB9S2Hm-mz08dz81hgLhY5Hc/view?usp=sharing,Table 3. Hyperparameter settings for FT-ERNIE.,Table ,Durga Srikari Maguluri
132,"Upon evaluation of the textual description lengths within the annotated dataset, it was determined that a vast majority (97 %) of tourism resource descriptions did not exceed 512 characters. The ‘max_seq_length’ parameter was established at this value. Accounting for the computational capabilities of the platform used, the ‘batch_size’ parameter was set at 4. To facilitate an optimal ‘learning_rate’, a parameter value of 3e-5 was implemented. To stave off model overfitting, the ‘weight_decay’ parameter of 0.01 was introduced. An initial ‘epochs’ setting of 200 was determined, with the cessation of model training contingent upon observable shifts in micro-F1 evaluations. The accuracy evaluation of cultural type identification was performed through the test set based on the fine-tuned model. The Baseline employs the BERT model (‘bert-base-chinese’), with its hyperparameters for model fine-tuning consistent with those of FT-ERNIE. We also used ChatGPT for direct cultural type identification of tourism resources through prompt words. The results of the model evaluation are shown in Table 4.FT-ERNIE outperforms Baseline across all evaluation metrics, highlighting its powerful performance. Specifically, FT-ERNIE's micro-F1 and macro-F1 scores are 12 % and 13 % higher, respectively, compared to Baseline. FT-ERNIE is also more accurate than ChatGPT. In the multilabel classification evaluation, the accuracy is strict, and those of FT-ERNIE and ChatGPT are 0.81 and 0.12, respectively. The micro-precision of FT-ERNIE is 0.94, 40 percentage points higher than that of ChatGPT. The micro-recall of FT-ERNIE is 0.92, which is 3 percentage points higher than that of ChatGPT. The micro-F1 of FT-ERNIE is 0.93, which is 26 percentage points higher than that of ChatGPT. FT-ERNIE's macro-precision, macro-recall, and macro-F1 are also higher than those of ChatGPT. The above results show that the FT-ERNIE method has the ability to support the identification of cultural types of tourist resources well, while the ChatGPT method needs further research.ChatGPT performs slightly worse than the fine-tuned model in the area of cultural tourism type classification because it cannot be fine-tuned. Another reason that cannot be ignored is that the corpus of Chinese descriptions of tourism resources in Fujian Province, China, may be sparse during the training of ChatGPT. In the ChatGPT results, the micro-recall (or macro-recall) metric is significantly higher than the micro-precision (or macro-precision) metric. This indicates that ChatGPT has a wider range of cultural type identification. Clearer constraints are needed to improve the final identification performance. The stability of the model is evaluated, and ChatGPT has the problem of inconsistent recognition results for the same tourism resource several times during use due to the use of prompt words. The stability of the recognition results of FT-ERNIE is better.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/1yqWvS2XOeOEexz8axIj3OT1Qj7axCVR_/view?usp=sharing,Table 4. Cultural type identification model evaluation results.,Table ,Durga Srikari Maguluri
133,"The highest F1 is for FT-ERNIE results of Tower & Bridge culture. The reason may be that the text description of the tower or bridge is clear and easy to judge. The identification accuracy of the pretrained method is also higher. Other similar types of culture include Intangible, Ecological, and Architectural culture. The identification result is poor for Shipbuilding culture because the total number of tourism resources annotated for Shipbuilding culture is small, and the number used for model training is only 7. The identification ability of the model after fine-tuning is weak. The number of tourism resources used for testing is 4, and the evaluation indices will have a large oscillation because of the small number. It is noteworthy that while the training and testing samples for the Hakka culture exceed those of the Mazu culture, FT-ERNIE demonstrates better recognition accuracy for the latter. This heightened accuracy for Mazu culture can be attributed to its centralized focus on the deity Mazu, which possesses a distinct and clear identity. However, its performance still falls short of the Tower & Bridge culture due to the Mazu culture's relatively smaller training sample size. In contrast, the Hakka culture spans diverse domains, from architecture and language to customs and music, leading to its more challenging recognition. Consequently, the efficacy of PLMs in distinguishing cultural types is influenced by both the cultural content's distinctiveness and the size of the annotated dataset. Regarding the erroneous samples, we initially observed the quantity of labeled and predicted labels in the FT-ERNIE error cases. Among the 107 errors, 39 had more predicted labels, 23 had equal, and 48 had fewer than the actual labels, showing ERNIE's conservative tendency, with most errors due to missed labels. Specifically, in Shipbuilding culture, despite explicit descriptions, some weren't identified due to limited fine-tuning data, such as Mawei Shipbuilding Tianhou Palace and Fuzhou Maxian Mountain Park. For the recognition of Hakka cultural, many cases were not identified if the term ‘Hakka’ was not accurately articulated in the description text. Particularly, many ancestral halls involving extensive descriptions of Han ethnicity's sacrificial customs—a typical feature of Hakka culture—were not identified, such as the Shanghang Li Clan Ancestral Hall and the Liancheng Zhixi Ancestral Hall. We also observed instances involving Tower & Bridge culture that were not identified accurately . For example, in the case of Zheng He Park in Changle, which includes the ‘Sanfeng Tower,’ the cultural type was not correctly recognized.",Information Processing & Management,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models,CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models - ScienceDirect,https://drive.google.com/file/d/1LYToEQO8mxuOxBlPADwXAbY2p1lN0fMW/view?usp=sharing,Table 5. Statistical significance test results for the cultural type identification model.,Table ,Durga Srikari Maguluri
134,"TKG extrapolation is concerned with the prediction of new facts according the existing temporal information. Specifically, accurately predicting future facts requires inferring the patterns of behavior and preferences exhibited by entities and relations based on their historical interactions. Previous works have attempted to learn the local perspective for predicting future facts, which leverage structural information within subgraphs by employing recurrent graph neural networks (GNN). For example, RE-GCN (Li et al., 2021) focuses on learning entity representations with evolution information by modeling sub-graph sequences of recent timestamps. EvoKG (Park et al., 2022) aims to model the local structural and temporal dynamics in TKGs through recurrent event modeling. To utilize valuable temporal information, Han, Ding et al. (2021) integrate the hidden representations over time using an ordinary differential equation solver to obtain dynamic entity representations. HGAT (Shao et al., 2023) introduces a hierarchical attention encoder to effectively absorb valuable knowledge from relevant historical facts. However, due to the limitations of RNNs and computational complexity, these works fail to utilize long-range historical facts. Taking the example shown in Fig. 1, from the local perspective, events on 2015-01-02 and 2015-01-03 suggest a close political relationship between Australia and Malaysia, which significantly influence Australia’s action on 2015-01-09. On the other hand, the facts of 2007 underline that the Asian Cup in Malaysia promotes the development of tourism. Despite these occurrence facts happen in 2007 with the long time ago, they still significantly affect the events of Australia in 2015. Consequently, accurately modeling and distinguishing local and global perspectives of entities and relations is critical",Information Processing & Management,Learning dual disentangled representation with self-supervision for temporal knowledge graph reasoning,Learning dual disentangled representation with self-supervision for temporal knowledge graph reasoning - ScienceDirect,https://drive.google.com/file/d/1YwZlu80l7mDka6K4CnaU6Xw11YEOoF6E/view?usp=sharing,"Fig. 1. An illustration of the local and global perspective representations over TKG. The yellow line indicates the local perspective representation while the blue line indicates the global perspective representation . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)",Flow Diagram ,Durga Srikari Maguluri
135,"To investigate the contributions of each component to TKG reasoning, Table 2 presents the results of the ablation study. The notation “w/o local” refers to the local perspective invalid in our DLGR, while “w/o global” represents our model DLGR without the global perspective. Additionally, “w/o disentangle” indicates that the self-supervised disentanglement framework is removed, and the “w/o TRAN” denotes the two-level relation-aware attention network replaced by relational graph convolution network. From Table 2, it is evident that whenever we remove a type of perspective, the performance of model declines to a certain degree. Specifically, on ICEWS14 dataset, the model variant “w/o global” exhibits a discernible decrease in performance, which indicates our representation learning on the global perspective are more reasonable and effective to depict more advanced characteristic of entities. Furthermore, even when the local perspective is removed, “w/o local” also achieves the comparable performance on all four datasets. The reason may be that both local and global perspectives are disentangled, enabling the global perspective to maintain a significant contribution. By comparing DLGR with two variants, the results further demonstrate that the disentanglement of these two perspectives is the key factor for improved reasoning. By comparing DLGR with “w/o disentangle” variant, the results verify our claim that unsupervised disentanglement of local and global perspectives is insufficient, and DLGR effectively addresses the challenge of lacking labeled data in a self-supervision way. Moreover, “w/o TRAT” demonstrates inferior performance compared to the proposed DLGR model. This result can be attributed to the effective learning of multiple types of relation-specific representations. It highlights the more important semantics correlated with query relations, leading to an overall improvement.",Information Processing & Management,Learning dual disentangled representation with self-supervision for temporal knowledge graph reasoning,Learning dual disentangled representation with self-supervision for temporal knowledge graph reasoning - ScienceDirect,https://drive.google.com/file/d/1U8ao98jhWySYk79mBBE6JTQmtzkUKSTl/view?usp=sharing,"Fig. 2. The architecture of our proposed DLGR. (A) construction of temporal subgraphs and temporal unified graph, based on the temporal KGs; (B1) local perspective representation on temporal subgraphs; (B2) global perspective representation on the temporal unified graph; (C) contrastive learning on the similarity between representation and proxies of local and global perspectives to enhance disentanglement.",Table,Durga Srikari Maguluri
136,"In terms of causality features recognition, knowledge-templates based approaches using predefined linguistic rules or patterns to identify the candidate causal instances directly, other systems employ a set of procedures or heuristic algorithms on the syntactic structure of sentences, which can simply and effectively extract information from the text and perform well in restricted domains (Sorgente et al., 2013). Therefore, in this step, the scheme for the extraction of causes and effects can be constructed as follow (see Fig. 1): Firstly, we manually defined suits of patterns according to the linguistics characteristics of Chinese causal words and syntax to check if a piece of review contains a causal feature. Then, if a causal relation was found the review was split and a set of rules was applied to extract the cause-effect sentences.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/1ZL_R8tO54MeKSrk8LkCQJJDovHuORXBR/view?usp=sharing,Fig. 1. Scheme for the extraction of causes and effects.,Flow Diagram ,Durga Srikari Maguluri
137,"Causality is usually labeled as a triples of (cause, effect, signal). The ‘cause’ and ‘effect’ represent cause events and result events respectively, ‘signal’ is a linguistic trigger word from the causal structure, such as because, so, thus, etc. As no relevant Chinese causality sentences dataset in the field of tourism and hospitality could be found, therefore, we have inspected the trigger words that occur frequently in Chinese causal sentences and the structure of causal correlated sentences to develop a set of specific causal connectives and lexico-syntactic patterns based on Sorgente et al. (2013) and Zhao et al. (2016), see Table 1.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/1DAB3t_U713MkXpGNu4KOWrVqKANVyD4u/view?usp=sharing,Table 1. List of patterns and related regular expression for causes-effects extraction.,Table ,Durga Srikari Maguluri
138,"Step1: Read a def rule.Step2: The patterns should be read. If it is read to the end, proceed to read the next rule and execute Step1.Otherwise, move on to the next step.Step3: Match the elements of the pattern with the sentence from left to right, and proceed to the next step only if all elements of the pattern are successfully matched; otherwise, repeat Step2. Step4: The causal relationship is successfully identified if it exists. the ['Cause', 'Effect'] and ['trigger'] are output, marking the end of the algorithm. If no relationship is found, the next rule is read and Step1 is repeated.We randomly selected 500 online review texts as samples for experiments in order to evaluate the approach. The evaluation indicators Recall (R), Precision (P) and F1 value were used to evaluate the experimental results (Be shown in Fig. 2 and Table 2). The average values showed that the accuracy of the automatic extraction of event causality relations of our system basically reaches more than 73 %, the recall rate was around69 %, and the F1 was maintained at about 0.71.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/1EgnioOneKXAdwju37od7LhwZEK6aVIns/view?usp=sharing,Table 2. Evaluation values for event causality extraction results,Table ,Durga Srikari Maguluri
139,"We adopted an unsupervised Chinese text clustering algorithm combined Doc2vec (Le & Mikolov, 2014, June) with K-means. Firstly, the method used Doc2vec model (see Fig. 3) for characteristic vectorization. Doc2vec based on a Distributed Memory Model of Paragraph Vectors (PV-DM) to train sample sentences of different length, which can learn fixed length feature representation from variable length sentences. In this paper, each pair of cause-effect was treated as a sentence and be represented by a unique vector, represented by a column of the matrix D. Each word was also represented by a unique vector, by some column of the matrix W. Add the vector of this sentence and the word vector of this sample to average or accumulate to form a new vector X, and then used this vector X to predict the prediction words in this window.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/18vK5V6n9z1cKOL47hrec5Ux_28zK52L2/view?usp=sharing,Fig. 3. Doc2vec based on a distributed memory model of paragraph vectors,Table ,Durga Srikari Maguluri
140,"The important phrases in cluster5 included 'children liked', ' children facilities', 'was suiTable for children' and other expressions related to children or families. It could be judged that the elements affected on this type of travelers ’choices were whether a B&B ‘entertainments’ were suiTable for the companions.In cluster1, the phrases 'easy to find', 'very convenient', 'newly open' and ' environment was good ' etc. were mostly talking about the convenience and surroundings in accommodation, which indicated that the main elements affected the travelers’ preferences in this category was the ‘convenient environment’.The high ranking phrases in cluster2 included ' separate toilet design', effect of soundproofing' 'Decorated style', and ' room facility', which were related to the decoration and equipment of a room. According to the phrases we concluded that travelers in these category paid more attention to the conditions of ‘indoor facilities’.Cluster3 contained important phrases were relevant to the ‘boss’ of the B&B, such as ' boss was hospitality', 'boss shuttled', etc. Obviously, such kind of tourists paid special concerns on the ‘host's attitudes’ towards customers during the lodging.In cluster4 we could see that ' B&B was super value ', ' living at home style ' and ' whether popular in internet’, were most important phrases, which mean that travelers in this category were mainly emphasized on B&Bs ‘reputation and experience’.The top phrases in cluster6 included 'check-in services', ' hotel services staff' and other statements about receptions and services. Therefore, the factors that affected travelers’ choices in this category could be attributed to the ‘services of staff’.In the final cluster7, phrases like 'the room was big', 'Tidy room' and 'very comforTable ' were generated in high positions, the contents were talking about spaces and the ‘comfort of rooms’, which could be seen as the main elements that affected the B&B preferences of tourists in this category.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/1ubQDG9T4IslIhyLrJ1kXiD0pjPJaY_Br/view?usp=sharing,Table 3. Parts of high ranking nodes in 7 clustered categories.,Table ,Durga Srikari Maguluri
141,"First of all, by observing the phrases in the k-core graph (Fig. 13), we set same labels for the phrases in the same community (see Table 5). From Table 5, we found that these phrases could be divided into six groups: (1) Phrases in the first group included ‘good service’, ‘free shuttle’, ‘pick-up service’ and ‘very enthusiastic’, which related to the service and staff of the B&Bs, so this group could be labeled as behaviors related to ‘Service’. (2)Phrases in the second group contained ‘very convenient’, ‘hotel location’, ‘convenient transportation’ and so on, which could be classified as the behaviors related to ‘Convenient’ accommodation. (3)In the third group, such as ‘supporting facilities’, ‘decoration style’, ‘room facilities’, etc., which could be regarded as behaviors related to the accommodation ‘Facilities’. (4)The fourth group included phrases such as ‘too comforTable ’, ‘very clean’ and ‘stay experience’, which would be labeled as behaviors related to ‘comfort’. (5)A fifth group included the phrases ‘with the children’ and ‘with a friend’, which could be labeled as behaviors related to ‘Companions’. (6)In the last group, ‘Internet popular’, ‘high cost performance’ was the phrase associated with a behavior related to the B&B's ‘Reputation’.",Information Processing & Management,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic,Predicting Chinese tourists’ B&B preferences through a method of online reviews causality analytic - ScienceDirect,https://drive.google.com/file/d/1a9yQ8WIsXWfd5ebvU78CV5Pt-IpV5Xvx/view?usp=sharing,Table 5. The list on the phrases in the k-core graph,Table ,Durga Srikari Maguluri
142,"In our approach, we advocate for the utilization of graphons as a means to accurately describe the task-related knowledge of graphs. Graphon (Goldenberg et al., 2010) is a continuous graph representation for modeling the structure of large-scale networks that captures the generalized structural features and properties of graphs by treating the probabilistic relationships between the corresponding nodes as random functions on a unit square. Graphon is usually estimated by statistical techniques such as spectral methods, singular value decomposition, or methods based on sorting and smoothing on the observed graph data. Consider a set of graphs, e.g., web networks or social networks, the corresponding graphon reflects the generative mechanisms and statistical graph models. A graphon is defined as a continuous, bounded, and symmetric function:",Information Processing & Management,G-Prompt: Graphon-based Prompt Tuning for graph classification,G-Prompt: Graphon-based Prompt Tuning for graph classification - ScienceDirect,https://drive.google.com/file/d/14s48EDnR0ZBry6-v6E4cuZ9TRX0xj0-l/view?usp=sharing,Fig. 1. The architecture of G-Prompt. G-Prompt is utilized for graph classification based on a graph PTM with frozen parameters. (a) Graphs in a training set are separated by class and inputted to GP module to estimate graphons and generate multiple graph-level sub-prompts for each class. (b) All sub-prompts and input graphs are fed into a graph PTM to obtain their representation. The graph sub-prompts for each class are separately integrated to obtain learnable graph-level prompts. Graph-level prompts will be optimized during the training process to guarantee adaptability. (c) GA module predicts labels based on the conditional probability that input graphs belong to the class corresponding to the prompt. It then selects the class with the largest probability as the prediction.,flow Diagram ,Durga Srikari Maguluri
143,"The datasets consisting of social graphs contain REDDIT-BINARY, IMDB-BINARY, IMDB-MULTI, and COLLAB. REDDIT-BINARY dataset (Yanardag & Vishwanathan, 2015) consists of 2000 graphs. Each graph represents a discussion thread and is labeled with a binary classification, indicating whether the thread falls under a question/answer-based or discussion-based community. IMDB-BINARY dataset (Yanardag & Vishwanathan, 2015) contains 1000 graphs that represent movie collaboration networks. The nodes and edges in each graph represent actors and collaborations, respectively. The classification task is whether the score of movies is rated higher than 6 or lower than 6. Unlike the IMDB-BINARY dataset, a binary classification dataset, IMDB-MULTI dataset (Yanardag & Vishwanathan, 2015) is a multi-class classification dataset, which is a variation of the IMDB dataset that features a balanced set of ego-networks extracted from comedy, romance, and science fiction movie types. COLLAB (Yanardag & Vishwanathan, 2015) dataset consists of 5000 scientific collaboration networks from the arXiv e-print repository, where nodes represent authors and edges represent co-authorship relationships. The task is to classify graphs based on their subject area, which includes three classes: High Energy Physics-Theory, Condensed Matter, and Astrophysics Field.",Information Processing & Management,G-Prompt: Graphon-based Prompt Tuning for graph classification,G-Prompt: Graphon-based Prompt Tuning for graph classification - ScienceDirect,https://drive.google.com/file/d/1NmVE_bj6QLP2dtKmQ9g2VgvM-s6Go9y3/view?usp=sharing,Table 1. Summary of the datasets used in our experiments,Table ,Durga Srikari Maguluri
144,"The performance of G-Prompt and other benchmark methods in graph classification tasks is reported in Table 2. The observations emphasize the characteristics of G-Prompt as follows. (1) G-Prompt demonstrates SOTA performance, achieving an average improvement of 5% over other compared methods. (2) G-Prompt has the generalization ability to combine with existing graph pre-training methods. Compared to GraphPrompt, G-Prompt shows a significant performance improvement based on the same graph PTM. Although GraphPrompt is based on the same graph PTM (pre-trained by GraphCL) as G-Prompt, its performance on some datasets is inferior to GraphCL. This observation indicates that GraphPrompt results in negative transfer on some datasets. We argue the reason is that GraphPrompt is specifically tailored for graph similarity pre-training methods, resulting in limited generalization ability to other pre-training methods. These findings emphasize the importance of designing prompt tuning frameworks with generalization ability. (3) G-Prompt improves the performance stability of graph PTMs on downstream tasks. G-Prompt enhances stability compared to GraphCL without prompt tuning, reducing standard deviation by an average of 10% across all benchmarks. (4) G-Prompt can compensate for the performance gap between different methods. While Infomax has better performance than GraphCL on 3 out of 6 datasets, GraphCL outperforms Infomax when combined with G-Prompt. Another example is that GraphCL is well below the performance of GAT on NCI1 and NCI109. G-Prompt shows significant performance improvements, even with the same PTM from GraphCL. This suggests that prompt tuning performed by G-Prompt can bridge the performance gap between different methods.",Information Processing & Management,G-Prompt: Graphon-based Prompt Tuning for graph classification,G-Prompt: Graphon-based Prompt Tuning for graph classification - ScienceDirect,https://drive.google.com/file/d/1LUVlG-2fhHfpxz945fJUBQTYdeC9wg4i/view?usp=sharing,"Table 2. The performance of G-Prompt and other testing methods in graph classification tasks. The best results are highlighted with boldface, and the second-highest results are underlined below.",Table ,Durga Srikari Maguluri
145,"The experimental results of varying the number of sub-prompts are shown in Fig. 2. We can find that G-Prompt outperforms baseline methods for different sub-prompt numbers. It indicates that the graph-level prompts generated by G-Prompt preserve accurate task-related knowledge independently of the quantity. Moreover, an interesting trend is observed, indicating that sub-prompts are not necessarily better with more or less number, but rather a suitable number. We conjecture the reason is that few sub-prompts may lead to insufficient knowledge, while too many sub-prompts lead to redundant knowledge.",Information Processing & Management,G-Prompt: Graphon-based Prompt Tuning for graph classification,G-Prompt: Graphon-based Prompt Tuning for graph classification - ScienceDirect,https://drive.google.com/file/d/1AvZZ-0rVXOp5ETmRQsi7QuQ05wAspx-0/view?usp=sharing,Table 3. The results of ablation studies.,Table ,Durga Srikari Maguluri
146,"The results are reported in Table 4. G-Prompt with a random model as the backbone demonstrates performance improvements over the randomly initialized backbone model, achieving an average performance improvement of 3%. When using randomly initialized models, the prompt representations  become noisy versions , resulting in a decreased quality of knowledge for the specific class. Decreased quality of knowledge further leads to decreased performance on all datasets. On the other hand,  and  remain in the same representation space since they are output by the same model. Therefore, calculating  still serves to improve the separability of representations, allowing it to achieve better performance than the baseline method. Additionally, when using the graph PTMs obtained from the GraphCL method, G-Prompt achieves an average performance improvement of 5%. These results suggest that G-Prompt can effectively utilize the PTM for graph-level transfer learning with low-resource scenarios.",Information Processing & Management,G-Prompt: Graphon-based Prompt Tuning for graph classification,G-Prompt: Graphon-based Prompt Tuning for graph classification - ScienceDirect,https://drive.google.com/file/d/1VR_mWyZsLNu1PA911TCQA9pGxCwYpkSp/view?usp=sharing,Table 4. The performance of the randomly initialized model with G-Prompt. The best results are highlighted in boldface.,Table ,Durga Srikari Maguluri
147,"Task-oriented Dialogue system (ToD) has garnered significant interest from researchers within both the academic and industrial communities, as they aim to assist users in accomplishing various tasks, including hotel booking and restaurant reservations. (Chen et al., 2023, Hong et al., 2023, Kepuska and Bohouta, 2018, Lee and Jeong, 2023, Ling et al., 2021). The traditional method of building the task-oriented dialogue system uses the pipeline manner (Liu and Lane, 2016, Mrkši? et al., 2016, Wen et al., 2015). Specifically, this method initially generates dialogue states by utilizing the dialogue history and then queries the knowledge base according to the dialogue history and dialogue states to get the retrieval results, which are used to generate corresponding dialogue actions. Finally, the system response is generated by utilizing the dialogue history, retrieval results, and dialogue actions. Although this approach yields satisfactory performance, it is likely to suffer from error propagation. Therefore, some works (Cholakov and Kolev, 2022, He et al., 2022, Lin et al., 2020, Su et al., 2021, Sun et al., 2022, Yan et al., 2022) utilize an end-to-end approach for the construction of task-oriented dialogue system. This approach leverages the seq2seq (sequence-to-sequence) mechanism to generate the system response directly based on both the dialogue history and the large-scale knowledge base.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1p6pp9uQTJfjwjU1IVBFIxwDeNfaPWoWy/view?usp=sharing,Fig. 1. Example of an error response. The red font is the knowledge base information related to the dialogue. The blue font in the dialogue is an inconsistent response generated by the dialogue system (the entity of the response is inconsistent with the information in the knowledge base).,Flow Diagram ,Durga Srikari Maguluri
148,"To address the issue of introducing noise, previous works (Guan et al., 2020, Madotto et al., 2020) embedded the knowledge base directly into the model’s parameters. As shown in Fig. 2(b), they trained the model on the knowledge base and then performed subsequent tasks using the trained model. Given that their method needs to be trained on the knowledge base, it is only suitable for static or pseudo-dynamic knowledge bases with periodic updates (e.g., hotel names, phone numbers, and addresses). In reality, some knowledge bases require in real time updates, such as weather and vehicle conditions. Such a method needs to retrain the model to update the knowledge base, which results in significant training costs. Moreover, this method trains the model on new dialogue data after training on the knowledge base, which makes it easy to suffer catastrophic forgetting and the loss of part of the previously memorized knowledge base information (Luo et al., 2023).",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1yvLlSFOIrstVnqRJj7RR4DMI_QfKs30I/view?usp=sharing,Fig. 2. Different ways of injecting knowledge into the method. Figure (a) directly flattens the knowledge base into a sequence and concatenates it with the dialogue history (Direct Concatenation Method). Figure (b) trains the model on the knowledge base first and then uses the trained model to perform the next task (Embedding the Knowledge Base into the Model Parameters Method). The method we proposed in Figure (c) incorporates the knowledge base with the PLM using an adapter and fusion layer.,Flow Diagram ,Durga Srikari Maguluri
149,"In this section, we first introduce the consistency identification task formulation. Then, we present the adapter module and describe how to design the fusion layer to avoid noise injection, and finally insert the adapter module into the language model to complete the consistency identification task (see Fig. 3).",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1uGUEATl97995iBJLVP7JeixczUjOsgfy/view?usp=sharing,"Fig. 3. Model architecture. Firstly, we transform the knowledge base into the relational triple form, then encode the knowledge base using the adapter. Meanwhile, we input the dialogue into the language model and inject the encoded knowledge base information into the model using the fusion layer. Finally, we output whether there is a consistency issue in the current dialogue.",Flow Diagram ,Durga Srikari Maguluri
150,"Transformer. The transformer architecture has proven to be highly effective in modelling interdependencies between positions in a sequence, thereby enhancing performance in various natural language processing tasks. Moreover, it excels in capturing contextual information. For a given input , the output formula of the transformer is as follows,",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1uGUEATl97995iBJLVP7JeixczUjOsgfy/view?usp=sharing,Fig. 4. Structure of the adapter layer (left). The adapter layer consists of two linear layers and  transformer layers. The transformer layer is shown on the right.,Flow Diagram ,Durga Srikari Maguluri
151,"As shown in Fig. 4, we propose a different adapter structure, which consists of a transformer layer and two linear layers. Unlike previous works (Emelin et al., 2022, Houlsby et al., 2019, Wang et al., 2021), where adapters are added to each layer or inserted among each different layer in a language model that increases the parameter size and augments the cost of training the model, our adapter module is a lightweight module that inserts the adapter at specific layers only, which can reduce the parameter size effectively. It can be treated as a plugin that injects knowledge into the language model.Transformer. The transformer architecture has proven to be highly effective in modelling interdependencies between positions in a sequence, thereby enhancing performance in various natural language processing tasks. Moreover, it excels in capturing contextual information. For a given input , the output formula of the transformer is as follows,(2)
",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/14s7_zmbDTZAZP1eHUBTzlFaMavTYQ4JX/view?usp=sharing,Fig. 4. Structure of the adapter layer (left). The adapter layer consists of two linear layers and transformer layers. The transformer layer is shown on the right.,Flow Diagram ,Durga Srikari Maguluri
152,"Table 1 shows the experimental results on CI-ToD. We use BERT as the base language model to compare with the baselines. The results indicate a great improvement in performance achieved by our method in comparison to the previous baseline methods. Our method outperforms the CGIM model with a significant improvement in F1 score of 5.7% for the HI category, 1.0% for the KBI category, and 2.9% for the overall accuracy. Our model outperforms the direct concatenation methods. For example, compared with BERT, our model achieves a significant increase in F1 score of 6.9% for the HI category, 8.1% for the QI category, 4.1% for the KBI category, and 9.2% for the overall accuracy. It is noteworthy that the previous models only transform the knowledge base into a sequence and concatenate the sequence with the utterance, which often introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base. The redundant information may become noise during model learning, and simply flattening the knowledge base into sequences cannot effectively model relevant information. Our model outperforms the methods that embed the knowledge base into the model parameters. For example, compared with BERT+KE, our model achieves a significant increase in F1 score of 8.0% for the HI category, 1.4% for the QI category, 3.4% for the KBI category, and 4.7% for the overall accuracy. The reason is that these methods train the model on new dialogue data after training on the knowledge base. This tends to suffer catastrophic forgetting and the loss of part of the previously memorized knowledge base information. Whereas our model injects the knowledge base into the model and then only selects relevant knowledge, which avoids this problem and thus improves performance. In addition, we can see that the PLM can perform much better than the non-pretrained model, which is mainly due to the fact that the PLM can better use the knowledge learned in the training process.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1cSdyXsjmMrQuMZuaVt_zkfX_l7mwCn11/view?usp=sharing,Table 1. Comparison of different methods on the CI-ToD dataset. The best result is highlighted in bold.,Table,Durga Srikari Maguluri
153,"w/o adapter: In this setup, we simply flatten the knowledge base into the sequence and then concatenate the sequence with the current conversation. From the results, we can see that in the CI-ToD dataset, the F1 score of this setup dropped by 7% on the HI category, 7.1% on the QI category, 2.6% on the KBI category, and 9% on the overall score. This shows that only the knowledge base is transformed into a sequence and concatenated with the utterance, which often introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base. This redundant information becomes noise during model learning, and simply flattening the knowledge base into sequences cannot effectively model relevant information.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1UPrBavNHL_zCd3HDoRYJVRPEuylh7TYp/view?usp=sharing,Table 2. Ablation study.,Table,Durga Srikari Maguluri
154,"w/o adapter: In this setup, we simply flatten the knowledge base into the sequence and then concatenate the sequence with the current conversation. From the results, we can see that in the CI-ToD dataset, the F1 score of this setup dropped by 7% on the HI category, 7.1% on the QI category, 2.6% on the KBI category, and 9% on the overall score. This shows that only the knowledge base is transformed into a sequence and concatenated with the utterance, which often introduces redundant and irrelevant information from the knowledge base, especially when dealing with a large knowledge base. This redundant information becomes noise during model learning, and simply flattening the knowledge base into sequences cannot effectively model relevant information.",Information Processing & Management,A plug-and-play adapter for consistency identification in task-oriented dialogue systems,A plug-and-play adapter for consistency identification in task-oriented dialogue systems - ScienceDirect,https://drive.google.com/file/d/1UPrBavNHL_zCd3HDoRYJVRPEuylh7TYp/view?usp=sharing,Table 2. Ablation study.,Table ,Durga Srikari Maguluri
155,"Second, in contrast to the non-parameterized methods (Ying et al., 2019, Yuan et al., 2021, Zhang et al., 2021) that directly learn rationales for a specific instance, parameterized generators demonstrate better generalization attributed to their collective and inductive learning abilities (Lin et al., 2021, Luo et al., 2020, Schlichtkrull et al., 2021, Shan et al., 2021). In the parameterized generators, masking an edge is determined by the embedding of the nodes involved in this particular edge. And the generators adopted, such as MLP, predict the mask value instance by instance, where the dependency between edges is not taken into account explicitly. Nevertheless, the message passing mechanism adopted in GNNs implies that structural dependency is essential in representation learning. Thus we consider the structural dependency between edges when building the rationale generator. Fig. 1 illustrates this idea via an example of explaining the prediction of . We can observe that the contribution of  to  is not only determined by the features of  and  but also by the edges that can pass messages to it, including  and . Therefore, explicitly modeling the structural dependency is essential in constructing an effective GNN explainer.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1jdtbdt6_kQoaMmSUWB7GWXPcIPJRftpr/view?usp=sharing,Table 1. Comparison of different objective functions.,Table ,Durga Srikari Maguluri
156,"With the assumption that input graphs consist of task-relevant and task-irrelevant edges for a specific prediction (Luo et al., 2020, Vu and Thai, 2020, Ying et al., 2019, Yuan et al., 2023, Yuan et al., 2021), the proposed RDPE attempt to find a subset of edges that contribute most to the prediction while preserving the fidelity to the original model. Fig. 2 illustrates the overall architecture. Specifically, to construct a dependency-preserving generator, we build an edge-centric graph by transforming the edges of the input instance into nodes (illustrated Fig. 3). And edges in the edge-centric graph are formed by properly considering the patterns of sharing nodes. We further propose a differentiable ranking loss (Neurosort based PL-Model illustrated Fig. 4), which preserves the prediction rankings between the masked and the original graphs, to regularize the learning process of the mask generator. By integrating these components, we optimize the RDPExplainer in an end-to-end manner.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1f2IBvQBq9YgsVo0WWIY6jYAXqcR0k2Bu/view?usp=sharing,Fig. 2. Architecture of the proposed RDPExplainer.,Flow Diagram ,Durga Srikari Maguluri
157,"In Fig. 3, there are three connecting patterns between two edges, chain, fork and collide. According to the message passing strategy adopted in GNNs, no message passing between two edges in the fork and collide patterns. Thus we only consider the chain pattern for the edge transformation. Given the original graph , we transform  into the edge-centric graph . In , the th row denotes the edge  and its relationships with other edges in the transformed graph, the  means  and  share one node in the chain pattern, and edge  is the ancestors of  in this pattern. And the feature of  is the concatenated features of nodes it connects. As illustrated in Fig. 3, we initialize the embedding of  by concatenating embeddings of  and . The embeddings of  and  can be obtained in the last layer of the model to be explained, and the concatenation is ordered by the direction of edge .",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1F_GftV-dRs_WUVRz4Gu5mi_5NBDlRPmb/view?usp=sharing,Fig. 3. Example for edge transformation schema.,Flow Diagram ,Durga Srikari Maguluri
158,"Graph-SST5 (Yuan et al., 2023): a real-world sentiment dataset for graph classification. It converts text sentences to graphs as same as Graph-Twitter. Each graph is labeled according to its fine-grained sentiment, which can be negative, somewhat negative, neutral, positive, and somewhat positive.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1X3E8WJwFM8hwVZ-rRX9I6yosFrm10Uhl/view?usp=sharing,Table 2. Statistics of the datasets used for evaluation.,Table ,Durga Srikari Maguluri
159,"We first compare the proposed RDPE with the baseline methods for node classification. As illustrated in Table 3, our proposed RDPE achieves improved percentages 149.67% and 51.43% in terms of , compared with the best baselines. The experimental results demonstrate the outperformance of the proposed RDPE from two perspectives, masking irrelevant edges, and retaining the rationales for the GNN predictions. We further illustrate the performance among diverse  settings, Fig. 5, Fig. 5 illustrate the performance in terms of  when setting  ranges [0.01, 0.1] with step 0.01, and [0.1, 1] with step 0.05. For the  metric combining two characteristics of  and , compared to the baselines that only concern the optimization of smoothed values, the proposed RDPE demonstrates its outperformance especially when  is small. We believe that adopting the ranking preserving loss benefits the rationale learning. Specifically, for  that evaluates the performance by masking unimportant edges, we observe that the RDPE shows a smaller and more stable value under different s, indicating that the unimportant edges determined by  in RDPE contribute less to the prediction. For  that targets masking important edges, RDPE shows an improvement and tends to be stable when  is small, indicating that RDPE learns rationales that contribute most to the prediction.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/14oDgU_0OIrBofF5XN_z8H1mSYL9sxBUO/view?usp=sharing,Table 3. Improvement of  F value fidelity,Table ,Durga Srikari Maguluri
160,"For the real-world datasets Graph-Twitter and Graph-SST5, we illustrate the sentences predicted as negative and positive respectively. In the fifth and seventh rows, only the proposed RDPE identifies word phrases with positive semantic meaning, such as “something new its amazing and original” and “surprised at the variety of tones”, indicating the RDPE can explain the prediction reasonably. Meanwhile, explanations provided by baselines are less semantically related. In the sixth and eighth rows whose predictions are negative, we found RDPE can accurately find sentences expressing negative emotions, such as “translating book to crude representations” and “that it is instead a cheap clich”. In the sixth row, we also observe that baselines except SubgraphX discover fragmented phrases that cannot express a full semantic. Furthermore, the baselines cannot find the most discriminative word “crude”. Overall, given different graphs, the proposed RDPE can provide explanations that are more in line with human cognition than baselines.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1Hv3MAda6JKUXOAw8xJiuxGzHI0vtc4Cz/view?usp=sharing,Table 11. Comparative cases.,Table ,Durga Srikari Maguluri
161,"Furthermore, we provide an empirical study of the inference time for all baselines on these four datasets, illustrated in Table 10. In which we run all the baselines using the open-source code provided by the authors. All the codes are running on a single RTX 3090 GPU. We observe that the inference times of parameterized methods, including PGExplainer, Gem, and RDPE, are significantly faster than those of non-parameterized methods such as GNNE and SubgrapX. For instance, in the BA-Shapes dataset, the inference speed of RDPE is approximately 100 times faster than that of GNNE, i.e., 0.057 s per instance versus 5.506 s per instance. Compared to Subgraphx, RDPE exhibits even greater gains in terms of inference time. Among the parameterized methods, the running time of the proposed RDPE is nearly identical to that of PGExplainer and slightly longer than Gem. Taking the Graph-Twitter dataset as an example, RDPE requires 0.044 s per instance, PGExplainer takes 0.047 s, and Gem takes 0.034 s. However, it is worth noting that in terms of , AUC, and NDCG, RDPE outperforms these parameterized baselines, highlighting its advantages in balancing efficiency and effectiveness.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1j7sYCGKdOLwpGpRyKBqWzECxpiKFGTuP/view?usp=sharing,Table 10. Inference time (Seconds) per instance.,Table ,Durga Srikari Maguluri
162,"As mentioned in Section 5.3, the RDPE will take  for the training and  for the testing, where  is the number of training iterations,  is 40 for graph classification and 60 for node classification, and  is the number of edges in the edge-centric graph. The total time complexity depends on the edge size  in the edge-centric graph. We first performed a statistical analysis on the edges  in the edge-centric graph, illustrated in Table 9. It obviously supports our claim that  as the number of  is typically one-ninth of . For the four datasets used in this paper, the largest number of labels is 8 in the BA-community dataset. Therefore, the calculation of the ranking loss  (shown in Eq. (9)-(10) in the paper) is small enough compared to the learning of the mask generator.",Information Processing & Management,Towards explaining graph neural networks via preserving prediction ranking and structural dependency,Towards explaining graph neural networks via preserving prediction ranking and structural dependency - ScienceDirect,https://drive.google.com/file/d/1q3XEqXevj_5IC-JLcpq2gBgkv2qeWRqB/view?usp=sharing,Table 9. Average number of edges per instance in the edge-centric graph.,Table ,Durga Srikari Maguluri
163,"The span-based classification model for XMRC is a classic MRC method in which the evidence and the answer are consecutive spans extracted from the context. With the help of powerful PLMs, more and more classification models for MRC (Clark et al., 2020, Cui et al., 2022, Lan et al., 2019, Zhang et al., 2021) outperform humans in answer selection. Nevertheless, with the improvement of the performance of MRC models, a disadvantage has also emerged: the interpretability of these models is not strong (Grabe and Yamashita, 2022, Nishida et al., 2021, Thayaparan et al., 2020). The reason for this problem is that the span-based classification model can only extract hard one-hot evidence from the context through the start position and end position rather than the soft evidence consisting of token probability distributions. The hard one-hot evidence, where each token is represented by a one-hot vector consisting of 0 and 1, cannot be differentiated, so the span-based classification model cannot be trained in an end-to-end manner to extract the answer from the extracted evidence. As a result, the model either only focuses on the extraction of answers that does not provide evidence for explanation (Zhang et al., 2021), or it can only achieve a rough explanation that provides evidence and answers at the same time (Cui et al., 2022), but not a precise explanation that extracts evidence first and then extracts answers from the evidence (our model). Unfortunately, because there may be multiple sentences containing answers in the context instead of only one, the evidence and the answer provided by the span-based classification model at the same time may be irrelevant, namely that the evidence may not be the correct interpretation of the answer. This is obviously inconsistent with human commonsense and significantly reduces the interpretability of the MRC model. Table 1 shows an example, which compares (Zhang et al., 2021) and Cui et al. (2022) with our model. From Table 1, the model proposed by Zhang et al. (2021) only give the answer “wave speeds” but cannot select evidence to explain the answer. Although the model proposed by Cui et al. (2022) can provide additional evidence for the selected answer, the evidence provided is not relevant to the selected answer.",Information Processing & Management,A T5-based interpretable reading comprehension model with more accurate evidence training,A T5-based interpretable reading comprehension model with more accurate evidence training - ScienceDirect,https://drive.google.com/file/d/1KrjUPi44kfOth5UIBivJILqB588O1_nQ/view?usp=sharing,"Table 1. Examples for answers and evidences predicted by BERT-large (Cui et al., 2022), Retro-Reader (Zhang et al., 2021) and our model. Underlined bold in paragraphs is golden evidence",Table ,Durga Srikari Maguluri
164,"We propose a T5-based hierarchical Interpretable MRC (InterMRC) framework with more accurate evidence, which first generates evidence from context and then generates an answer from the evidence We propose a threshold-based evidence loss filtering method to avoid the effect of the error evidence on the training of our InterMRC model.We propose a label reconstruction method and a data augmentation method to improve the accuracy of the T5 model in STS tasks. We use the improved T5-based STS model to develop a heuristic strategy to label the evidence of the MRC dataset, which makes the evidence used to train our InterMRC model more accurateThe rest of the paper is arranged as follows: Section 2 introduces the related work on STS and MRC; Section 3 describes the research objective of our paper; Section 4 describes our methods and the proposed model in detail; Section 5 presents the experimental setup, results, ablation studies and case analysis; finally, Section 6 provides the conclusion",Information Processing & Management,A T5-based interpretable reading comprehension model with more accurate evidence training,A T5-based interpretable reading comprehension model with more accurate evidence training - ScienceDirect,https://drive.google.com/file/d/1lZUQopPsJKvqbsxkmkWeVerj1iswHErJ/view?usp=sharing,Table 3. Examples for token and its ID of real-number similarity in different word segmentation methods.,Table ,Durga Srikari Maguluri
165,"In this paper we propose a kind of Interpretable MRC (InterMRC) framework, aiming to provide more accurate evidence of answers to enhance model interpretability. Fig. 1[b] describes the overall framework of InterMRC. The InterMRC framework is composed of two modules: an Evidence module and a Question-Answering module. The Evidence module first generates evidence, and then the Question-Answering module answers the question on the basis of the evidence. Following (Shi et al., 2021), we take advantage of the Gumbel-Softmax technique (Jang et al., 2016) to train our model in a differentiable manner.Evidence module is composed of an encoder shared with the Question-Answering module and a separate decoder (named “EV-Decoder” in Fig. 1[b]).",Information Processing & Management,A T5-based interpretable reading comprehension model with more accurate evidence training,A T5-based interpretable reading comprehension model with more accurate evidence training - ScienceDirect,https://drive.google.com/file/d/1PFgXyrzWV9hPFPD3hx9acRIItSUK8Thc/view?usp=sharing,"Fig. 1. Overview of the proposed InterMRC framework. For the left part, [a] shows the shared decoder framework, [b] shows the separated decoder framework.",Flow Diagram ,Durga Srikari Maguluri
166,"After obtaining the annotated evidence label by using the method described in Section 4.2, we follow the frameworks as shown in Fig. 1 to build and train MRC models. We design two T5-based models to correspond to [a–b] in Fig. 1, and finally select the optimal model as the final model. The initialization weights of the encoder and decoder come from the official parameters (i.e. T5-base and T5-large). Our model is trained on one Tesla A100 GPU. We use gradient accumulation to train our model, and we additionally use Gumbel-Softmax (as mentioned in Section 4.1.3) and evidence loss filtering (as mentioned in Section 4.1.4) to train our model. The calculation of the loss function is shown in Section 4.1.5. During training MRC models, we perform a grid search on the hyper-parameter setting composed of the left threshold () and the right threshold (). We determine thresholds according to the Cross-Entropy loss function (see Eq. (19)). We correspond the level of accuracy of evidence to the average probability of each token generated by the model being correct. The left threshold, 0.01/0.05/0.1, corresponds to the average correct probability of each token in the generated evidence, which is approximately 99%/95%/90%. Similarly, the right threshold, 0.3/0.5, corresponds to the correct probability of each token in the generated evidence, which is around 60%/70%. Some hyper-parameters are shown in Table 4, and other hyper-parameters use default settings.",Information Processing & Management,A T5-based interpretable reading comprehension model with more accurate evidence training,A T5-based interpretable reading comprehension model with more accurate evidence training - ScienceDirect,https://drive.google.com/file/d/1aDOH9wAqY4oBS8bf0q4FymfsQSpvbuGB/view?usp=sharing,"Table 4. Hyper-parameters are used in SQuAD1.1.  denotes the max length of input and  denotes the maximum output length of the evidence. Hyper-parameters marked with  are used in the large/base-level model, respectively. The other unmarked hyper-parameters are shared by these two levels.",Table ,Durga Srikari Maguluri
167,"We perform ablation experiments from three aspects:•w/o LR-DA ablates the label reconstruction and the data augmentation in STS model. It is equivalent to train STS tasks directly using the original T5 model without the label reconstruction and the data augmentation. Then, we use this STS model to annotate evidences. Finally, we use the annotated evidences to train our T5-InterMRC model.•w/o ELF ablates the evidence loss filtering when calculating the overall loss in our T5-InterMRC. This means that we use Eq. (21) to calculate the overall loss, where is equal to 0.1.•w/o S&E Loss ablates the losses at the starting and ending positions when calculating the evidence loss in our T5-InterMRC model.The experimental results of ablation studies are shown in Table 10. When we ablate the label reconstruction and the data augmentation, the results become worse, which further indicates that the label reconstruction and the data augmentation can make the generated evidence more accurate. Our T5-InterMRC model is better than its w/o ELF model in all metrics, which indicates that the evidence loss filtering can reduce the influence of false evidence on answer generation and improve the performance of our model. We see that our model without the losses at the starting and ending positions produces suboptimal results, which confirms the effectiveness of increasing the weights of evidence at the starting and ending positions.",Information Processing & Management,A T5-based interpretable reading comprehension model with more accurate evidence training,A T5-based interpretable reading comprehension model with more accurate evidence training - ScienceDirect,https://drive.google.com/file/d/1hB4lYA1wz0iMbbUAlXcTYmOK8brz20p1/view?usp=sharing,Table 10. Experimental results of ablation studies on SQuAD1.1 dev set.,Table ,Durga Srikari Maguluri
168,"In fact, we observe that numbers play an important role in math description understanding. For example, in Table 1, “has completed 40%” vs “has completed 450” determines the production efficiency of the garment factory. Numerical perturbations in the problem description bring significant variations in expression and answer. Similarly, one must understand that the number “1200” represents a bigger value than “450”. If the numbers are properly encoded into the low-dimension space as words while keeping the numeracy, we can use the deep learning models to represent the math descriptions better. While numeracy is essential in the MWP domain where numbers are prevalent, most existing MWP solvers either directly discard the numbers in the pre-processing step or treat the numbers as special symbolic placeholders (e.g., [NUM]) (Liang et al., 2022, Wu, Zhang, Wei, and Huang, 2021). Prior works (Liang et al., 2022, Wu, Zhang, Wei, and Huang, 2021, Zhang, Wang, Lee, et al., 2020) make preliminary studies to improve the numerical reasoning ability for the MWP task. However, these methods perform poorly on the MWP task since they cannot deal with a large of numbers with a wide range, especially for the numbers that are not appeared in the training data (e.g., out-of-vocabulary (OOV) numbers), which severely limits their use in encoding any number for MWP solving. Besides, we also find that the mainstream GTS decoder (Xie & Sun, 2019) only preserves the parent nodes and siblings when generating the new goal vector. The early-stage parent node information (e.g., the ancestor node information) is lost, resulting in unsatisfactory generated math expressions, especially for complex problems with more operators.",Information Processing & Management,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving - ScienceDirect,https://drive.google.com/file/d/1NEusubWiWxhsktkKKsx5YEZhnLuTXstc/view?usp=sharing,Table 1. Two MWP examples with different numerical values cause to different expressions and answers.,Table ,Durga Srikari Maguluri
169,"Numerical reasoning and the semantic understanding of the math text description are two critical factors for MWP solving. In this paper, we construct a number graph and a semantic graph from two different perspectives. The former graph attempts to construct the magnitude relationships among the numbers and learn the numeracy-preserved embeddings, while the latter graph captures the structural information among words to understand the semantics behind the text description better.Fig. 1 shows the overall architecture of our proposed NERHRT, which consists of five modules: (1) graph construction; (2) numerical graph embedding and update; (3) semantic graph embedding and update; (4) information fusion; (5) hierarchical recursive tree-structured decoder.Specifically, given a math text description, NERHRT constructs two graphs (e.g., numerical and semantic graphs) from two different perspectives (e.g., numerical and semantic relationship). The numerical graph aims to capture mathematical relationships (e.g., magnitudes) and numeration, which can inject more numerical skills into the MWP solver. Similarly, the semantic graph can model the local graph structures information and global properties. To comprehensively use the advantages of two different features, we use a transformer to obtain the number-enhanced word representation. Finally, a hierarchical recursive tree decoder generates the expression tree.For the numerical perspective, NERHRT concentrates on constructing a numerical graph. The numerical graph can represent numbers and their magnitude relations correctly and intuitively. We regard all numbers extracted from the math text descriptions as nodes to build a directed acyclic graph where nodes are numbers with values. The nodes are linked by a relationship type called “isLessThan” (“”), which ensures the magnitude property of numbers can be captured. In other words, if from the numerical graph. Such a numeric graph with a single relationship type “isLessThan” provides proper numerical embeddings, which can heuristically lead to transformation in two numbers (e.g., subtraction and division).",Information Processing & Management,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving - ScienceDirect,https://drive.google.com/file/d/1X6uz_XmdONbrS-FgoGaCV0MxtxHmQjca/view?usp=sharing,Fig. 1. In the framework of our proposed model.,Flow Diagram ,Durga Srikari Maguluri
170,"Since both the digit value and the digit position are integers, we initialize the and using the pre-trained integer embedding learned from the online encyclopedia of integer sequences (OEIS) (Ryskina & Knight, 2021). Fig. 2 shows an example how the integer embedding is learned. The integer embedding generator consists of two feed-forward networks (FFNs), and a multilayer perception (MLP) with ReLU activations. Moreover, if a number whose digit position is more than “9” or less than “?9”, we only keep the values whose position is between “?9” and “9. Our number embedding method is intuitive and human mathematical thinking, which captures the scale and precision of numerals in a continuous manner. Without sophisticated mathematical operations, we can obtain the unseen number embeddings in the test phase directly.Although our decimal-based numerical embedding method can canonicalise numbers and encode them in an approximately contiguous space, it needs to capture the magnitude relations of numbers that can heuristically guide the reasoning of two numbers (e.g. subtraction and division). The role of a number in reasoning is not only decided by itself but also related to the neighbors. It is important to propagate the neighborhood numerical information to update the number representation. However, the conventional graph learning methods (e.g., GATs (Velickovic et al., 2018, Wang, Ji, et al., 2019)) only consider the neighbors from the inbound direction (e.g., the neighborhood numbers whose value is greater than the central number), ignoring the neighbors from the out-bound directions. In fact, the neighborhood numbers from both directions are essential for number understanding. Therefore, we develop a dual-direction graph attention network to learn the bidirectional neighbor propagation.",Information Processing & Management,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving - ScienceDirect,https://drive.google.com/file/d/1UBXd9GLD29BtYDqW2tepF__kuk4cZgMy/view?usp=sharing,Fig. 2. A example for our probing model. We pass numbers through two embedders and train a probing model to solve decoding a number.,Flow Diagram ,Durga Srikari Maguluri
171,"We evaluate the results using four standard MWP benchmark datasets: Math23K (Wang et al., 2017), MAWPS (Koncel-Kedziorski et al., 2016) and SVAMP (Patel et al., 2021) and MathQA (Amini et al., 2019). In these datasets, five operators will be considered: addition, subtraction, multiplication, division and exponentiation. The Math23K contains 23, 161 Chinese MWPs for elementary school students, and each sample is labeled with the corresponding equation expressions and the answers. The MAWPS consists of 1, 987 English MWPs with one or more unknown variables for elementary-level arithmetic problems. Different from Math23K and MAWPS datasets, SVAMP and MathQA are more challenging. The SVAMP consists of 4, 138 English MWPs, with a large amount of unseen numbers in the math texts. The MathQA contains 20, 207 English MWPs, and the questions involve more operations. Following the previous works (Jie et al., 2022, Tan et al., 2022), we split each dataset into the subsets of train/dev/test.5 The statistics are shown in Table 4.We follow the standard evaluation metrics used in previous works (Jie et al., 2022, Wu, Zhang, Wei, and Huang, 2021, Zhang, Wang, Lee, et al., 2020) and report the results using 5-fold cross-validation (e.g., MAWPS). Besides, we present the results using the mean answer accuracy (e.g., MAWPS).",Information Processing & Management,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving - ScienceDirect,https://drive.google.com/file/d/1Ig8i8aMBOcRKlm3BZmUjc9_eIkO3G7PO/view?usp=sharing,Table 4. Dataset statistics.,Table ,Durga Srikari Maguluri
172,"In this section, to demonstrate the contribution of each module in our model, we conduct extensive ablation experiments on four benchmark datasets shown in Table 8. The ablation study shows the contributions from the following three aspects:First, in the graph construction, we construct number and semantic graphs based on different strategies. The “w/o number graph” stands for the NERHRT, which only considers semantic information, while the “w/o semantic graph” stands for the NERHRT, which removes the effect of the semantic graph. That is to say, we consider only one graph each time. From the results, both the number graph and semantic graph improve the performance. For Math23K, MAWPS, and MathQA, the number graph contributes more to the performance, which improves 2.1% and 2.8% and 1.9% on the test split, respectively. The semantic graph also shows 1.8% and 2.4% and 1.6% improvements on Math23K, MAWPS, and MathQA. As mentioned above, we are the first to model quantity of numerals in the problem text to the continuous embedding and leverage the magnitude relations between numbers to numerical reasoning. The effectiveness of the number graph also verifies our proposed method.Graph encoders.Secondly, we explore the impact of three encode modules on the model performance. “w/o decimal notation embedding” denotes that the model uses number mapping technique, ignoring the numerical information. “w/o dual-direction graph attention network” denotes that the model updates node representations using conventional GAT, ignoring the bidirectional information. “w/o relation-aware messaging mechanism” indicates that the model only takes node-to-node message passing, ignoring the rich edge information. The results show that the performance of the “w/o decimal notation embedding” and “w/o dual graph attention network” is significantly decreased. The reason is that numerical values and magnitude relations are critical to understanding MWPs and influence the final performance. Meanwhile, from the result of “w/o relation-aware messaging mechanism” that regards all nodes under the same semantic relation, the results show that considering the heterogeneous issue can improve performance.Tree decoder.Finally, we focus on the effectiveness of hierarchical recursive tree decoder modules in the proposed NERHRT. Without learning ancestry nodes, the answer accuracy is reduced to 85.7%, 90.9%, 43.9% and 76.3% for Math23K and MAWPS and SVAMP and MathQA datasets, respectively. It illustrates that our tree decoder has the advantage of inferring the implicit knowledge within the ancestry nodes.",Information Processing & Management,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving,Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving - ScienceDirect,https://drive.google.com/file/d/1uzPG2lLyAP0W-gfQ-acME_-NWTQbouc6/view?usp=sharing,Table 8. Ablation analysis of different part in ours model. Accuracy improvements are marked as .,Table ,Durga Srikari Maguluri
173,"Deploying the FAE-CF without additional protection could not adequately protect user privacy. In this section, we first discuss the potential privacy risks associated with the FAE-CF, then propose an attack to highlight these issues.A FAE-CF model with an -layer neural network has parameters which can be divided into and  Here, represents the parameters for the side layers of the FAE-CF, that is, the first layer and the last layer On the other hand, Typically, the weight of FAE-CF’s first encoder layer, has dimensions and the weight of last decoder layer, , has dimensions We found that both are strongly correlated with the item ID, indicating that user interactions with specific items are easily discernible from the corresponding parameters’ gradients.stands for the parameters associated with the middle layers of the FAE-CF model.n the encoder side, the existence of the dropout layer may introduce some degree of perturbation to the input. However, the dropout only results in a randomized partial omission of the original input, and the final input remains a true subset of the original. Thus, it leaks a portion of the user’s positive item ID. Furthermore, due to the stochastic nature of the dropout, the more a user engages in the training rounds, the more privacy is compromised. Ultimately, the user’s positive item ID may become fully disclosed, Fig. 1 illustrates a basic instance of FAE-CF gradient leakage. Consider a user with a interactive item set  This corresponds to the interaction vector After applying the dropout layer, we obtain the corrupted interaction vector As a consequence, the gradient of the first encoder layer, has non-zero columns 0 and 3, while columns 1 and 2 are zero. Furthermore, this results in the gradient of the last decoder layer with columns 0, 1, and 3 exhibiting opposite signs to column 2 in each dimension.",Information Processing & Management,HN3S: A Federated AutoEncoder framework for Collaborative Filtering via Hybrid Negative Sampling and Secret Sharing,HN3S: A Federated AutoEncoder framework for Collaborative Filtering via Hybrid Negative Sampling and Secret Sharing - ScienceDirect,https://drive.google.com/file/d/1dRsrMkAAE_YqCwyIKwNX-qep0mXO_bzG/view?usp=sharing,Fig. 1. Illustrations of FAE-CF’s privacy leakage.,Flow Diagram ,Durga Srikari Maguluri
174,"Follow (Polato, 2021), we use four public datasets in the experiments, including Citeulike-a, Steam, MovieLens-1M (ML-1M) and FilmTrust:Citeulike-a5: These were gathered from CiteULike and Google Scholar. CiteULike provides a platform for users to curate personal collections of academic articles. Each inclusion in the collection serves as an implicit positive rating for that particular article. To ensure the integrity and reliability of the dataset, any users who have contributed less than 2 article ratings have been excluded.Steam6: This is user behavior data provided by the Steam PC gaming platform. It includes two types of behaviors: “purchase” and “play”. However, we only retain a single behavior for each user-item pair, regardless of its type. These behaviors are treated as implicit feedback. Furthermore, users with fewer than two interactions are excluded from the dataset.ML-1M7: This dataset contains user movie ratings collected from movie recommendation services. We binarize the explicit ratings by keeping the ratings 4, and users with less than five ratings are removed.ilmTrust8: FilmTrust (Guo, Zhang, &Yorke-Smith, 2013) is a small dataset crawled from the entire FilmTrust website. Ratings are on a scale of 1–5 with a threshold of 3. Users with less than five ratings are filtered out.Note that the efficiency of single-machine simulated federation recommender systems is low, and building a clustered federation recommender system is too costly, so current work (Ammad-ud-din et al., 2019, Chai et al., 2021, Liang et al., 2021, Lin et al., 2021, Lin et al., 2022, Wu et al., 2022) on federation recommender systems almost always uses small to medium-sized datasets. The statistics of the dataset after preprocessing are shown in Table 2.",Information Processing & Management,HN3S: A Federated AutoEncoder framework for Collaborative Filtering via Hybrid Negative Sampling and Secret Sharing,HN3S: A Federated AutoEncoder framework for Collaborative Filtering via Hybrid Negative Sampling and Secret Sharing - ScienceDirect,https://drive.google.com/file/d/1hemoLquHvPU4j7MIWukwOV4a-OhD5GJ4/view?usp=sharing,Table 2. Attributes of datasets after preprocessing. Held-out users is the number of validation/test users out of the total number of users in the first column.,Table ,Durga Srikari Maguluri
175,"In addition, the construction process of macro and micro semantic environments is depicted in Fig. 2. News in the micro semantic environment is defined as verified posts constructing strong event constraints with common linguistic components under common topics within time intervals, normally the inner relationship between fake news and posts in the micro environment is more explicitly demonstrated in semantic contradiction evidence. In contrast, news in the macro semantic environment consists of a larger amount of posts concerning current affairs that are derived from different public perspectives within time-constrained comprehensive semantic circumstances. For example, the micro semantic environment contains the official dogmatic text from the perspective of the Russo Ukrainian War, while the macro semantic environment can include broader and more diverse discussion environments such as online discussions, livelihood discussions, and live streams produces by internet celebrities through social networks. This provides semantic guidance for distinguishing between fake news and real news, and is also a new perspective for debunking any semantic inconsistencies or factual contradictions.Therefore, a fake news detection framework based on news semantic environment perception (NSEP) is proposed to form early fake news detection techniques. This framework consists primarily of the following three scholarly steps. First, a micro- and macro semantic environment distinguisher module is applied to form an integrated news semantic environment for each possible fake news, and the news semantic environment is further divided into macro- and micro semantic environments according to their times and events. Second, a micro semantic detection module consisting of multihead attention and sparse attention is applied, which reduces the computational complexity and redundant information required to identify fake news with strengthened semantically contradictory features. Third, a macro semantic detection module including a graph convolutional network is applied to deeply identify semantic contradictions and inconsistencies between news content and the corresponding environment, and fused perceptual features are obtained to determine the authenticity of fake news candidates. The experimental results indicate that NSEP outperforms other state-of-the-art baselines with accuracies as high as 86.8% and 74.1% on two real-world Chinese and English datasets, respectively.The main contributions are summarized as follows. 1) To our knowledge, NSEP is the first framework to achieve early fake news detection by perceiving the news semantic environment through deep neural networks. 2) A novel sparse attention mechanism is proposed to fully capture and perceive semantic contradictions between fake news items and selected posts in the micro semantic environment. In addition, sparse attention combines multihead attention to obtain more detailed auxiliary information for determining whether a post is true or false. 3) Extensive experiments conducted on Chinese and English datasets show that NSEP outperforms other full baseline methods. The findings of this paper indicate that observing the relationship between a given news item and its news semantic environment can provide a new perspective for fake news detection.Our research goal is to perceive the internal relationships between news items and their macro- and micro semantic environments. Two perceptual features are captured separately and combined by applying deep learning technology to better judge the authenticity of a news item. The main objectives are as follows:By effectively modeling macro- and micro semantic environments, sufficient groundwork is laid for the early detection of fake news.The macro semantic detection module is used to explore the potential impacts of posts in the macro semantic environment on fake news items, i.e., gather implicit evidence.The micro semantic detection module, composed of multihead attention and sparse attention, is used to gather explicit evidence from the micro semantic environment.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1tOKYHEH6yhSz2hVjclCCS9v6JD79w6Y6/view?usp=sharing,"Fig. 2. The construction method and inconsistent example of news semantic environment, where the red node represents the news source, the blue nodes represent news in the micro semantic environment, and the green nodes represent news in the macro semantic environment. Explicit and implicit evidence can be found in both micro- and macro semantic environments to determine the authenticity of news.",Flow Diagram ,Durga Srikari Maguluri
176,"In this study, a fake news detection framework based on semantic-aware news environment perception is proposed to concentrate on authenticity judgments involving text processing management in news contents and post contexts in macro and micro environments, as shown in Fig. 3. The NSEP framework consists of three parts, i.e., news semantic environment construction, news semantic environment perception, and prediction. Demonstrated on the left side of Fig. 3, each target post , represented as a red node, and its surrounding news items set , represented as other nodes, form a semantic-aware news environment that is further divided into micro- and macro news semantic environments denoted as  and . Afterward, in the news semantic environment perception procedure, the micro semantic detection module and the macro semantic detection module are used to evaluate the relationships between posts and the micro semantic environment and between posts and the macro semantic environment. A multilayer perceptron is also applied to adjust the inner dimensions to better extract micro- and macro semantic environment perception features, denoted as F and G, respectively. F and G are also referred to as explicit and implicit evidence, operating in the upper and lower concatenations illustrated in the middle part of Fig. 3. Finally, the perceived features are handled in the prediction procedure to determine whether the given post is true or false. The following sections separately describe each of the above modules in detail.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1R9wm9OIO4j0TSytMwl1h42Znp9GLhcFM/view?usp=sharing,"Fig. 3. The architecture of our NSEP framework is divided into three stages: construction, perception, and prediction. The red, green, and blue entities represent the fake content candidates and the news items in the macro- and micro semantic environments, respectively.",Flow Diagram ,Durga Srikari Maguluri
177,"g the existing datasets as a basis, (Sheng et al., 2022) built Chinese and English datasets that included the news environment information of the same period. Table 1 shows the statistical details of the two datasets. Datasets can be found in https://forms.office.com/r/Tr6FMGQJt0.The Chinese dataset includes integrated posts from 2010 to 2021 (Ma et al., 2016; Sheng, Cao, Zhang, Li, & Zhong, 2021; Song et al., 2019; Zhang et al., 2021). We crawled 583,208 news items from microblog accounts opened by six well-known Chinese mainstream media sources to build a news environment for the posts.The English dataset consists of news articles posted from 2014 to 2018 (Augenstein et al., 2019; Kochkina, Liakata, & Zubiaga, 2018; Shaar, Babulkov, Da San Martino, & Nakov, 2020). Accordingly, to build a news environment for the posts, 1003,646 headlines from 2014 to 2018 were obtained from three well-known English media sources, namely, Huffington Post, National Public Radio and Daily Mail.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1_mBgHCtjqJcipFS3qYlpPceuvz17iu90/view?usp=sharing,Table 1. Statistics of the datasets.,Table ,Durga Srikari Maguluri
178,"Effectiveness of perceived features. To verify the effectiveness of the features obtained by each module in NSEP, we feed macro semantic environment perception feature , micro semantic environment perception feature , and joint feature  into several layers of simple MLP. The experimental results obtained on the Chinese and English datasets are shown in Table 2.The experimental results in Table 2 show that using only the feature information of a single module for fake news detection can achieve good prediction results. In addition, using the joint features can further improve detection performance, with an accuracy improvement of at least 5.3% and 1.3% compared to using single features in Chinese and English datasets, respectively. This indicates that the perceptual features obtained by the two modules are complementary, and it is reasonable to enhance fake news detectors by exploring the interrelationships between posts and the environment.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/15Wdux63lnxa-SvzwgmIAKNgxNE-PTIdW/view?usp=sharing,Table 2. Experimental results of individual and joint features of two modules in NSEP.,Table ,Durga Srikari Maguluri
179,"First, NSEP+BERT outperforms various single detectors on both datasets under all metrics. This indicates that our NSEP framework can effectively improve fake news detection; that is, considering environmental perception features can effectively improve the detection level. Second, BERT and BERT-Emo have better experimental results than the other baseline methods that focus on the post itself, so pretrained BERT can better learn text features and represent the semantic features of news. Finally, NSEP+BERT improves the accuracy metric by 8.7% and 2.3% compared to MAC on both datasets, indicating that capturing the semantic differences between posts and environments more effectively improves false news detection performance than using external knowledge. In addition, the experimental results further indicate that NSEP can correctly distinguish many fake news and reduce false positives and negative impacts, which helps to improve the performance of the NSEP model.Effectiveness of NSEP. The following experiments are conducted to determine whether the NSEP framework can improve the performance of the model in fake news detection. We use the currently popular BERT as a fake news detector along with perceptual features with detector features to distinguish false news, denoting this method as BERT+NSEP. The results of a comparion between BERT+NSEP and various detectors are shown in Table 3.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1qJ9RGU-6XIbJ5EqWXNCmZX4YvoN9gvUh/view?usp=sharing,"Table 3. The performance comparison between NSEP and various detectors, with the detectors in the NSEP framework using the representative BERT.",Table ,Durga Srikari Maguluri
180,"Two ablation experimental groups are established to verify the effectiveness of micro and macro semantic detection modules in improving NSEP performance.Two ablation experimental groups are established to verify the effectiveness of micro and macro semantic detection modules in improving NSEP performance.(w/o) Mac semantic detection module: The above evaluation indicators accuracy, macF1 and F1 value of each classification result are still used, and the macro semantic detection module is removed to observe the impact on the performance of the method. The detector model here uses BERT.(w/o) Mic semantic detection module: Here, the micro semantic detection module is removed to observe the impact on the performance of the method, and everything else remains the same as above.The results of ablation experiments conducted on the two datasets are shown in Table 5. Adding the macro and micro semantic detection modules to the Chinese dataset can improve accuracy indicators by 2.5% and 0.8%, respectively. Adding the macro and micro semantic detection modules to the English dataset can improve accuracy indicators by 0.6% and 1.0%, respectively. At the same time, NSEP outperforms the variant methods in all experimental indicators. The above results indicate that the various modules of the NSEP framework are essential to the performance of the framework",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/14Ac60ilkX2p81Xho3KwlAZAl0Ek_VWvh/view?usp=sharing,Table 5. Performance comparison of the NSEP and its variants. The best result is in boldface.,Table ,Durga Srikari Maguluri
181,"In order to clarify the effectiveness of the designed micro semantic detection module, we conduct experiments on four experimental groups on the Chinese dataset and find that the experimental results support our hypothesis. The detailed experimental results are shown in Table 6.Group A, multihead attention. This group only uses multihead attention in the micro semantic detection module, with the number of heads initialed as 12. Because the original method performs best when the number of heads is 12.Group B, multihead sparse attention. This group only uses multihead sparse attention in the micro semantic detection module, with the same number of heads as group A, with a value of 12.Group C, multihead sparse attention + self-attention. This group combines 12 heads sparse attention and self-attention in the same way as they are applied in this article to perform a more comprehensive contrast experiment.Group D, multihead attention + sparse attention (our method). This group adopts sparse attention and 12 head attention.Three findings are demonstrated in the experimental results given by Table 6. First, Group A achieves better performance compared to Group B because multihead sparse attention reduces the computational complexity and inevitably losses some key dependency information captured from the input. Second, it is possible that self-attention fills in some of the information missed by multihead sparse attention, and the results of Group C outperform the method that only uses multihead sparse attention. Third, we observe that the combination of multihead attention and sparse attention can achieve the best performance, indicating that our method can more fully capture the differences between fake news candidates and the micro semantic environment than the other experimental group methods. The excellent results are achieved due to the sparse attention that eliminates the influence of redundant information by actively searching for active queries, which could be a new research perspective of reconciling the tension between fake news detection and heavy traffic data through model optimization",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/15J8ELXBaj5PbX1dGRvYojCyfPt6QYup_/view?usp=sharing,"Table 6. Contrast experiment of four experimental groups in micro semantic detection module, where the detector used is a representative BERT. The best result is in boldface",Table ,Durga Srikari Maguluri
182,"Effects of time length  in micro- and macro semantic environment distinguisher module. In this subsection, we discuss how the time length  required in the construction phase of the news semantic environment effects the performance of the proposed framework. Specifically, the time length  is set to [12, 24, 36, 48, 60, 72, 84, 96] with intervals of 12 h. The experimental results are shown in Table 7. The performance of the NSEP first increases and then decreases with increasing time length , with optimum performance when =72. It is believed that this occurs because the news scale extracted when  is too small is not enough to verify the authenticity of the news; however, when  is too large, some noise news is introduced, affecting the performance of the NSEP.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/112GseI7hwb4XslWhpGSXGN5c7yVdr-sn/view?usp=sharing,Table 7. The experimental results for different time lengths . The detector used is a representative BERT.,Table ,Durga Srikari Maguluri
183,"Effects of the ratio  in the micro- and macro semantic environment distinguisher module. In this subsection, we discuss what ratio  is needed in the construction of the macro semantic environment to make our NSEP work best. To set the parameter range in all directions, the ratio  is increased from 0.3 to 0.7 in intervals of 0.2. The results obtained are shown in Table 8. This method performs best when half of the news semantic environment is selected as the macro semantic environment. This is why  is set equal to 0.5.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1uheamCmsyzToC7uSC2O-5BuRxLfAbyLF/view?usp=sharing,Table 8. The experimental results of different ratios . The detector used is a representative BERT.,Table ,Durga Srikari Maguluri
184,"In addition, a study is conducted to verify the feasibility of the method. First, 200 samples are randomly selected for an independent sample T test, including 100 real news items and 100 false news items. For each sample, three news items from the macro environment and three news items from the micro environment are randomly selected to manually verify the authenticity of the target news. Second, five undergraduate students are selected to rate news items in both micro and macro environments, with the score set of each news item within the [0, 5] range. Scoring is based on to what extent the news item can prove the authenticity of the target post; that is, the higher the score, the greater extent to which the news item can prove the authenticity of the target news. The scores of three news items in the micro environment and three news items in the macro environment are added together to obtain manually measured micro confidence scores and macro confidence scores, with scores being within [0, 15]. To avoid subjective influence from individual students, we averaged the micro and macro confidence scores obtained by five students to obtain the final micro and macro confidence scores. Finally, the obtained data are used for independent sample T tests to compare the macro and micro confidence scores of true and false news. The conclusions are as follows. The S?W test is applied to the micro confidence score and the macro confidence score, and the significance P-values are 0.083 and 0.669, respectively, both of which are greater than 0.05. Therefore, the two groups of values are not significant horizontally, meeting the normal distribution. The results of the homogeneity of variance test show that the significance P-values for the micro and macro confidence scores are 0.094 and 0.892, respectively, not showing significance at this level. Therefore, the original hypothesis cannot be rejected, and the data satisfies the homogeneity of variance. Next, as shown in Table 9, the mean values of true and false news on micro confidence scores are 11.231 and 6.907, and the mean values on macro confidence scores are 9.525 and 6.496, respectively. The P value results of both significance results are less than 0.01, indicating significant differences in confidence scores between true and false news at both the micro and macro levels. This shows that it is the right choice for exploring the micro and macro semantic perception features to assist in identifying fake news candidates.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/1GNUlo1IjIfrEW_MCDbJmHul2zdqOg1QG/view?usp=sharing,Table 9. Comparison of differences in micro and macro confidence scores between true and fake news.,Table ,Durga Srikari Maguluri
185,"Then, binary logistic regression is performed on the data to explore whether there is a clear and simple relationship between the dependent variable, namely, true or false news, and the independent variable, namely, micro and macro confidence scores. We obtained a significance of 0.722 for the Hosmer-Lemeshow test, indicating that the equation obtained has a good degree of fit. The prediction accuracy percentage of the classification table shown in Table 10 can reach 0.81, which is a good result, indicating that simple environmental information can better assist in the detection of fake news and further supporting our speculation that micro and macro semantic perception features can more fully determine fake news.",Information Processing & Management,NSEP: Early fake news detection via news semantic environment perception,NSEP: Early fake news detection via news semantic environment perception - ScienceDirect,https://drive.google.com/file/d/15A2Ao3akLVR9TDeCx4MY0u84NFAPFKs5/view?usp=sharing,"Table 10. The classification table in binary logistic regression, where 0 indicates that the news is real and 1 indicates that the news is fake.",Table ,Durga Srikari Maguluri
186,"The existence and importance of the referent utterance have been emphasized in 1981 by the Centering Theory (Grosz et al., 1995, Joshi and Weinstein, 1981), which holds that there is always a center utterance in the context which is most related to the next utterance. For example, in Fig. 1,  is the next question which talks about “band lacked effort” and  talks that as one of reasons for breaking up, which is the referent to ask . Thus,  is the center of .Different from existing works, we propose to identify and retain the center utterance in the context, ensuring semantic coherence during fine-tuning and thus improving model performance on ranking. As shown in the bottom of Fig. 1, it is likely that previous methods (Han et al., 2021) occasionally fine-tune the rank models with a context–question pair by masking . This will largely reduce the coherence between the context and the question, thus leading to performance degradation for CQR. Instead, we can avoid such situation by keeping the center  Therefore, we propose a plug-and-play UCAR framework to train the rank model with auxiliary fine-tuning tasks by retaining the center. First, we propose an unsupervised MCD module to identify the center by summing center scores calculated from multiple perspectives. Then, we devise an auxiliary center-retained conversational question ranking (CR-CQR) task to fine-tune the rank model by masking non-center utterances in the context, expecting to learn a robust representation (Vincent, Larochelle, Bengio, & Manzagol, 2008) and prevent overfitting (Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov, 2014) while preserving the coherence between the context and the question. Experimental results on two benchmark datasets show that:when fine-tuning.",Information Processing & Management,Center-retained fine-tuning for conversational question ranking through unsupervised center identification,Center-retained fine-tuning for conversational question ranking through unsupervised center identification - ScienceDirect,https://drive.google.com/file/d/1x-LAqwmdC8i00Y0ZZsedl8UOeNOzv3Qp/view?usp=sharing,Fig. 1. An example of Centering Theory and our proposed fine-tuning task.  represents the utterance in the th turn of the context.  denotes the next question.,Flow Diagram ,Durga Srikari Maguluri
187,"The overall performance of all methods on CANARD and QReCC are listed in Table 1. DPT, FP and DP are post-training methods which apply domain-specific fine-tuning followed by task-specific fine-tuning. UMS, CLR and LCC are fine-tuned by multi-task learning with their designed tasks. HCL applies the curriculum learning technique in fine-tuning. Note that our UCAR can be equipped with any methods.From the results, we can see that UCAR brings improvement to all methods on two benchmark datasets. Specifically, UCAR outperforms CLR and DP by 1.5% in terms of MRR on CANARD and QReCC. This indicates that the CR-CQR task which masks non-center utterances is really beneficial by keeping semantic coherence in conversations when fine-tuning.Besides, we observe that the improvement on QReCC is larger than that on CANARD in most cases. For instance, UCAR improves nearly 1% over DPT, UMS and FP in terms of P@1 on CANARD, while the improvements are 2% on QReCC. The reason is that the samples in QReCC are more diverse and cover more domains than CANARD and UCAR learns a robust representation and generalizes better to QReCC.",Information Processing & Management,Center-retained fine-tuning for conversational question ranking through unsupervised center identification,Center-retained fine-tuning for conversational question ranking through unsupervised center identification - ScienceDirect,https://drive.google.com/file/d/1mbXJk8HGQksBQfnDjYnVnBMat5Q3BS6Q/view?usp=sharing,Table 1. Overall performance (%) on CANARD and QReCC. Boldface indicates the best results in terms of the corresponding metrics. Underline mark indicates the improvement equipped with UCAR exceeds 1%.,Table ,Durga Srikari Maguluri
188,"As the CANARD and QReCC datasets do not contain center annotation, we conduct human evaluation on 500 random samples and evaluate the CQR performance for the identified centers from “Leave-one-out center score”, “Isolation center score”, “Preceding-aware center score”, “Succeeding-aware center score” and MCD. We assign each sample to three annotators and obtain the labels based on majority voting. The hired annotators are well-trained and informed of our task. For each context–question pair and the identified center, we ask the annotators to choose a score among 0, 1, 2, and 3, representing the validity of the identified center. The overall Fleiss’ kappa among the three annotators is 52.3%, which means that the opinions of the three annotators are very consistent.The evaluation results are shown in Table 2. We first list the proportion of centers scored by 3, 2, 1 and 0 respectively, then we calculate a linear score 
 to indicate the overall evaluation. Here, denotes the proportion of centers with the score Finally, we list the CQR performance by applying the corresponding center identification methods. We find that the centers identified by MCD have high agreement with those identified by human. As we can see, 79.2% identified centers receive the highest scores by human, which shows that most human annotators agree that the identified centers are indeed the true centers.The results in Table 2 show that MCD achieves best performance for center identification and CQR. 79.2% of the samples from MCD get the highest human evaluation score. MCD achieves 0.3% improvement in terms of P@1 over “Leave-one-out” which has the best CQR performance. The reason is that MCD can summarize the results of all perspectives and select the best one among them.Interestingly, although there is only marginal improvement in MCD compared to “Isolation”, MCD outperforms “Isolation” by a large margin in CQR performance. As we can see, MCD outperforms “Isolation” by 0.02 in terms of but 0.4 in terms of P@1. We think that there are some samples which have minimal effect on human evaluation but significant effect on the final CQR performance. MCD can do better on these samples by learning a weight for center identification, as shown in Eq. (13).Besides, the centers identified by “Preceding-aware center score” and “Succeeding-aware center score” are scored relatively low, especially the latter, which reaches 2.06 and 1.23 in linear score , respectively. This is because the desired centers are mostly in the first turn or the last turn, especially the last turn (49.4%). For those samples, “Preceding-aware center score” and “Succeeding-aware center score” have no advantage",Information Processing & Management,Center-retained fine-tuning for conversational question ranking through unsupervised center identification,Center-retained fine-tuning for conversational question ranking through unsupervised center identification - ScienceDirect,https://drive.google.com/file/d/10p7FMXM4IFQ3Rjf_ja4jKtin0qdjNY58/view?usp=sharing,Table 2. Human evaluation and automated metric assessment on CQR performance of center identification (%) on CANARD. .,Table ,Durga Srikari Maguluri
189,"To verify the stability of each center identification method, we compare the identification differences on two well-trained rank models. As shown in Table 3, MCD is the most stable and effective center identification method. The number of different identifications on the two rank models is 72, which is lower than that of “Leave-one-out”, i.e., 86. Besides, of MCD drops from 2.45 to 2.41 on shadow rank model, while that of “Isolation” drops from 2.43 to 2.32 and that of “Leave-one-out” drops from 2.29 to 2.23, compared to on rank model. When the model changes, the calculated center scores of single perspective center identification methods change, leading to unstable results. Therefore, we propose MCD to summarize the results of all methods for more stable and effective identification.We also select a representative case to demonstrate the stability of MCD in Section 5.7. In this case, the center scores calculated by each single perspective identification method vary largely on different rank models, leading to various center identification results. Nevertheless, our proposed MCD makes stable and correct identification even based on different rank models. We believe that stability is another aspect of MCD’s superiority.",Information Processing & Management,Center-retained fine-tuning for conversational question ranking through unsupervised center identification,Center-retained fine-tuning for conversational question ranking through unsupervised center identification - ScienceDirect,https://drive.google.com/file/d/11jWslecIdDdGrre7GVRlufJE9IggMdyV/view?usp=sharing,"Table 3. Difference of center identification on rank model and shadow rank model, as introduced in Section 3.2.2. Here, ‘Diff’ and ‘Diff ratio’ denotes the number and ratio of different identifications on the two rank models in 500 annotated samples.",Table ,Durga Srikari Maguluri
190,"To address the above existing problems, we propose a novel MKL graph clustering method, i.e., pure kernel graph fusion tensor subspace clustering (PKGT) under the non-negative matrix factorization framework. In PKGT, to avoid the noise effects of kernel data in the process of optimizing the target affinity graph, we deal multiple feature matrices generated by MKL through NMF, and construct pure local affinity feature graphs with exact block diagonal structure by using consistency class indicator matrices. To better find the potential inconsistent information existing between multiple local affinity feature graphs, we design and implement a weighted strategy which is based on the rank-constrained bipartite graph fusion method to achieve the optimal weight assignment. Finally, we stack the multiple local feature affinity graphs after assigning weights into a third-order tensor, and use the tensor nuclear norm (TNN) constraint to capture the higher-order correlation between different local affinity feature graphs, to obtain more feature information. After PKGT completes the iterations, we take the average of all the frontal slice sums of this tensor as the target affinity graph. Fig. 1 shows the framework structure of the PKGT method. The main contributions of this paper are as follows:We proposed an adaptive weighted local affinity feature graph learning method for direct use in MKL scenes firstly, which directly optimizes the interrelationships among multiple local affinity feature graphs to learn the target affinity graph, effectively avoiding the interference of kernel noise.We construct the fused bipartite graph to supervise the weight assignment in the model optimization process, which can assign the optimal weight to each local affinity feature graph.We have stacked the weighted local affinity feature graphs to the graph tensor, to capturing the higher-order correlation between different local affinity feature graphs and providing more feature information for the learning of the target affinity graph.By comparing with the existing state-of-the-art MKL subspace clustering methods and DSC methods, the PKGT method has superior performance.The rest of the paper is organized as follows: Section 2 presents the related work, Section 3 describes the proposed PKGT model, Section 4 provides the experimental validation of the PKGT model, and Section 5 draws conclusions.",Information Processing & Management,Pure kernel graph fusion tensor subspace clustering under non-negative matrix factorization framework,Pure kernel graph fusion tensor subspace clustering under non-negative matrix factorization framework - ScienceDirect,https://drive.google.com/file/d/1bpoe-AvXBBh9PVK-lqvblXPdviCYkFeR/view?usp=sharing,Fig. 1. Schematic diagram of the PKGT structure.,Flow Diagram ,Durga Srikari Maguluri
191,"n order to scientifically evaluating the clustering performance of the PKGT method, we selected nine widely used real datasets. They include: one floral dataset, IRIS1 ; four face datasets, YALE,2 ORL,3 AR4 and Umist5; tow object dataset, COIL206 and COIL1007; two character datasets, BA8 and MNIST.9 The MNIST dataset takes the first 200 images of each class, and dimensions reduced to 500. All the details of these nine datasets, we count them as Table 2.",Information Processing & Management,Pure kernel graph fusion tensor subspace clustering under non-negative matrix factorization framework,Pure kernel graph fusion tensor subspace clustering under non-negative matrix factorization framework - ScienceDirect,https://drive.google.com/file/d/1JE2AUHhDOqZWB4YgdjBC7mmXTW2s3o3U/view?usp=sharing,Table 2. Details of the datasets used in the experiments.,Table ,Durga Srikari Maguluri
192,"VL is particularly important in the online environment, as learning from one's own experience through direct instruction can be challenging (Hua et al., 2023; Myers, 2021). As the interpersonal nature of learning in communities and organizations has increased, individual informal learning has shifted from independent and one-way forms of vicarious learning (IVL), such as observing, to engaging in back-and-forth discussions with others (coactive vicarious learning, CVL) (Dai & Shi, 2022; Myers, 2018). CVL is a give-and-take process that covers sharing, questioning, explaining, debating, and support, and enables better contextualization and co-construction of the meaning-making process (Myers, 2020). Chen et al. (2017) and Wang and Yu (2017) view observational learning as a cognitive evaluation bridge connecting the social environment and the user's decision-making behavior. In other words, they posit that VL relies on external environmental support and that users can modify their behavioral outcomes through indirect learning. Myers (2018) conceptualizes VL in terms of “contextual antecedents—CVL—developmental outcomes” in an organizational environment. This paradigm theorizes the importance of relational context (e.g., contexts involving relationship quality, familiarity, and affective tone) and structural context (e.g., proximity, role structure, and task structure) in facilitating the occurrence of VL in respect to frequency and intensity. It also conceptualizes the outcomes of VL interactions on the individual level (e.g., awareness, knowledge, and skills) in organizations (as shown in Fig. 1).While Myers’ (2018) VL paradigm was initially introduced in an organizational context, he emphasized the importance of further investigating how this framework may apply in different contexts, especially in computer-mediated environments. Currently, VL has been widely explored in the fields of organizational behavior research and social commerce across diverse knowledge domains. In organizational settings, VL is widely acknowledged as a powerful predictor of enhanced employee performance. For instance, observational learning processes significantly improve task performance (Yi & Davis, 2003). Similarly, team performance is notably influenced by VL reciprocity (Myers, 2021), which can increase knowledge reuse among employees (van Zoonen et al., 2022). Benabid and Mikhaeil (2019) developed a typology of informal VL behaviors based on two learner roles – consumer and producer – within the symbolic learning environment of Web 2.0. Myers (2020) pointed out that social media can amplify the benefits of VL, as through its interactive functions it enabled health professionals to share techniques and seek advice to fight COVID-19. In social commerce, customers primarily engage in observational learning behaviors, such as browsing ratings and reviews posted by others or considering recommendations from their social networks, to assess information. This cognitive process significantly influences their attitudes toward products and websites, as well as their purchasing decisions (Chen et al., 2017; Wang & Yu, 2017). In sum, learning is associated with different types of knowledge in various contexts.Given the characteristics of the learning environment provided by live streaming (e.g., real-time visual presentation and strong synchronous social interaction) (Men et al., 2023), the VL theory is well-suited for application to the context of live streaming. The latest research has focused increasingly on the role of VL in enhancing credibility within the context of new social media, particularly live streaming (Hua et al., 2023; Li et al., 2020). In live streaming, consumers can directly and instantly interact with the streamer to compare and analyze relevant content, which reduces their perception of uncertainty toward others regarding the authenticity of the demonstrations and communication. Furthermore, they can act as observers and have a virtual psychological experience resembling a concrete experience by observing and perusing comments and behaviors during live broadcasts. Ultimately, they make action decisions through this vicarious cognitive process (Men et al., 2023). Therefore, our research posits the coexistence of two complementary forms of learning in live streaming – IVL and CVL. Given the informal nature of VL, Myers’ (2018) framework is suitable for analyzing general live-streaming scenarios with frequent social interactions to understand the contextual factors that contribute to VL and its developmental outcomes in the live-streaming context.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1ZUp1O81iYvUjTBTAoIgZT7r3tX4GZGeH/view?usp=sharing,"Fig. 1. Conceptualization of VL. (Adapted from Myers, 2018).",Flow Diagram ,Durga Srikari Maguluri
193,"In general, users exhibit selective cognition regarding information adoption, tending to doubt inconsistent information and selectively attending to information congruent with others in order to appear rational (Chou et al., 2015). Meanwhile, the level of alignment among the information shared on social media platforms (also comprehended as accuracy) determines knowledge consensus, which can facilitate knowledge adoption by influencing users' trust in the information provided (Huo et al., 2018; Jin et al., 2021). We argue that information with a high knowledge consensus encourages people to adopt it. Moreover, a prevalence of homogeneous content in live streaming, to some extent, implies common knowledge (Wang et al., 2019). Therefore, the convenience and quick perception of knowledge consensus among live-streaming users will lead to knowledge adoption. On one hand, knowledge consensus can strengthen PSI by bolstering credibility. In addition, users who have parasocial relationships with streamers or viewers are driven by their emotional attachment to adopt knowledge, and this emotional connection may be intensified through knowledge that achieves consensus, thereby further increasing their knowledge adoption. That is, PSI is more likely to be stronger among individuals who share similar beliefs (Sokolova & Kefi, 2020), and such users are typically more willing to adopt knowledge from individuals who disseminate knowledge that attains consensus.On the other hand, when users engage in VL, knowledge consensus can help them to make value judgments about the information/knowledge while watching live broadcasts or interacting with live members. Even when users come across content in live streaming that they do not fully trust, they can enhance their perception of its reliability and embrace information adoption through the recognition of knowledge consensus and consistency (Chou et al., 2015). Especially in live broadcasts, where a large amount of homogeneous content can be found, knowledge consensus is a way to gauge credibility and reliability. That is, when users perceive knowledge held in common across different live broadcasts, it helps them save time from having to search for and carefully evaluate information (Ayeh et al., 2013). In essence, developing a perception of knowledge consensus and trust provides a sense of reliability for further VL. Users engaged in VL benefit from increased information sharing and knowledge acquisition through more dependable relationships (Myers, 2018). In other words, when viewers come across information/knowledge that aligns with the viewpoints of other platform users, they are more likely to embrace that information/knowledge. Based on the above argument, we assumed that knowledge consensus serves as the primary moderating variable that promotes knowledge adoption:",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1SianF1ryw8kxZqgQnmTW6JSCLlIOmRkB/view?usp=sharing,Fig. 2. The research model.,Flow Diagram ,Durga Srikari Maguluri
194,"Discriminant validity was assessed using three different approaches: the Fornell-Larcker criterion, the matrix of loadings and cross-loadings, and the criterion involving the heterotrait-monotrait (HTMT) correlation ratio. Table 3 shows that the square root of the AVE of each construct is evidently higher than its correlations with other constructs, confirming the discriminant validity (Fornell & Larcker, 1981). Knowledge adoption showed a significant positive correlation (r = 0.568, p < 0.01; r = 0.564, p < 0.01; r = 0.616, p < 0.01) with both vicarious learning (CVL and IVL), and PSI. As shown in Table 4, the loadings of all the items exceed the cross-loadings, further confirming discriminant validity (Chin, 1998). Furthermore, all the HTMT ratios of correlations (Henseler et al., 2015), shown in Table 5, exhibited values below the conventional most-critical value of 0.85 (Voorhees et al., 2016). These three rigorous tests verified the discriminant validity of our model.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1oEoupVeoX6fSh9I3obloYzlXN195LFXx/view?usp=sharing,Table 5. Heterotrait-monotrait (HTMT) ratios.,Table ,Durga Srikari Maguluri
195,"A thorough analysis of the structural model was carried out to evaluate directional relationships during the different stages of knowledge adoption development and to test hypotheses. Bootstrap resampling was performed with 5000 iterations, using SmartPLS 4.0 to ensure result stability. The t-values, path coefficients, and R-squared values are reported in Table 6 and Fig. 3. The hypotheses testing results show that VL significantly and positively affected knowledge adoption (? = 0.358, p < 0.001), supporting H1. Furthermore, the affordances of metavoicing (? = 0.207, p < 0.001), communication (? = 0.218, p < 0.01), browsing others' content (? = 0.188, p = 0.001), and relationship formation (? = 0.197, p < 0.01) had significant positive effects on VL, supporting H2a, H2b, H2c, and H2d. Additionally, PSI is seen to positively influence both VL (? = 0.206, p < 0.001) and knowledge adoption (? = 0.225, p < 0.001), indicating that both H3 and H4 are supported. Although the metavoicing affordance does not show a significant effect on PSI (? = 0.076, p > 0.1), the significant positive effects of communication (? = 0.214, p < 0.01), browsing others' content (? = 0.362, p < 0.001), and relationship formation (? = 0.263, p < 0.001) on PSI provide support for H5b, H5c, and H5d. Overall, the results confirm all the hypotheses except H5a.This study also investigated how the moderating effects of knowledge consensus come into play. To visually represent these effects, slope graphs depicting the interactions are provided in Fig. 4, Fig. 5. Fig. 4 illustrates how PSI impacted the adoption of knowledge under conditions of high knowledge consensus (M + 1 SD) and low knowledge consensus (M – 1 SD). It indicates that knowledge consensus (? = ?0.168, p = 0.036) served as a negative moderator in the PSI-knowledge adoption relationship, which contradicts our previous hypothesis. Fig. 5 illustrates the predictive trends of VL on knowledge adoption, also under conditions of high knowledge consensus (M + 1 SD) and low knowledge consensus (M – 1 SD). It shows that the interaction between knowledge consensus and VL also had significant moderating effects on knowledge adoption (? = 0.276, p = 0.001), indicating that knowledge consensus positively moderated the VL-knowledge adoption relationship.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1nFILWjOLNSU5yn6hZOrK0QeShpYqAwh3/view?usp=sharing,Fig. 3. Results of the structural model and hypotheses testing.,Flow Diagram ,Durga Srikari Maguluri
196,"To determine whether each social media affordance impacted VL through PSI, as well as whether PSI indirectly impacted knowledge adoption through VL, a post-hoc analysis was conducted to examine the results of mediation using the bootstrapping procedures (5000 resamples) available in SmartPLS (Vance et al., 2015). The results presented in Table 7 indicate that VL partially mediated the relationship between PSI and knowledge adoption (? = 0.074, p < 0.01). Specifically, VL transmitted a portion of the impact from PSI to knowledge adoption. Moreover, we found that the effects on VL of the affordances of browsing others' content (? = 0.074, p < 0.01), communication (? = 0.044, p < 0.05), and relationship formation (? = 0.054, p < 0.05) were partially and positively mediated by PSI, but PSI did not mediate the effect of metavoicing.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1BGKqp5RyskJtoX01wnvVGAKLXuJoF4YR/view?usp=sharing,Table 7. Results for mediating effects.,Table ,Durga Srikari Maguluri
197,"Table 1 shows the overall performance on One-Billion-Word and Yelp. We can observe that: (1) Our method can outperform all baselines in most metrics on different datasets, demonstrating the effectiveness and generalization ability of our model. (2) CGMH achieves the best performance on SB-4, D-2, and D-4. Because it comes at the expense of degrading text quality, which is consistent with previous work (He, 2021, He and Li, 2021). (3) On the content quality metrics (BLEU, NIST, and M), our proposed method gets improved slightly. Because DM can force to mask some generated tokens, using more keywords to improve the quality of the generated tokens. (4) On the diversity metrics (SB-4, D-2, and D-4), our model achieves a great improvement compared with state-of-the-art CBART and POINTER. Because we flexibly insert multiple tokens per action, which brings more different generation distributions over vocabulary, and thus generates longer and more candidate sentences. (5) Our approach outperforms previous works (CBART and POINTER) in both the BERT and BART models, showcasing superior performance for our approach and illustrating its generalizability.For complementary to automatic metrics, we conduct a human evaluation. We randomly select 50 sentences and invite three volunteers4 to compare the generated sentences with CBART and Human Reference. Following previous works (He, 2021, Zhang et al., 2020), we use the Fluency and Complete to demonstrate the text quality and use the Informativeness and Correlation to demonstrate the diverse text. For inter-annotator agreement, the values of Cohen’s kappa (Fleiss, 1971) are 0.67, 0.79, 0.69, and 0.62 for Fluency, Complete, Informativeness, and Correlation. From the Table 2, we can see that: (1) Quality. Compared with Human Reference, the results of our proposed method still have a large gap. But we are preferable to CBART. (2) Diversity. Human reference still has an overwhelming advantage, but the performance gap between our proposed method and CBART gets larger, demonstrating the effectiveness of our proposed lightweight refinement strategy.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1TiWlFt6cziGPoeeRRglc5pmuu1l6RElL/view?usp=sharing,Table 2. Human evaluation on One-Billion-Word.,Table ,Durga Srikari Maguluri
198,"Effect of DM and DR. We conduct experiments to analyze the effect of DM and DR, as shown in Table 3. We can see that: (1) Without the DM, the performance of B-2 and N-2 is reduced, and the results of SB-4 and D-2 are better than our method. These indicate two facts: First, DR does enhance the diversity of texts, but at the same time, it can cause a decrease in text quality. Second, the quality and diversity of the text are a game, and DM can balance them by providing more keywords, while improving the diversity and quality of the text. (2) Compared w/o DM with w/o DR & DM, all results of w/o DM are better than w/o DR & DM, particularly in terms of diversity. This demonstrates that inserting multiple tokens at once makes sentences longer and increases text variety without degrading the text quality. (3) Removed DR leads all results worse than our approach, especially in the diversity. The major reason is that DR can flexibly insert multiple tokens between two keywords to improve diversity. (4) Compared w/o DR with w/o DR & DM, DM can improve text quality, but it has little impact on diversity.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1kyoNH_qIkgNmGvF1xzbZWruMzWCK5nLc/view?usp=sharing,Table 3. An ablation study on One-Billion-Word. The number of constrained keywords is 4.,Table ,Durga Srikari Maguluri
199,"Effect of the Number of Constraints. As shown in Table 5, as the number of constrained keywords increases, the scores of B-2 and N-2 increase rapidly. These results are consistent with our hypothesis that the model can generate high-quality text only if there is sufficient context information. Therefore, it is reasonable to increase the ratio of masking on generated tokens in the last iteration, because there are more given works for the current iteration. Besides, we also observe a similar trend in terms of diversity. This is because more keywords provide a longer sentence as initialization, and it is relatively easy to generate longer sentences, which brings a higher probability of involving more different words.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1huuVlElukTMfvpIGwpQk_4qEfKMonWgD/view?usp=sharing,Table 5. The number of constrained keywords on One-Billion-Word.,Table ,Durga Srikari Maguluri
200,"(2) From Fig. 5 (Top)5, we can see that our method has a similar number of Verb, Pronouns, MD, Adjective, and Adverbs with Human. Only the Noun is slightly less than Human. But the number of CBART for POS tagging is far less than that of Human.Case Study. From Fig. 5 (Bottom), although our method and CBART have good fluency and grammatical rules, etc., generated sentences via our method are longer and contain richer information, such as more Verb (voiced, called) and Pronoun (that) in first example.",Information Processing & Management,"How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus","How vicarious learning increases users’ knowledge adoption in live streaming: The roles of parasocial interaction, social media affordances, and knowledge consensus - ScienceDirect",https://drive.google.com/file/d/1yJEFz6-7fsbmK8JnIT8chlCs00WF-TvB/view?usp=sharing,"Fig. 5. At the top, we count the number of POS tagging. At the bottom, we show generated texts by CBART and our method with same keywords extracted from One-Billion-Word test. MD is Modal verb.",Table ,Durga Srikari Maguluri
201,"Sparse Temporal Knowledge Graph (STKG) is a temporal knowledge graph with fewer facts and relationships along the time dimension. Due to storing only essential information, coupled with the continuous updates and changes in knowledge based on actuality, STKGs miss facts and relationships at various time points. Fig. 1 presents an example of a sparse temporal knowledge graph. Suppose a user wants to query the location of ""Sam"" at ""9:40?. Due to the absence of the quaternary relationship (Susan, Meet, Mary, 9:30), the evidence chain between ""Sam,"" ""Suan,"" and the target entity ""Supermarket"" is severed at different time points, making it impossible to retrieve the desired information through multi-hop path reasoning methods. Additionally, due to severe information gaps, STKGs face numerous issues such as uncertain answers, incomplete relationship paths, and difficulties in updates.To tackle this issue, TKG reasoning gains significant research focus. Some reasoning models (García-Durán et al., 2018; Goel et al., 2020; Leblay et al., 2018; Messner et al., 2022; Xu et al., 2023) learn features of entities, relations, and timestamps on the temporal knowledge graph. They map entities, relations, and timestamps to a low-dimensional vector space to compute similarity or distance between them, enabling reasoning over the problem in the low-dimensional vector space. Although these models perform well, they fail to provide interpretable evidence chains for the reasoning results and lack expressive power for complex relationships. To tackle this problem, some researchers model temporal knowledge graph reasoning as a multi-hop path reasoning task (Bai et al., 2023; Bai et al., 2021; Du et al., 2023; Han et al., 2021; Lin et al., 2018; Lv et al., 2020; Sun et al., 2021; Xia et al., 2022). These models perform reasoning by tracking the propagation of information along paths in the knowledge graph, which not only provides the interpretability of the reasoning outcomes but also offers explicable evidence chains to showcase the reasoning process.lthough the existing multi-hop path reasoning models demonstrate promising performance in TKG reasoning, there are still three major challenges when applied to STKGs. (1) Path sparsity: multi-hop path reasoning models rely on evidence chains for reasoning search, but STKGs contain very few paths, making it difficult to find reasoning paths that reach the correct target entity (Lv et al., 2020; Xia et al., 2022). (2) Sparse rewards: existing reinforcement learning-based multi-hop path reasoning models mostly focus on hit rewards, resulting in lower hit rates for reasoning on STKGs. This directly affects the training effectiveness of reinforcement learning agents due to the problem of sparse rewards (Lin et al., 2018). (3) Lack of information: STKGs contain limited information, making it challenging for agents to select accurate reasoning paths (Bai et al., 2021; Bai et al., 2023; Du et al., 2023; Han et al., 2021; Sun et al., 2021).",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1DKdEGwVjecJo_ltABt_ZTeMSKucR_A2l/view?usp=sharing,"Fig. 1. An example of multi-hop path reasoning task on STKG. The solid black arrows represent existing relationships between entities, the dashed black arrows represent missing relationships between entities, and the dashed red arrows represent missing path segments in the multi-hop path reasoning task.",Flow Diagram ,Durga Srikari Maguluri
202,"In this section, we present an introduction to our model. Firstly, we introduce a reinforcement learning framework for multi-hop path reasoning. Then, we introduced a dynamic completion strategy based on temporal embedding models. Then, we introduced the design of a new reward function. Finally, we introduced the training process of STKGR-PR. The overall framework diagram of our model is depicted in Fig. 2. Part 1 of Fig. 2 shows the overall design of our model.State. In the given definition, we can represent the state after the agent completes the t-hop path search as Here, represents the current entity, represents the query relation, and represents the historical search paths up to the t-hop, which includes the LSTM-encoded information of the previous hops. LSTM (Long Short-Term Memory) is a variant of recurrent neural networks that is widely used in sequence data processing tasks. In the context of multi-hop path reasoning, LSTM is utilized as a model to capture and handle long-term dependencies within input sequences. During the multi-hop path reasoning process, the agent's decision is influenced by multiple factors. In addition to the query relation and the timestamp information corresponding to the query fact, the historical search paths also play a role. These factors collectively affect the agent's decision-making and path selection at each step.Action. For the current state the action space can be defined as the set of all outgoing edges from the vertex In other words, if there exists a quaternary relation then is an action for the current state where represents the entity associatedwith in TKG, represents the relation between and represents the time when the fact occurs. The complete set of actions for state forms the action space denoted as Before reaching the maximum limit of hops, if the agent finds the correct target entity, it should stay at the current entity node. Therefore, for each state we also need to add a self-loop action ""self-loop"" to the action space specifically represented as (""self-loop"": When and the number of hops searched is still within the given limit, the agent should choose to stay at the current node. Hence, the self-loop action serves as a ""stop"" operation.",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1W2xQCGwGUkmldFtpseQbKe4INY0zHanQ/view?usp=sharing,Fig. 2. Framework diagram of our model STKGR-PR. Part 1 provides an overview of the multi-hop path reasoning process and the design of reward functions guided by a dynamic completion strategy based on temporal embedding models. Part 2 provides a detailed introduction to the construction process of potential action spaces guided by dynamic completion strategies. Part 3 provides a detailed introduction to the design of soft rewards based on temporal path embedding.,Flow Diagram ,Durga Srikari Maguluri
203,"There are two stages in the complete training procedure. The initial stage incorporates pre-training, which uses the model of temporal embedding. Following this, we proceed to train our reinforcement learning agent in the second stage.Fig. 3 illustrates the iterative training process of STKGR-PR. First, the current state is passed to the reinforcement Learning (RL) agent, which searches for actions on the original TKG. If the agent cannot find any paths in the current state, it utilizes high-quality potential actions obtained from the temporal embedding model. Then, the policy network prompts the agent to choose a specific action, denoted as which is then communicated to the following state, This process repeats until either the agent successfully reaches the designated target entity or the maximum reasoning length is reached. Finally, the agent updates according to the final reward composed of hit reward, temporal embedding model soft reward, and path embedding soft reward.Our strategy is to train the policy network by maximizing the expected reward for each query quadruple in the TKG, aiming to achieve optimal performance with our policy network. To optimize the parameters of the policy network, we use the REINFORCE algorithm to train our agent. The expected reward for each query quadruple in the training set is calculated as follows:",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1XQFipKXfYcl5AN51n0cIcO7pttKn-_j1/view?usp=sharing,"Fig. 3. The training process of our model STKGR-PR.  represents the current state, and  represents the high-quality potential action set obtained based on the embedded model in the corresponding state.  represents the action vector in the corresponding state selected by the agent through the policy network. The dashed line represents a situation where the agent selects actions from a set of potential actions due to a lack of path.",Flow Diagram ,Durga Srikari Maguluri
204,"For the purpose of evaluating our model, we have utilized ICEWS, which includes two sub-collections of data - ICEWS14 and ICEWS0515. These datasets have been extensively used as benchmark datasets for TKG link prediction, and contain event records with temporal information, represented by a quadruple vector that comprises of head entity, relation, tail entity and timestamp. In order to analyze the reasoning effect of our approach on TKGs that exhibit varying levels of sparsity, we have constructed six separate sparse sub-datasets that are based on ICEWS14 and ICEWS05-15. These sub-datasets have been created by randomly retaining different proportions of data - 2 %, 3 % and 5 % for ICEWS14 and 0.3 %, 0.4 % and 0.5 % for ICEWS05-15. We name these datasets as ICEWS14-2 %, ICEWS14-3 %, ICEWS14-5 %, ICEWS05-15-0.3 %, ICEWS05-15-0.4 %, and ICEWS05-15-0.5 %. To better describe the sparsity level of these datasets, we calculate the average out-degree and in-degree of the TKG in these sparse datasets. These statistical data are shown in Table 2.",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1511lUpYVwdteTIQzdMpdlCbMBo_6DPvD/view?usp=sharing,Table 2. Statistics of the six datasets in our experiments.,Table ,Durga Srikari Maguluri
205,"Tables 3 and 4 present the link prediction results on six sparse temporal datasets. In the tables, embedding-based methods are displayed in the upper section, with the best scores marked with an underscore; multi-hop path-based methods are displayed in the lower section, with the best scores marked in bold. All evaluation metrics are represented in percentages. Considering the uncertainty introduced by randomly retaining data in the sparse temporal datasets and its potential impact on the credibility of the experimental results, we conducted 30 rounds of training and testing for STKGR-PR on each dataset separately, and took the average of the experimental results as the link prediction performance of STKGR-PR on that dataset. The link prediction results of the comparative methods were generated using their source code.Table 4 presents the link prediction results on ICEWS05-15-0.3 %, ICEWS05-15-0.4 %, and ICEWS05-15-0.5 %. Similar to the results on ICEWS14, STKGR-PR outperforms previous multi-hop path reasoning models on all evaluation metrics. This indicates that our model is applicable to other STKGs and exhibits universality. Furthermore, we observe a decreasing trend in the relative improvement rates of STKGR-PR as sparsity decreases, indicating the generalizability of this pattern. Fig. 4 illustrates the variation of the improvement rates of STKGR-PR over the baseline model IMR with changes in dataset sparsity.",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1NYvcPNrs-_60rkdHLgaxUCefOklBV7Ck/view?usp=sharing,"Table 4. Link prediction results on three sparse temporal datasets: ICEWS05-15-0.3%, ICEWS05-15-0.4%, and ICEWS05-15-0.5%.",Table ,Durga Srikari Maguluri
206,"In this section, we performed ablation study to investigate the influence of various modules on the link prediction outcomes of STKGR-PR. The results of the experiments are presented in Tables 6 and 7. As can be observed from the tables, the elimination of any of the three modules results in a reduction in model performance, indicating the advantageous character of these modules for our model.Temporal Vector. We observed that after removing the temporal vector, we obtained a static model. Compared to STKGR-PR, the performance of the static model significantly decreased. Specifically, for the evaluation metric Hits@10, the static model exhibited a reduction of 6.04 %, 3.66 %, and 3.96 % on ICEWS14-2 %, ICEWS14-3 %, and ICEWS14-5 % respectively. This indicates that incorporating the temporal vector effectively addresses the issue of STKG lacking information. Additionally, we found that for the Hits@10 metric, STKGR-PR had a relative improvement rate of 31.41 % on ICEWS14-2 % and 13.45 % on ICEWS14-5 %. This is predictable because as sparsity increases, the inclusion of temporal information becomes more significant due to the diminishing amount of information present in the TKG.",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping - ScienceDirect,https://drive.google.com/file/d/1mV1ie1jIKrffi-MRtfINotj_308fpjHr/view?usp=sharing,"Table 6. Ablation study result on ICEWS14-2%, ICEWS14-3% and ICEWS14-5%. Time represents the time vector, DC represents the dynamic completion strategy, and TPR represents the temporal path embedding soft reward.",Table ,Durga Srikari Maguluri
207,"It should be noted that the magnitude bias is not a major issue in achieving high accuracy (i.e., the average Recall@20 or NDCG@20) on IID test data. Under the evaluation with long-tailed data, it is easier to hit common interactions than unique interactions (i.e., interaction with unpopular items). The magnitude of the item vector in the inner product may be particularly favorable for this evaluation protocol, although it has negative consequences for the recommendation of unpopular items. Historically, the performance of recommendation models has been measured on IID test data (i.e., long-tailed data), and various inner product-based recommendation models have achieved high accuracy (He et al., 2020, Rendle et al., 2012, Wu, He et al., 2022).",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Flexibly manipulating popularity bias for tackling trade-offs in recommendation - ScienceDirect,https://drive.google.com/file/d/16GoIUvq4G--yH6doOQlEqSGA7f5r6_eI/view?usp=sharing,Table 1. Comparison of the accuracy when the cosine similarity is adopted.,Table ,Durga Srikari Maguluri
208,"For the normal models, which exhibit popularity bias, the accuracy for UWN is lower than that of Overall and UWP This means that these models are unsuitable for users seeking novel and niche itemS.CPR can be said to provide a fair recommendation for UWP and UWN In addition, compared with LightGCN and BPR-MF, although Overall accuracy of CPR is inferior, the accuracy for UWN is higher However, the accuracy for UWP is much lower, suggesting that some users would be disadvantaged by popularity bias mitigation DirectMag can achieve high accuracy for both UWN and UWP by adjusting  In particular, the improvement for UWN is remarkable, and DirectMag will help users discover novel items Furthermore, we assume an ideal situation when each user can adjust within the range shown in Fig. 7, to select the recommendation list that hits the most items Table 3 shows that DirectMag achieves a much higher accuracy if the user can ideally adjust 
Compared with the curve of DirectMag shown in Fig. 5, we can easily find that the curve is due to the trade-off among users with diverse needs This indicates the importance of analyzing each user individually, considering the popularity, and suggests that an observation of the average accuracy alone is inadequate.",Information Processing & Management,Multi-hop path reasoning over sparse temporal knowledge graphs based on path completion and reward shaping,Flexibly manipulating popularity bias for tackling trade-offs in recommendation - ScienceDirect,https://drive.google.com/file/d/1cuyk6kDZ-_nbwkY-Vs_LyjyxsMpDxE82/view?usp=sharing,Table 3. The accuracy of DirectMag with the ideal situation (DirectMag  ideal).,Table ,Durga Srikari Maguluri
209,"ur proposed SGP model consists of two main stages: (1) a social-aware pre-training stage, where a multi-layer GNN is employed to generate the pre-trained embeddings and (2) an information distillation stage, where we incorporate the Gaussian Mixture Model (GMM) to distil information from the pre-trained embeddings for the subsequent model’s training and generation of recommendations. In the following, we first define the tasks and some preliminaries in Section 3.1. Next, Section 3.2 describes how to incorporate the social relations and a light GNN model to propagate the social information into users’ and items’ embeddings. Finally, in Section 3.3, we demonstrate how to employ the GMM to distil the social information from those pre-trained embeddings for the subsequent training and the production of final recommendations. To clearly illustrate our model, Fig. 1 depicts the overall structure of SGP, where the upper and bottom regions describe the pre-training and fine-tuning stages, respectively. We conclude the presentation of the SGP model with a discussion of its benefits for extreme cold-start users (Section 3.4) as well as an analysis of its time complexity (Section 3.5).",Information Processing & Management,A Social-aware Gaussian Pre-trained model for effective cold-start recommendation,A Social-aware Gaussian Pre-trained model for effective cold-start recommendation - ScienceDirect,https://drive.google.com/file/d/1kAI26txfO-rZT_OJgWuXn7BERs-Tbc1Y/view?usp=sharing,"Fig. 1. An illustration of our SGP model, where the pre-training stage and the fine-tuning stage are located above and below, respectively. In this figure,  and  are the initial embedding matrices of the users and items, which are randomly generated.  is the social similarity graph, which can be computed using Eq. (1). Furthermore,  and  contain the mean and standard deviation of the pre-trained embeddings, which are defined in Section 3.3.1.",Flow Diagram ,Durga Srikari Maguluri
210,"To determine the extent to which predictive models can predict the movies’ eudaimonic and hedonic scores, we have developed machine learning models that incorporate various data sources including metadata such as movie genre, released year, popularity, and critical ratings, as well as audio and visual features of the movies. The process of eudaimonic and hedonic score prediction is demonstrated in Fig. 1 which involves: (i), data acquisition, (ii), data pre-processing, (iii), building, training, and testing the predictive models for the classification task, and (iv), evaluating the model performance. The findings of this study will provide insights into the relationship between different movies’ features and their eudaimonic and hedonic scores.n this study, we leveraged various movie features, including low-level and high-level visual features, audio features, and metadata, to predict movies’ eudaimonic and hedonic scores. To collect eudaimonic and hedonic scores of movies, we conducted a user study with several steps. One part of the user study involved users completing a questionnaire designed by Oliver and Raney (2011) to evaluate their perception of the movies. We used the responses collected from this questionnaire to assign eudaimonic and hedonic scores to the movies. The movies shown to the users were selected from a pool of 1000 popular movies across different years, whose popularity was estimated based on the number of votes they received from MovieLens users. We developed a web application to acquire movies’ E and H scores. When designing the data collection web app, particular attention was paid to avoiding data sparsity. We implemented a web interface where the participants filled out the questionnaires on personal characteristics and provided ratings and labels for at least ten movies out of 50. These sets of 50 movies, different for each participant, were selected through a sparsity-minimization mechanism. In this mechanism, we took a subpool of 200 movies from the larger, 1000-movies pool of popular movies from the Movielens dataset. We randomly pulled out 50 movies from the sub-pool for each participant to label/rate. These 50 movies were compiled from sets of 5, ensuring that the initial set included movies curated by one user, followed by two in the next set, and so forth, until reaching ten. After a movie reached a certain number of ratings/labels (10 in our case), we removed it from the sub-pool and replaced it with another from the larger pool. This ensured that we did not have a long tail problem (with few movies getting many labels/ratings and the majority having few) but the rated movies in our dataset had a more flat distribution of labels/ratings. We chose a threshold of ten ratings to strike a balance between gathering sufficient data for each movie and obtaining information for a wide range of movies. When the sub-pool of 200 movies was used up, another batch of 200 movies was introduced. In this study, the average E and H experiences annotated by users are referred to as the E and H scores, respectively. In total, we collected E and H scores for 709 movies released from 1930 to 2015. To formulate the classification problem, we used the median split method to define low/high E score and low/high H score classes.Since the datasets we used in this study encompass many features, it naturally leads to a high dimensionality challenge. We opted for Principal Component Analysis (PCA) as a dimensionality reduction technique across all datasets to address this. This approach significantly reduced the overall dimensionality of the data, as indicated in Table 2, Table 3. While feature selection is a common practice for reducing dimensionality, we would like to provide insight into why we selected PCA over traditional feature selection methods for addressing this concern. Our decision was informed by an extensive exploration of the dataset and its characteristics. We initiated our machine learning pipeline by thoroughly analyzing feature correlations and assessing the individual importance of features in predicting eudaimonic and hedonic scores. Our correlation analysis did not unveil any linear relationships between features and target values. We also conducted feature selection with varying numbers of features (k) as a hyperparameter in a nested cross-fold validation technique using the different machine learning models used in our study. Notably, the results from feature selection exhibited better performance than the baseline but were noticeably inferior to the outcomes achieved through feature reduction. We speculate that the reason can be attributed to the potential information loss inherent in feature selection. Considering that our dataset consists of preprocessed aggregated features from larger datasets, the inherent probability of encountering noise within the features was low. Consequently, the potential risk of information loss through feature selection seemed a valid concern. Employing PCA enabled us to reduce feature dimensions while still preserving a substantial variance in the original features. We attribute the success of PCA to its ability to capture complex relationships across different feature types without sacrificing predictive power.The inputs to the models are preprocessed data, including metadata, audio, and visual features. The machine learning models include linear and non-linear models to investigate which models can more effectively predict E and H scores. The models were trained and tested using the k-fold nested cross-validation repeated five times with different folds. One single iteration of the nested cross-validation approach is shown in Fig. 2. We used ten folds for the outer cross-validation (for evaluating the model’s performance) and five for the inner cross-validation (for tuning the model’s parameters).Many candidate machine learning algorithms could perform well on this task. However, using them in our study would imply consuming many resources. So, we decided to make a triage of models to include in the final thorough optimizations and evaluations. In the triage process, we used AutoML tools, did a one-time train–test splitting (80% train set) and ranked several models according to their accuracy of the predicted variables (E and H scores). This exploratory analysis enabled us to create ranked lists of algorithms for each dataset, predicting both E and H scores. Our approach involved compiling the best-performing algorithms from these ranked lists. Notably, we took the top three algorithms for each dataset and prediction task. While evaluating the outputs, we observed that the top-3 algorithms that were above the dummy classifier in the ranked list displayed competitive performance, exhibiting marginal differences in key metrics such as accuracy, F1 score, and ROC AUC. With the knowledge that there was not a single model standing out as a significantly superior choice, we opted to proceed with a selection of foundational and well-known machine learning models encompassing distinct underlying algorithms. Interestingly, among the consistently high-performing models, the decision tree classifier was frequently the best algorithm across various datasets and prediction tasks. Given this consistency, we intentionally included it in our final selection. Our aim was to present a varied set of classification algorithms while also focusing on the model that consistently performed well across different datasets and prediction situations in our initial exploratory analysis.In outer cross-validation, we split the data into 10 folds, keeping one for testing. The model trains on 9 folds and is tested on the left-out test fold. This procedure is repeated within an outer loop, with different folds serving as the test fold in each iteration. The final assessment involves computing average metrics across the 10 distinct test folds. For each test fold, from the 9 training folds, we create 5 subsets for inner cross-validation. One subset becomes the validation set, while the others form the training set for inner hyperparameter tuning. This inner cross-validation trains the model on 4 folds and is evaluated on the left-out validation fold. In the inner cross-validation, we experiment with different parameters to find the best setup for the validation set. We then use this chosen setup on the left-out test set. The model is fine-tuned using various parameters on the validation set. The best parameters are used to assess the model on the reserved test part for the test. We used a search algorithm called Bayesian search to create diverse parameter combinations.",Information Processing & Management,"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features","Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features - ScienceDirect",https://drive.google.com/file/d/1QOb1dhLuerqPHpli8r-vZeHzXU7MJV2o/view?usp=sharing,Fig. 1. The process of eudaimonic and hedonic scores prediction using machine learning models.,Flow Diagram ,Durga Srikari Maguluri
211,"To determine the extent to which predictive models can predict the movies’ eudaimonic and hedonic scores, we have developed machine learning models that incorporate various data sources including metadata such as movie genre, released year, popularity, and critical ratings, as well as audio and visual features of the movies. The process of eudaimonic and hedonic score prediction is demonstrated in Fig. 1 which involves: (i), data acquisition, (ii), data pre-processing, (iii), building, training, and testing the predictive models for the classification task, and (iv), evaluating the model performance. The findings of this study will provide insights into the relationship between different movies’ features and their eudaimonic and hedonic scores.n this study, we leveraged various movie features, including low-level and high-level visual features, audio features, and metadata, to predict movies’ eudaimonic and hedonic scores. To collect eudaimonic and hedonic scores of movies, we conducted a user study with several steps. One part of the user study involved users completing a questionnaire designed by Oliver and Raney (2011) to evaluate their perception of the movies. We used the responses collected from this questionnaire to assign eudaimonic and hedonic scores to the movies. The movies shown to the users were selected from a pool of 1000 popular movies across different years, whose popularity was estimated based on the number of votes they received from MovieLens users. We developed a web application to acquire movies’ E and H scores. When designing the data collection web app, particular attention was paid to avoiding data sparsity. We implemented a web interface where the participants filled out the questionnaires on personal characteristics and provided ratings and labels for at least ten movies out of 50. These sets of 50 movies, different for each participant, were selected through a sparsity-minimization mechanism. In this mechanism, we took a subpool of 200 movies from the larger, 1000-movies pool of popular movies from the Movielens dataset. We randomly pulled out 50 movies from the sub-pool for each participant to label/rate. These 50 movies were compiled from sets of 5, ensuring that the initial set included movies curated by one user, followed by two in the next set, and so forth, until reaching ten. After a movie reached a certain number of ratings/labels (10 in our case), we removed it from the sub-pool and replaced it with another from the larger pool. This ensured that we did not have a long tail problem (with few movies getting many labels/ratings and the majority having few) but the rated movies in our dataset had a more flat distribution of labels/ratings. We chose a threshold of ten ratings to strike a balance between gathering sufficient data for each movie and obtaining information for a wide range of movies. When the sub-pool of 200 movies was used up, another batch of 200 movies was introduced. In this study, the average E and H experiences annotated by users are referred to as the E and H scores, respectively. In total, we collected E and H scores for 709 movies released from 1930 to 2015. To formulate the classification problem, we used the median split method to define low/high E score and low/high H score classes.Since the datasets we used in this study encompass many features, it naturally leads to a high dimensionality challenge. We opted for Principal Component Analysis (PCA) as a dimensionality reduction technique across all datasets to address this. This approach significantly reduced the overall dimensionality of the data, as indicated in Table 2, Table 3. While feature selection is a common practice for reducing dimensionality, we would like to provide insight into why we selected PCA over traditional feature selection methods for addressing this concern. Our decision was informed by an extensive exploration of the dataset and its characteristics. We initiated our machine learning pipeline by thoroughly analyzing feature correlations and assessing the individual importance of features in predicting eudaimonic and hedonic scores. Our correlation analysis did not unveil any linear relationships between features and target values. We also conducted feature selection with varying numbers of features (k) as a hyperparameter in a nested cross-fold validation technique using the different machine learning models used in our study. Notably, the results from feature selection exhibited better performance than the baseline but were noticeably inferior to the outcomes achieved through feature reduction. We speculate that the reason can be attributed to the potential information loss inherent in feature selection. Considering that our dataset consists of preprocessed aggregated features from larger datasets, the inherent probability of encountering noise within the features was low. Consequently, the potential risk of information loss through feature selection seemed a valid concern. Employing PCA enabled us to reduce feature dimensions while still preserving a substantial variance in the original features. We attribute the success of PCA to its ability to capture complex relationships across different feature types without sacrificing predictive power.The inputs to the models are preprocessed data, including metadata, audio, and visual features. The machine learning models include linear and non-linear models to investigate which models can more effectively predict E and H scores. The models were trained and tested using the k-fold nested cross-validation repeated five times with different folds. One single iteration of the nested cross-validation approach is shown in Fig. 2. We used ten folds for the outer cross-validation (for evaluating the model’s performance) and five for the inner cross-validation (for tuning the model’s parameters).Many candidate machine learning algorithms could perform well on this task. However, using them in our study would imply consuming many resources. So, we decided to make a triage of models to include in the final thorough optimizations and evaluations. In the triage process, we used AutoML tools, did a one-time train–test splitting (80% train set) and ranked several models according to their accuracy of the predicted variables (E and H scores). This exploratory analysis enabled us to create ranked lists of algorithms for each dataset, predicting both E and H scores. Our approach involved compiling the best-performing algorithms from these ranked lists. Notably, we took the top three algorithms for each dataset and prediction task. While evaluating the outputs, we observed that the top-3 algorithms that were above the dummy classifier in the ranked list displayed competitive performance, exhibiting marginal differences in key metrics such as accuracy, F1 score, and ROC AUC. With the knowledge that there was not a single model standing out as a significantly superior choice, we opted to proceed with a selection of foundational and well-known machine learning models encompassing distinct underlying algorithms. Interestingly, among the consistently high-performing models, the decision tree classifier was frequently the best algorithm across various datasets and prediction tasks. Given this consistency, we intentionally included it in our final selection. Our aim was to present a varied set of classification algorithms while also focusing on the model that consistently performed well across different datasets and prediction situations in our initial exploratory analysis.In outer cross-validation, we split the data into 10 folds, keeping one for testing. The model trains on 9 folds and is tested on the left-out test fold. This procedure is repeated within an outer loop, with different folds serving as the test fold in each iteration. The final assessment involves computing average metrics across the 10 distinct test folds. For each test fold, from the 9 training folds, we create 5 subsets for inner cross-validation. One subset becomes the validation set, while the others form the training set for inner hyperparameter tuning. This inner cross-validation trains the model on 4 folds and is evaluated on the left-out validation fold. In the inner cross-validation, we experiment with different parameters to find the best setup for the validation set. We then use this chosen setup on the left-out test set. The model is fine-tuned using various parameters on the validation set. The best parameters are used to assess the model on the reserved test part for the test. We used a search algorithm called Bayesian search to create diverse parameter combinations.",Information Processing & Management,"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features","Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features - ScienceDirect",https://drive.google.com/file/d/1r6IrRQUvXY-IaSdZb6dB4F24Jytbrbza/view?usp=sharing,Fig. 2. The block diagram of a single iteration of the K-fold nested cross-validation machine learning pipeline.,Flow Diagram ,Durga Srikari Maguluri
212,"We employed logistic regression, K-nearest neighbors, Ridge classifier, SVC, decision tree and random forest to predict the E and H scores (Hastie, Tibshirani, Friedman, & Friedman, 2009). We explored different models that incorporate both linear and non-linear approaches. Moreover, we selected a range of hyperparameter values based on the feature sets’ dimensions, and findings from previous studies (Probst, Boulesteix, & Bischl, 2019). A detailed list of the algorithms we used and their hyperparameters is available in Table 1.Furthermore, we also evaluated the performance of deep learning models. In particular, we performed experiments with lightweight architectures of neural networks to avoid overfitting. The performance of the neural network models was inferior to the performance of the several traditional models, although in certain cases, they exhibited the potential to outperform some of the used algorithms. Nonetheless, we acknowledge that the margin of improvement was not substantial. With this observation, we conjectured that the inherent limitations of our dataset size have been a key factor contributing to the relatively modest performance exhibited by deep learning models in our study. In light of these observations, we shifted our attention from deep learning models to traditional machine learning methods for this study.",Information Processing & Management,"Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features","Predicting movies’ eudaimonic and hedonic scores: A machine learning approach using metadata, audio and visual features - ScienceDirect",https://drive.google.com/file/d/17NuEPGR5NETE2eanNZe52ESlmVySVzrV/view?usp=sharing,Table 1. ML models and hyperparameters. The optimal parameter will be selected based on the largest accuracy obtained during the inner cross-validation loop. Scikit-learn was used to implement all models,Table ,Durga Srikari Maguluri
213,"We reason that WeChat-based group chat unfolds as a complex communication process, which involves personal factors (e.g., needs, interests), interpersonal factors (e.g., common objective), and group factors (e.g., collective atmosphere). Furthermore, these communication components are not operating in isolation and in a vacuum, but rather are enabled as well as constrained by technological/material factors or affordances. It is impossible to exhaust all affordances and communication factors bearing on WBSCs. Instead, we focus on those components seemingly most connected to the WBSCs context and propose a conceptual framework by integrating those components, as presented in Fig. 1.The persistent information stream enables members to track or engage with conversational threads within the group. The congruency theory posits that users’ persistent participation in a communication setting is influenced by their prior experiences in the setting (Islam, Rahman, & Hollebeek, 2018). Members often choose to participate in a specific topic based on their initial interests or preferences. Individuals’ continuous engagement in communication exchanges signifies the consistency of their preferences and beliefs, which affects their perceived rewards such as gratification or identification (Argyris, Wang, Kim, & Yin, 2020). People are inclined to invest time in supportive events that contribute to accessing their ideal self-value (Hsieh, Lee, & Tseng, 2022). Within WBSCs, members’ repeated engagement in the same or similar topics demonstrates the alignment of those topics with their preferences or beliefs. Consequently, members are inclined to engage repeatedly in certain topics to obtain intrinsic rewards. Association, another type of affordance provided by the WeChat group, facilitates the interaction between group members and enables them to find common interests or grounds (Zhang & Jung, 2023). In WBSCs, individual behavior tends to exhibit a selective exposure trend (Slechten, Courtois, Coenen, & Zaman, 2022). Members typically engage in discussion out of personal interests or desires (Peng & Yang, 2022). As such, the sustained dyadic interactions among members imply the homophily that certain members share with each other. The concept of homophily, pointing to the commonality between people, has been identified as one key determinant of interpersonal interactions (Ma, Krishnan, & Montgomery, 2015; Koiranen, Koivula, Keipi, & Saarinen, 2019; Kwon, Oh, & Kim, 2017). For instance, Song, Cho, and Benefield (2020) demonstrated that in a forum community, individuals are inclined to interact with those who share similar political preferences.In WBSCs, not all contents are equally visible, nor are the actors. Topics with high participation imply high visibility to group members. Actors who participate in discussion frequently tend to have high visibility compared to those who are inactive or dormant. In group communication, members can observe the dynamics within the group and sense the popularity of a particular discussion topic. According to the selective attention theory, the prominence and popularity of issues play a pivotal role in capturing individuals’ attention (Geise & Baden, 2015). This prominence is a result of complex social dynamics between group members (Salganik, Dodds, & Watts, 2006). As for the visibility of a particular topic in WBSCs, topics that receive intense discussion are often considered interesting, important, and valuable, and thus are more likely to attract more people to engage in subsequent discussions. Such “the popular gets more popular” phenomena have been well documented in the social media communication context. For example, in online learning communities, individuals who receive higher approval, evidenced by the high number of likes, tend to find it easier to access knowledge shared by others (Wu & Wu, 2021). Similarly, in the realm of online health communities, well-established interpersonal connections have been found to boost the acquisition of information and emotional support (Liu et al., 2023). In the context of professional question-and-answer communities, questions that have garnered substantial attention are more likely to attract further engagement from users (Tonellato, Tasselli, Conaldi, Lerner, & Lomi, 2023). In light of these findings, this study formulates the following hypothesis.Within WBSCs, interactions among members are characterized by a gradual and dynamic evolution. Sustained interactions often entail people's shared needs, interests, or attitudes—evidence of homophily. When people observe the participation of a particular member with whom they have previously interacted, provided support, or established a personal connection, they are more likely to join in, too. A study by Xiong, Feng, and Tang (2020) provided important evidence that users interaction in online public communities is highly reciprocal out of interest-based homophily. Koiranen, Koivula, Keipi, and Saarinen (2019) demonstrated that connection ties between parliamentarians on social network are based upon shared contextual factors, matching values, and semblable background. Therefore, we posit the following hypothesisHowever, the affordances of persistence provided by WBSCs simultaneously make it easier for users to instrumentally seek and retrieve useful information. When one poses a question, the person can receive responses from other members or browse through the chat records for useful information. The Hierarchy of Needs theory suggests that individual behavior is driven by unmet needs (McLeod, 2007). That said, when one has an unsatisfied need, they may persistently pursue that need, or continue to engage in discussions about similar topics in the WBSCs scenario. On the other hand, when an individual's need is gratified, or they have obtained enough information to satisfy their needs, they may stop searching for or participating in further communication exchanges about that issue (Harlow & Brown, 2021). Based on these two contrasting points, this study poses the following question:",Information Processing & Management,Users’ engagement in WeChat-based support communities: A multilevel perspective,Users’ engagement in WeChat-based support communities: A multilevel perspective - ScienceDirect,https://drive.google.com/file/d/1aZV8yJN-3q0f9Qj-J6ExoSoIen-HMSfh/view?usp=sharing,Fig. 1. A conceptual framework of WeChat-based support community.,Flow Diagram ,Durga Srikari Maguluri
214,"The results are reported in Table 2. Model 1 examined the impact of control variables (accumulated activity of members and groups) on members’ engagement. Results showed a positive correlation between the accumulated activity of members and their engagement in discussions (? = 0.0489, p < 0.001). Members with a higher outdegree were more likely to engage in discussions within the community. Furthermore, for each unit increase in members’ activity, member engagement in discussions was expected to increase by 105.02 %. Meanwhile, the accumulated activity of the group exhibited a negative association with members’ engagement in discussions (? = ?0.0002, p < 0.001). The increase of discussion was linked to a reduction in members’ engagement intention.Model 2 examined the impact of dynamic engagement principles outlined in this study. Regarding RQ1, results showed a positive association between individual previous discussion engagement and subsequent engagement (? = 0.3665, p < 0.001). As for RQ2, results showed that the discussion engagement was negatively correlated with the time interval between two topics that a member engaged in continuously (? = ?0.2660, p < 0.001).H1 was supported, as members’ experience of dyadic engagement in the same topic with others positively influenced their subsequent behavior (? = 1.513, p < 0.001). Furthermore, as the instances of dyadic engagement increased, engagement in discussions was expected to increase substantially, approximately by 472.7 %.H2 was supported, evidenced by a positive relationship between the popularity of the discussion and members’ engagement (? = 0.0949, p < 0.001). With an increase in the popularity of a topic, members’ engagement in discussions was projected to increase by 109.9 %.",Information Processing & Management,Users’ engagement in WeChat-based support communities: A multilevel perspective,Users’ engagement in WeChat-based support communities: A multilevel perspective - ScienceDirect,https://drive.google.com/file/d/1lG9nXg-adOQz9F6vYilfQYkEtzqsWvox/view?usp=sharing,Table 2. Cox regression model: partial likelihood estimates of relational event models (standardized estimates).,Table ,Durga Srikari Maguluri
