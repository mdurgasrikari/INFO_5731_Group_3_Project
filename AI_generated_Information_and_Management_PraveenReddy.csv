index,Image Name,AI generated Summary,Author Summary,Cosine_Similarity Score,ROUGE-WE,BertScore,BLEU Score,METEOR Score
0,01.jpg," This graph shows the frequency of different types of climate information used by farmers. The most frequently used type of information is weather forecasts, which are used by 40% of farmers. This is followed by seasonal forecasts, which are used by 20% of farmers. Climate change projections and observed historical climate data are used by 10% and 5% of farmers, respectively. The least frequently used type of information is observed historical weather data, which is used by only 5% of farmers.","Survey results - usage and demand among Swedish extension offices
The respondents were asked what type of weather or climate information they currently use in their profession, and the results show that weather forecasts are the most common type of information used among the respondents (Fig. 2), whereas climate change projections are the least used. On the four-point Likert scale ranging from ?Never? to ?Very often? using the information, 26 (40.0 % of the respondents to that question) replied that they never use climate change projections. While only six respondents use climate change projections ?very often?, 52.5 % (n = 21) of the respondents that replied that they have used projections at some point (n = 40,) regarded the information as very important to their profession (see, Supplementary Material). Conversely, two respondents that use climate projections responded that they do not regard the information as important in their profession.",27.99%,81.28%,85.05%,23.64%,81.86%
1,02.jpg, This algorithm shows the diagnostic algorithm for pleural effusion.,"We developed a flowchart to diagnose tuberculous pleurisy, pleural infection, malignant pleural effusion, and other diseases by using the above markers (Fig. 1). The flowchart includes the following seven markers: pleural ADA =40 IU/L, pleural fluid LDH <825 IU/L, pleural fluid ADA/TP < 14, neutrophil predominance or cell degeneration, peripheral blood WBC =9200/µL or serum CRP =12 mg/dL, pleural amylase =75 U/L, and presence of pneumothorax; pleural LDH/ADA <15 was excluded according to the algorithm of a decision tree. The final decision columns in the flowchart are labeled from left to right as Groups A to I. The diagnostic accuracy rate was 71.7 % for the four diseases, with 79.3 % sensitivity and 75.4 % positive predictive value (PPV) for tuberculosis pleurisy, 75.8 % sensitivity and 83.2 % PPV for pleural infection, 88.6 % sensitivity and 68.8 % PPV for malignant pleural effusion, and 33.0 % sensitivity and 60.0 % PPV for other diseases in the flowchart. Misdiagnosis ratios were 4.6 % (n = 36/775) for tuberculosis pleurisy, 6.8 % (n = 52/762) for pleural infection, 8.3 % (n = 41/494) for malignant pleural effusion, and 16.6 % (n = 140/843) for other diseases.The diagnostic accuracy rate was 71.7 % for the four diseases, with 79.3 % sensitivity and 75.4 % positive predictive value (PPV) for tuberculosis pleurisy, 75.8 % sensitivity and 83.2 % PPV for pleural infection, 88.6 % sensitivity and 68.8 % PPV for malignant pleural effusion, and 33.0 % sensitivity and 60.0 % PPV for other diseases in the flowchart. The misdiagnosis ratios were 4.6 % (n = 36/775) for tuberculosis pleurisy, 6.8 % (n = 52/762) for pleural infection, 8.3 % (n = 41/494) for malignant pleural effusion, and 16.6 % (n = 140/843) for other diseases.",32.24%,41.40%,79.10%,0.00%,91.47%
2,03.jpg, A mixed methods approach was used in this study. Data were collected in two phases: quantitative and qualitative. The quantitative phase involved a survey of PY2/3 pharmacy students enrolled in the integrated pharmacy care course. The survey was used to collect data on students' perceptions of the integration of the pharmacy care course with other courses in the pharmacy curriculum. The quantitative data were analyzed using descriptive statistics and ANOVA. The qualitative phase involved interviews with PY2/3 pharmacy students enrolled in the integrated pharmacy care course. The interviews were used to collect data on students' experiences with the integration of the pharmacy care course with other courses in the pharmacy curriculum. The qualitative data were analyzed using thematic analysis. The results of the quantitative and qualitative phases were then integrated to provide a comprehensive understanding of students' perceptions and experiences of the integration of the pharmacy care course with other courses in the pharmacy curriculum.,"A diagram illustrating the mixed-methods design is shown in Fig. 1. In the QUANT phase, students enrolled in the IPC CVS-II course were invited to participate in a voluntary (with no compensation) pre- and post-survey on the effectiveness of flowcharts using a 5-point Likert-like rating scale (Strongly agree to Strongly disagree). Three students from the 2019 class and 2 faculty members were asked to review the initial survey and provide feedback. Students' performance was measured as scores in the 3 content areas (shock, antiarrhythmic, and acute coronary syndrome) on the summative examinations with (2019?2021) or without flowcharts (2017?2018). The data were analyzed using SPSS (version 28.0.0.0, Chicago, IL). Once the assumptions of normality and homogeneity were met, a one-way analysis of variance was used to determine the effectiveness of student performance, and the descriptive statistics for each cohort were recorded.",57.04%,75.41%,83.51%,56.13%,82.85%
3,04.jpg," The flowchart shows the different uses of a decision-making framework. The framework can be used to help students retain information, organize content, and break down information. It can also be used to help students apply information to other courses, build confidence, and reduce study time. Additionally, the framework can be used as a study tool to help students with outlining, quick reference, and understanding content.","As shown in Fig. 2, the coding analysis yielded 21 codes placed into 4 categories. Three major themes emerged from the qual analysis, namely ?used as a medium for retention and recall,? ?used as a study tool,? and ?used as a decision-making framework.?",24.86%,84.12%,84.85%,42.56%,76.77%
4,05.jpg," The algorithm shows the decision-making process for extubation or surgical airway in pediatric patients who require intubation in the intensive care unit (ICU). If the patient's intubation period exceeds or is expected to exceed two weeks, extubation should be considered. If the patient is not reversible, a surgical airway should be considered. If the patient is reversible, additional observation for 1 to 2 weeks is recommended. If extubation is possible, the patient should be extubated. If extubation is not possible, tracheostomy with a temporary tracheostoma should be performed. If the patient has a history of repeated aspiration pneumonia, tracheostomy with a permanent tracheostoma should be performed. If the patient has continuous saliva aspiration and permission from the parents for elimination of the patient's vocal function, aspiration prevention surgery (LTS, SGLC, LC, etc.) should be performed.","A flowchart for selecting an appropriate surgical airway for the long-term management of NIPP with intubation was created by a laryngologist and a pediatrician. Our flowchart includes the following main branching points; possibility of extubation, intubation period, reversibility related to prognosis, history of repeated aspiration pneumonia and/or continuous saliva aspiration, and permission from the parents for elimination of the patients? vocal function (Fig. 1). The included surgeries were as follows: Tracheostomy with Temporary Tracheostoma (TwTT); Tracheostomy with Permanent Tracheostoma (TwPT), which includes sutures from the anterior wall of the trachea to the surrounding skin flaps7, 8; and APSs such as LTS.",37.57%,93.46%,86.55%,51.40%,85.77%
5,06.jpg, The figure shows an OTR ontology. The upper part of the ontology is the core ontology which is independent of the domain. The lower part is the domain ontology which is specific to the food packaging domain.,"TRANSMAT [2], [3] Ontological and Terminological Resource (OTR) represents concepts in the food packaging domain and the relations between them. This OTR is structured in two parts, a core ontology and a domain ontology (see Fig. 1 and [1] for further details). The up-core ontology is the representation of the n-Ary relations structure, defined as a relation concept linked to the arguments composing the relation. The down-core ontology includes concepts specific to the experimental fields, while the concept is categorized as symbolic or quantitative which are associated with measurement units used in experimental fields.",59.93%,74.60%,86.32%,15.64%,89.19%
6,07.jpg," The figures show theの影響 recall and precision@k of different filtering strategies on the CoQA dataset. We can observe that precision generally increases as the filtering threshold increases, but recall decreases. This is because the filtering process removes some of the noisy data, which can lead to a decrease in recall. However, the decrease in recall is usually accompanied by an increase in precision, as the filtered data is of higher quality.","Overall, an acceptable recall i.e. can be obtained with slight candidate filtering. The selection of several candidates has little effect when using frequentist methods. On the other hand, precision is a problem with these methods, i.e. the precision did not exceed with any of the parameters we tested.",36.12%,94.42%,85.19%,44.60%,80.42%
7,08.jpg," The figure shows the four main steps of the text mining process.

**1. Data Preprocessing:**
The first step is to preprocess the text data. This includes cleaning the data, removing stopwords, and stemming the words.

**2. Text Embeddings:**
The text data is converted into numerical vectors using a technique called word embeddings. This allows the text data to be processed by machine learning algorithms.

**3. Text Clustering:**
The text data is clustered into different groups based on the similarity of the text. This can be done using a variety of clustering algorithms.

**4. Topic Modeling:**
The final step is to identify the topics that are discussed in the text data. This can be done using a variety of topic modeling algorithms.","BERTeley has three main features: scientific article preprocessing, topic modeling, and metric calculation. BERTeley?s preprocessing suite addresses the aforementioned challenge by removing specific words from the input data. The topic modeling and metric calculation add quality-of-life features such as pre-selected language models trained specifically on scientific articles, and one-line topic modeling metric calculation. The BERTeley workflow can be seen in Fig. 1. BERTeley can be installed by running the command pip install berteley at the command line (Eric Chagnon and Ushizima, 2023).",30.61%,79.89%,82.49%,43.80%,81.56%
8,09.jpg, Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram.,"Fig. 1 outlines the selection process. The search yielded 9427 potential publications from databases, registries and additional sources. After removing duplicates, 7328 records were screened based on their titles and abstracts. Next, 147 articles from databases and registries, and 17 articles from additional sources were assessed for eligibility in full-text. Ultimately, 59 articles comprising 50 studies met the inclusion criteria. When study findings were reported in multiple publications, or there was a need to evaluate a study?s protocol or a pilot test to comprehensively assess an intervention, they were included in the data extraction form together with the main study. However, if additional publications described findings in a research design different from the parent study (e.g. qualitative findings), they were analyzed separately due to differences in methods. The data extraction form synthesizing the review findings can be found in Supplementary file 2.",4.25%,69.59%,79.85%,0.00%,70.88%
9,10.jpg," This diagram shows the relationship between scientific literacy, socioscientific issues, and environmental socioscientific issues. It also shows the different aspects of environmental socioscientific issues, such as the yearly publication, geographical spread, topics addressed, skills, nature of studies, and implications mentioned.","Studies that have used environmental SSI were analyzed. Different aspects on which these studies were analyzed are mentioned in the flowchart (see Fig. 1). All these aspects were finalized after deliberations among the researchers, leading to a consensus being achieved.",13.68%,81.54%,84.55%,40.00%,73.07%
10,100.jpg," **H1a and H1:** The figure shows the hypothesized relationships between media type, closeness to the target, and prediction accuracy. H1a states that the closer the relationship between the influencer and the target, the higher the prediction accuracy. H1 states that the higher the prediction accuracy, the higher the actual effectiveness.","To better understand the signaling process, we propose the following conceptualization. Our key idea is that universities can signal the availability and value of their scientific knowledge through three types of signals transmitted through distinct channels (see Fig. 1): (1) signals to members of scientific communities transmitted through scientific outlets; (2) signals to economic agents transmitted through patents5 and patent offices; and (3) signals to members of society at large transmitted through media outlets.",18.99%,71.63%,82.05%,28.05%,74.28%
11,101.jpg," The figure shows the relationships between the perceived differences between face-to-face (FtF) and email (EMl) in convenience, awkwardness/embarrassment, and effectiveness, and the closeness level to the target (friend vs. stranger), and media selection decision.","The architecture of ArZiGo is composed of 5 main modules (see Fig. 1): User Interface, Knowledge Bases, Search Module, Interaction Processing Module, and Recommendation Module. The User Interface Module allows the users to perform queries and to filter the results, sending the queries to the search module and displaying the results. It also provides the captured interactions to the Interaction Processing Module.",37.79%,77.17%,81.53%,22.37%,74.42%
12,102.jpg," The table shows the results of a study that compared how easy it is for people to seek help from friends vs. strangers, how awkward they feel seeking help, and how embarrassed they feel if they are rejected. The p-value column shows the results of a statistical test that compared the two groups. The effect size column shows the strength of the relationship between the two variables.

For the first question, the p-value is 0.05, which means that there is a 5% chance that the difference between the two groups is due to chance. The effect size is 0.68, which means that the difference between the two groups is large. This suggests that people find it easier to seek help from friends than from strangers.

For the second question, the p-value is <0.001, which means that there is less than a 0.001% chance that the difference between the two groups is due to chance. The effect size is 0.129, which means that the difference between the two groups is moderate. This suggests that people feel more awkward seeking help from friends than from strangers.

For the third question, the p-value is 0.188, which means that there is a 18.8% chance that the difference between the two groups is due to chance. The effect size is 0.015, which means that the difference between the two groups is small. This suggests that people feel equally embarrassed about being rejected by friends and strangers.

Overall, the results of this study suggest that people find it easier to seek help from friends than from strangers, but they feel more awkward doing so. They also feel equally embarrassed about being rejected by friends and strangers.","Fig. 2 illustrates the process that generates users and their interactions. As a result, 684 users have been created with a total number of 3,500,000 interactions approximately. Once the users have been created, a pseudo-random selection of the users whose recommendations will be evaluated has been carried out, where the selected number has been limited to the number of experts recruited by each domain. To avoid any kind of bias, users with a profile which is too general have been discarded.",23.76%,88.07%,80.29%,37.86%,79.26%
13,103.jpg," The table shows the Cronbach's alpha values for each of the three constructs. The Cronbach's alpha values for AwkEmbar and Money are 0.817 and 0.655, respectively, which indicates that these constructs have good internal consistency. However, the Cronbach's alpha value for Effectiveness is not available, which indicates that this construct may not be reliable. Additionally, the Cronbach's alpha values for Convenience and Effectiveness are not available, which indicates that these constructs may not be reliable.","Pre-processing module is used to identify the edges present in the flowchart. Input to this module will be a flowchart which will be uploaded by the user. This module first convert input image into a binary image then apply the Canny edge detection algorithms [10] to identify the edges present in the given figure. Among the edge detection methods developed so far, canny edge detection algorithm is one of the most strictly defined methods that provides good and reliable detection. Owing to its optimality to meet with the necessary criteria for edge detection and the simplicity of process for implementation, it became one of the most popular algorithms for edge detection. ",25.80%,87.76%,81.49%,29.98%,74.01%
14,104.jpg," The table shows the results of a bootstrap analysis. The analysis was conducted to determine the effect of various independent variables on the dependent variable, which is the medium selected. The independent variables are closeness, awkwardness, effectiveness, convenience, money, and the interaction effects of money and closeness, effectiveness and closeness, and convenience and closeness.

The results show that the main effects of closeness, awkwardness, and effectiveness are not significant. However, the main effects of convenience and money are significant. The interaction effects of money and closeness, effectiveness and closeness, and convenience and closeness are also significant.

The results of the bootstrap analysis suggest that the relationship between the independent variables and the dependent variable is complex. The main effects of some of the independent variables are not significant, but the interaction effects of these variables are significant. This suggests that the relationship between the independent variables and the dependent variable is not linear.","We identified publications indexed in the Scopus database. A paper on epilepsy in LA was defined as having at least one author with a Latin American country affiliation. The search strategy included Medical Subject Heading (MeSH) terms for ?epilepsy?. Publications were limited to original research articles, review articles, letter, note, conference paper, short survey, editorial and erratum. For the complete and final search strategy, please refer to Supplementary material 1. A flowchart of the bibliometric search is shown in Figure 1. The period of publication dates was restricted from 1989 to 2018 and to English, Spanish, and Portuguese language only.",30.70%,88.80%,79.69%,40.31%,76.56%
15,105.jpg," Table 4 shows the results of the bias-corrected bootstrap analysis with 5000 replications for the structural model. The results indicate that the path from AwkEmbargo to Effectiveness is significant and negative (b = -0.47, p < 0.05). The path from AwkEmbargo to Convenience is not significant (b = 0.25, p > 0.05). The path from Money to Effectiveness is not significant (b = 0.29, p > 0.05). The path from Money to Convenience is not significant (b = 0.34, p > 0.05). The path from Effectiveness to Strangers is significant and negative (b = -1.59, p < 0.05). The path from Convenience to Strangers is significant and positive (b = 1.64, p < 0.05).","MOOSE, as shown in Fig. 1, can be divided into core capabilities, located in the framework, and physics-specific capabilities, which have been placed in physics modules. The Libtorch-based ML functionalities have been divided between the framework and the MOOSE stochastic tools module (MOOSE-STM) [11], which incorporates algorithms necessary for efficient stochastic analysis, surrogate generation, and data analysis. Even though the syntax of Libtorch is directly available in MOOSE and MOOSE-based applications, several wrapper classes have been created to simplify the utilization, creation, and training of NN models in MOOSE. Fig. 1 presents the integration of Libtorch-based functionalities in MOOSE:",19.35%,67.96%,78.85%,40.35%,73.90%
16,106.jpg," The table shows the results of a bootstrap analysis. The analysis was conducted to determine the effect of closeness and gender on the selection of medium. The results show that closeness has a significant positive effect on the selection of medium, while gender has no significant effect. The interaction between closeness and gender is also not significant.

The bootstrap analysis was conducted using 1000 bootstrap samples. The bias, standard error, and 95% confidence intervals are reported for each coefficient. The bias is the difference between the coefficient estimate from the original sample and the coefficient estimate from the bootstrap sample. The standard error is the standard deviation of the coefficient estimates from the bootstrap samples. The 95% confidence interval is the range of values that is likely to contain the true coefficient value.

For closeness, the bias is 0.032, the standard error is 0.526, and the 95% confidence interval is (-0.215, 1.933). This indicates that the coefficient estimate for closeness is not significantly different from zero.

For gender, the bias is 0.034, the standard error is 0.712, and the 95% confidence interval is (-0.900, 1.682). This indicates that the coefficient estimate for gender is not significantly different from zero.

For the interaction between closeness and gender, the bias is 0.416, the standard error is 2.902, and the 95% confidence interval is (-1.899, 20.382). This indicates that the coefficient estimate for the interaction between closeness and gender is not significantly different from zero.

Overall, the results of the bootstrap analysis suggest that closeness has a significant positive effect on the selection of medium, while gender has no significant effect. The interaction between closeness and gender is also not significant.",Fig. 2. Left: Flowchart of the steps performed to develop a Decision Tree in a ?Classical? analysis and with our proposed Interactive analysis. Right: Strengths and Limitations of Decision Trees Algorithms and experts.,18.68%,72.01%,79.38%,35.97%,71.74%
17,107.jpg," The table shows the results of a bootstrap analysis. The analysis was conducted to determine the 95% confidence intervals for the coefficients in a linear regression model. The dependent variable in the model was selected medium, and the independent variables were closeness, AwkEmbar, effectiveness, convenience, money, AwkEmbar*Closeness, effectiveness*Closeness, convenience*Closeness, and money*Closeness.

The results show that the coefficients for closeness, AwkEmbar, effectiveness, convenience, and money are all significant at the .05 level. The coefficients for AwkEmbar*Closeness, effectiveness*Closeness, convenience*Closeness, and money*Closeness are not significant.

The 95% confidence intervals for the coefficients show that the range of values that the coefficients could plausibly take on is relatively large for some of the coefficients. This suggests that the results of the analysis are not particularly precise. However, the confidence intervals do not include zero for any of the significant coefficients, which suggests that the results are reliable.","This paper proposes a method and an algorithm to design a queuing network in terms of parallel test stations at city entrance links. Since the queuing network optimization is based on traffic inflow prediction, a two-stage model is proposed. The first stage is traffic inflow prediction, and the second stage is queuing network optimization. Fig. 2 shows the model's conceptual framework. The first stage is a feedback procedure between trip distribution and traffic assignment. It is usually known as transportation system equilibrium. The detailed models are elaborated on in the following sections.",30.78%,87.79%,80.47%,44.06%,72.16%
18,108.jpg, Results of the Mixed Model ANOVA and Cronbach’s alpha for the averaged scores of the items in each index.,We also present a practical flowchart for deciding which algorithm to use in each specific use case: We believe that this flowchart is a unique contribution to DL users as it can help identify which techniques to use for different use cases in a simple way (Fig. 3). ,6.10%,71.75%,83.21%,9.53%,76.15%
19,109.jpg," The figure shows the research model. The model consists of eight constructs: SHA characteristics, perceived creepiness, resistance, perceived ease of use, perceived usefulness, and intention to use. The constructs are related to each other as follows:
- SHA characteristics have a positive effect on perceived creepiness.
- Perceived creepiness has a positive effect on resistance.
- Resistance has a negative effect on perceived ease of use and perceived usefulness.
- Perceived ease of use and perceived usefulness have a positive effect on intention to use.","All articles for this study were retrieved from the Web of Science Core Collection (WoSCC) database and published between January 1, 2003, and December 31, 2022. The retrieval strategy included the topics ?tuina?, ?Chinese manipulation?, ?Chinese massage?, ?Chinese? and the topic ?manipulation or massage?. Overall, 2064 papers were retrieved and screened according to the following inclusion criteria: (a) written in English, (b) with an abstract, and (c) original articles or reviews. Overall, 1902 articles were obtained, screened, and excluded independently by two researchers (BZ and TY). Any disagreements were resolved by a third researcher (MWS). Finally, 25 studies published in 2023 were excluded, and 1877 studies were included (Fig. 1).",11.55%,68.59%,80.21%,29.52%,75.15%
20,11.jpg," The figure presents a Venn diagram with three circles. The circles represent three types of signals: signals in the form of scientific impact, signals in the form of economic impact, and signals in the form of social impact. The first type of signal is transmitted through scientific outlets, such as books, articles, and conference papers. The second type of signal is transmitted through patents. The third type of signal is transmitted through media outlets, such as newspapers, magazines, and blogs. The size of the audience reached by each type of signal is represented by the area of the circle. The Venn diagram shows that the signals in the form of scientific impact are captured by citations in other publications. The signals in the form of economic impact are captured by citations in patents. The signals in the form of social impact are captured by mentions in mass media.","As shown in Fig. 2, four scenarios (system design, design calculation, procedural and coordination) are considered in this examination for evaluating the three knowledge capabilities (recall, analysis and application) of HVAC designers. These scenarios comprise of key concepts, knowledge, and skills related to HVAC design. Each scenario contains multiple various tasks. System design involves the fundamental concepts and terms in HVAC design, such as conditions, variables, and operational parameters. Tasks in this scenario include ?size supply, return, and exhaust ducts?, ?create HVAC zoning and sensor locations?, and so on. Design calculation involves the tasks related to calculations for HVAC design, such as calculating HVAC system requirements and supporting project estimates for system selection. Procedural focuses on the processes executed by HVAC designers, including analyzing compliance with codes and standards, reviewing shop drawings, performing field reviews, etc. Coordination includes the tasks that need cooperation among multiple project stakeholders, such as ?assist in basis of design development?, ?coordinate HVAC equipment space requirements? and ?comply with client specs and performance?.",24.51%,89.54%,81.03%,39.22%,86.55%
21,110.jpg," This image shows the consensus mental model of the factors that influence people's willingness to use wearable health technology. The model was developed through a series of interviews with potential users of wearable health technology. The factors that emerged from these interviews were then grouped into two categories: inhibitors and enablers. Inhibitors are factors that make people less likely to use wearable health technology, while enablers are factors that make people more likely to use wearable health technology. The strength of each factor is indicated by the path coefficient, which ranges from 0 to 1. The higher the path coefficient, the stronger the relationship between the factor and the likelihood of using wearable health technology. Perceived usefulness (.33) and perceived ease of use (.11) are the strongest enablers, while perceived privacy concerns (.16) and perceived creepiness (.21) are the strongest inhibitors.","The performance test flowchart of LLM models is illustrated in Fig. 4. The prompt is generated using the prompt template. It consists of two parts: instruction and question. Instruction explains the task that LLMs should conduct, which is ?Answer the following single-choice questions about CERTIFIED HVAC DESIGNER exam and explain the reason:? for this study. ",34.74%,82.83%,82.05%,40.43%,73.42%
22,111.jpg," All the items have high factor loadings, which indicates that they are all good measures of the latent variable."," Fig. 3 shows some typical questions as instances. These questions can be applied to test the recall capability to remember key concepts of HVAC design (?What does a system manual typically include??), the analysis capability to solve questions that are focused on deep insights and analysis (?What's the MOST accurate statement about the task of sequencing heating and cooling??), and the application capability to answer calculation questions (??calculate the total pressure of a centrifugal fan?).",17.43%,77.97%,82.37%,2.67%,74.88%
23,112.jpg, Table 3. Results of the structural equation model.,"Distinguished from being created by human authors, AIGC refers to the automated creation of large-scale content in a manual-cost saving way based on Generative AI (GAI) techniques (Cao et al., 2023). Developed based on GAI techniques from GAN (Goodfellow et al., 2020) to ChatGPT (C. Zhang et al., 2023), AIGC technically refers to utilizing the GAI algorithms for content generation in the context of human instructions that would guide the model to complete the required task, with minimum human engagement in the production process (Fig. 1).",15.59%,71.98%,83.71%,0.00%,84.02%
24,113.jpg," The figure shows the relationships between job resources, personal resources, technology-driven hindrance stressors, technology-driven challenge stressors, and workarounds.
Job resources are formal and informal support structures. Personal resources are trait resilience. Technology-driven hindrance stressors are technology-driven role stressors and technology-driven time stressors. Technology-driven challenge stressors are technology-driven learning stressors and technology-driven emotional stressors. Workarounds are coping strategies.
The relationships between the variables are as follows:
- Job resources have a negative relationship with technology-driven hindrance stressors.
- Job resources have a negative relationship with technology-driven challenge stressors.
- Personal resources have a negative relationship with technology-driven hindrance stressors.
- Personal resources have a negative relationship with technology-driven challenge stressors.
- Technology-driven hindrance stressors have a positive relationship with workarounds.
- Technology-driven challenge stressors have a positive relationship with workarounds.
- Job resources have a negative relationship with workarounds.
- Personal resources have a negative relationship with workarounds.",The historical view of the development of Generative AI is given in Fig. 2.,3.09%,73.40%,81.16%,34.24%,57.17%
25,114.jpg," The figure shows the structural model with standardized path coefficients. All hypothesized relationships were supported. Workarounds were positively related to personal resources (β = 0.36, p < 0.01) and trait resilience (β = 0.26, p < 0.01). Technology-driven hindrance stressors were positively related to workarounds (β = 0.20, p < 0.05), and technology-driven challenge stressors were negatively related to workarounds (β = -0.18, p < 0.05). Finally, personal resources were positively related to trait resilience (β = 0.56, p < 0.01). All control variables were significantly related to workarounds, except for age.","The autoregressive model can be interpreted as a feedforward network incorporating all the preceding contextual information, which forms the foundation of GPT models. In an early stage, models from GPT-1 to GPT-3 are built based on the decoder-only Transformer architecture below and further fine-tuned on specific tasks (Fig. 3).",8.91%,75.33%,80.34%,36.96%,59.26%
26,115.jpg," The table shows the frequency and percentage of each category, as well as the non-response bias tests (early vs. late 25% of respondents). The t-value and p-value are also shown.

For gender, the t-value is 0.87 and the p-value is 0.32. This means that there is no significant difference between the early and late respondents in terms of gender.

For age, the t-value is 0.96 and the p-value is 0.40. This means that there is no significant difference between the early and late respondents in terms of age.

For education, the t-value is 1.33 and the p-value is 0.22. This means that there is no significant difference between the early and late respondents in terms of education.

For usage experience, the t-value is -1.20 and the p-value is 0.19. This means that there is no significant difference between the early and late respondents in terms of usage experience.","GPT-3.5 and GPT4. On top of the GPT-3 model, the GPT-3.5 is extended with supervised finetuning, and reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022). Capturing a large number of language patterns and representations, GPT-3.5 is then expected to logically generate suitable content in the specific contexts. Therefore, pairs of prompts and the corresponding answers written by human labeler are constructed as the dataset for the supervised fine-tuning (Fig. 4), namely instruction tuning (Wei et al., 2022), and large-scale code data are used for model enhancement as well (Chen et al., 2021). Furthermore, considering a higher-level human-machine alignment and the creativity of the model to avoid boilerplate answers, the RLHF is applied to the model, in which a reward model is trained firstly based on the data given by human labeled that indicate the rank of the generated contents for a single prompt, and is then used to give the rewards for the outputs to update the model policy using Proximal Policy Optimization (PPO) (Schulman et al., 2017) in the large loop. Due to the data label-independent property, RLHF punishes and rewards the model behavior and plays the significant role in the breakthrough of GPT-3.5 (Fig. 4). Beyond unimodality, the latest GPT-4 model can handle multimodal tasks from image and textual input to textual output process multimodal input. Despite of the unreleased technical details, GPT-4 demonstrates higher capacity of understanding and human alignment, indicating the continuation of the previous but enhanced training framework.",45.16%,79.49%,78.31%,27.09%,86.50%
27,116.jpg," Table 1 shows the descriptive statistics, reliability coefficients, and correlations among the study variables. All of the variables were normally distributed, and the reliability coefficients were all above 0.70, indicating good reliability. The correlations between the variables were all significant at the 0.05 level, except for the correlation between age and gender, which was not significant.","Disinformation Creation with LLMs (Borji, 2023): Feed the generated prompt into a LLMs to produce high-quality, persuasive disinformation content in different flavors. For instance, using an LLM like GPT-4, the input prompt could generate false news articles outlining alleged unethical behavior by the targeted candidate, complete with fabricated quotes and supporting evidence. A flow diagram for this process can be seen in Fig. 8.",16.19%,92.21%,82.09%,34.06%,67.66%
28,117.jpg," From the table, we can see that:

M11, M12, M13, F11, F12, F13 are in the Marketing, Logistics, Development planning department.
M21, M22, M23, M24, F21 are in the Finance, Logistics, Human resource department.
M31, M32, F31, F32, F33 are in the Finance, Marketing, Human resource, Logistics department.
M41, M42, M43, F41 are in the Human resource, Development planning department.","Disinformation Packaging: Create a suitable presentation for the disinformation content, such as a fabricated news article, social media post, or multimedia message. This may involve developing fake headlines, crafting misleading images or thumbnails, and selecting eye-catching formatting styles using AI tools such as break your own news1 and Midjourney2: a tool for generated life like images (Oppenlaender, 2022). This can also be given to an AI video-generating tool such as Synthesia3 to generate an AI anchor for reading out the script for the article. For example, the fabricated article could be given a sensational headline like ?Shocking Scandal: Candidate X Caught in Corruption Scheme!? and paired with an edited image of the candidate in a compromising situation. This can then be made into a video using Synthesia and reported as a news channel report. The flow of this step can be seen in Fig. 9.",12.56%,77.21%,77.61%,10.51%,69.40%
29,118.jpg, **Figure 1.** Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram.,"Content Dissemination: Execute the dissemination strategy by posting the disinformation content on the designated social media platforms using the fake or compromised accounts. For example, share the fabricated news article on Twitter, Facebook, and Reddit, using the established accounts and following the planned dissemination strategy. This has been illustrated in Fig. 10.",4.19%,41.62%,81.12%,1.91%,76.04%
30,119.jpg," The figure shows the relationships between data breach facilitators and data breach impacts in the context of health data breaches. Data breach facilitators are the factors that make it easier for a data breach to occur, while data breach impacts are the negative consequences of a data breach.

The data breach facilitators identified in the study are:
* Non-compliance
* Lack of awareness
* Data protection failure in third-party sources
* Organizational characteristics
* Inappropriate collective responses

The data breach impacts identified in the study are:
* Privacy concerns
* Health information sharing
* System use
* Stakeholder trust
* Care quality
* Trust repair costs

The study found that there are significant relationships between data breach facilitators and data breach impacts. This means that organizations that have strong data protection practices are less likely to experience a data breach, and if they do experience a data breach, the impact will be less severe.

The study also found that the type of data that is breached can have a significant impact on the severity of the breach. For example, a breach of health information can have a more serious impact than a breach of financial information.

The study's findings suggest that organizations need to take steps to improve their data protection practices in order to reduce the risk of a data breach and to mitigate the impact of a breach if it does occur.","To implement a domain-aware question-answering (QA) system, LeanContext implements a traditional retrieval augmented generation (RAG) framework (Lewis et al., 2020). The RAG system is constructed through two distinct phases: (a) domain data ingestion and (b) query-response. Fig. 2 illustrates the workflow of a domain-aware QA system based on RAG.",15.69%,57.51%,79.82%,36.18%,76.65%
31,12.jpg, The figure shows the system architecture of the proposed approach. The user interface module interacts with the user and collects the search query. The search module then retrieves the relevant documents from the knowledge bases. The interaction processing module processes the user’s interaction with the system and extracts the user’s preferences. The recommendation module uses the extracted user preferences and the retrieved documents to generate recommendations. The knowledge bases store the documents and the user’s interaction data.,"For context-based QA, generally, the answers reside within a couple of sentences. If the smallest amount of context for a certain question can be identified, the same response can be provided by LLMs at a lower cost i.e. less prompt tokens. So, identifying the top-sentences can reduce the context without compromising accuracy. Motivated by this simple idea, we propose LeanContext which is shown in Fig. 3.",28.91%,85.31%,83.19%,38.53%,76.15%
32,120.jpg," The diagram shows the ransomware risk assessment and mitigation process. The process starts with identifying the organization's assets and vulnerabilities. Once the assets and vulnerabilities have been identified, the organization can assess the risks associated with ransomware attacks. The organization can then develop and implement mitigation strategies to reduce the risks of ransomware attacks.

The following are some of the key steps involved in the ransomware risk assessment and mitigation process:

1. Identify the organization's assets and vulnerabilities.
2. Assess the risks associated with ransomware attacks.
3. Develop and implement mitigation strategies to reduce the risks of ransomware attacks.
4. Monitor the effectiveness of the mitigation strategies and make adjustments as needed.

By following these steps, organizations can reduce the risks of ransomware attacks and protect their data and systems.","Fig. 4 shows how LeanContext constructs the Reduced Context. It retains the most relevant top-sentences, which are critical for preserving the core context, while concurrently applying an open-source text reduction method to condense the sentences that lie between these top-sentences. This step helps in eliminating extraneous information and noise from the text. Moreover, any sentences that appear beyond the last top-sentence are omitted from consideration. This selective approach ensures that only the most contextually relevant information is retained, contributing to a more efficient and focused analysis. Importantly, LeanContext also maintains the original order of both the top-sentences and the other sentences in the text. This preservation of sentence order ensures that the structural integrity of the input text remains intact. This holistic approach ultimately leads to more accurate and informative results when processing text data, while still achieving the goal of minimizing LLM API usage costs.",39.59%,89.00%,82.81%,42.17%,74.44%
33,121.jpg," This image shows the relationships between the different constructs in the study. The constructs are:
* Threat appraisal
* Vulnerability assessment
* Ransomware risk assessment
* Coping appraisal
* Ransomware risk mitigation

The study found that there is a positive relationship between threat appraisal and ransomware risk assessment. This means that the higher the level of threat appraisal, the higher the level of ransomware risk assessment. The study also found that there is a positive relationship between vulnerability assessment and ransomware risk assessment. This means that the higher the level of vulnerability assessment, the higher the level of ransomware risk assessment. The study also found that there is a positive relationship between ransomware risk assessment and coping appraisal. This means that the higher the level of ransomware risk assessment, the higher the level of coping appraisal. Finally, the study found that there is a positive relationship between coping appraisal and ransomware risk mitigation. This means that the higher the level of coping appraisal, the higher the level of ransomware risk mitigation.","In this section, we introduce ChatDiet, a framework for personalized nutrition-oriented food recommendation chatbots. This framework utilizes LLMs to effectively incorporate not only population knowledge but also individual-specific data through augmented models. It includes an Orchestrator that interacts with personal and population models to extract relevant information based on users? inquiries. It then sends the aggregated information to an LLM to be integrated with the LLM?s internal knowledge and offer interactions with users. Fig. 1 indicates an overview of the framework. To clarify the functionality and definitions of the framework, we present and exemplify different components of ChatDiet via a case study. In the following, we initially delve into our case study, focusing on mHealth system leveraging wearable and mobile-based data logging.",23.30%,88.06%,80.02%,41.13%,78.40%
34,122.jpg,Error: Invalid response from model,"There are different types of human reasoning, each with its unique characteristics and applications (Fig. 1). In this section, we describe these different types of reasoning, following the classification done by Bang et al. (2023): logical reasoning, non-textual semantic reasoning, commonsense reasoning, causal reasoning, and multi-hop reasoning. Understanding different types of human reasoning is essential to evaluate the reasoning skills of advanced language models like GPT-3.5, GPT-4, and BARD.",0.00%,60.21%,81.21%,0.00%,65.93%
35,123.jpg," | Variable | Values | Count | Min | Max | Mean | Std Dev |
| ------- | -------- | ----- | --- | --- | ---- | -------- |
| Critical Industry (CI) | Yes | 604 | 0 | 137 | 27 | 25 |
|  | No | 869 | 2 | 89 | 41 | 22 |
| Organization Size (Size) | Small | 619 | 0 | 95 | 22 | 23 |
|  | Medium | 854 | 0 | 137 | 49 | 29 |
|  | Large | 732 | 0 | 86 | 27 | 25 |
| Digital Intensity (DI) | High | 732 | 0 | 57 | 17 | 17 |
|  | Low | 741 | 0 | 63 | 19 | 19 |
| Network Segmentation (NS) | Yes | 474 | 0 | 161 | 51 | 47 |
|  | No | 999 | 1 | 69 | 22 | 27 |
| Vulnerability Assessment (EVSS) |  | 279 | 0 | 39 | 8 | 13 |
| Attack Vector (AV) | Local | 223 | 0 | 181 | 61 | 53 |
|  | Network | 1164 | 0 | 5 | 2 | 1 |
|  | Physical | 152 | 0 | 181 | 66 | 57 |
| Privileges Required (PR) | High | 152 | 0 | 9 | 73 | 2 |
|  | Low | 1321 | 0 | 221 | 60 | 60 |
| Attack Complexity (AC) | High | 1088 | 0 | 219 | 69 | 63 |
|  | Low | 1451 | 0 | 177 | 59 | 54 |
| User Interaction (UI) | None | 375 | 0 | 45 | 12 | 12 |
|  | Required | 1098 | 0 | 223 | 63 | 55 |
|  | Not Changed | 1264 | 0 | 7 | 69 | 32 |
| Scope (S) | Changed | 209 | 0 | 223 | 66 | 57 |
|  | Not Changed | 1203 | 0 | 7 | 66 | 32 |
| Confidentiality-Integrity-Availability (CIA) | Confidentiality | 133 | 0 | 18 | 1 | 1 |
|  | Integrity | 1203 | 0 | 5 | 66 | 1 |
|  | Availability | 137 | 0 | 34 | 7 | 8 |
| Information Security Governance |  | 1156 | 1 | 203 | 60 | 57 |
| Cyber-security Role (CSR) | Yes | 317 | 1 | 19 | 65 | 61 |
|  | No | 1207 | 0 | 188 | 26 | 55 |
| Attack | Ransomware | 266 | 0 | 32 | 65 | 797 |
|  | In Million US$ | 1473 | 0 | 4219 | 1254 | 797 |
<br>

* represents per year occurrence","LangTest workflow involves a systematic process to improve language models. Initially, the library trains a language model using a diverse real world data set. Subsequently, it rigorously tests the model?s performance in various dimensions, including bias, robustness, accuracy, fairness, and security. If the model fails on specific tests, data augmentation can be used on training data to incorporate extra examples specific to the gaps identified. The model is then retrained using this augmented data set, leading to a more reliable version. The iterative nature of this process involves repeated tests to gauge the model?s progress in terms of robustness, fairness, and overall effectiveness (see Fig. 1).",0.73%,-8.13%,73.37%,0.00%,30.04%
36,124.jpg," **Correlation Matrix**

|  | CI | Size | DI | NS | EVSS | CSR | Type of Attack |
|---|---|---|---|---|---|---|
| CI | 1 | 
| Size | -0.042 | 1 | 
| DI | -0.121** | -0.074 | 1 | 
| NS | -0.096** | -0.048 | 0.070 | 1 | 
| EVSS | 0.009 | -0.055 | -0.049 | 0.048 | 1 | 
| CSR | 0.000 | 0.215** | 0.034 | -0.004 | -0.070 | 1 | 
| Type of Attack | 0.170 | 0.204** | 0.143** | -0.141** | 0.052 | -0.024 | 1 | 

**Note.** *p < .05 (2-tailed).
**p < .01 (2-tailed).","As presented in Fig. 1, which illustrates the process of the intervention protocol, the Critical Thinking Scale (CTS), Creative Thinking Scale (MCTS), and Reflective Thinking Scale (RTS) were administered as a pretest to both groups of respondents. After participating in sampling method activities over 3 weeks, the respondents were re-examined utilizing the same scales. A semi-structured opinion guide for students was also implemented to garner qualitative data. This form was employed to acquire qualitative data established on the students' research objectives and, therefore, to achieve triangulation by gathering data from respondents leveraging quantitative and qualitative approaches to enhance the findings and achieve more details. The semi-structured guide for the students was utilized to determine the benefits and hindrances of ChatGPT in the activities and to expose underlying characteristics that could influence the quantitative findings.",1.83%,8.90%,76.01%,27.47%,52.15%
37,125.jpg," From the table, we can see that all the p-values are less than 0.05, which means that all the hypotheses are supported.","The process of software development often involves retrieving relevant source code snippets. However, traditional keyword-based search engines may not always be effective in retrieving the most relevant code snippets. To address this issue, various techniques have been proposed in the literature, including machine learning-based approaches that use NLP techniques to match query statements with source code. In this paper, we propose a novel approach that combines Bi-LSTM and GNNs to improve the accuracy of source code retrieval. This methodology involves using a Bi-LSTM and GNNs model to encode query and source code and then applying attention mechanisms to identify relevant code snippets. The detailed methodology for building LLM for source code retrieval using Bi-LSTM is shown in Fig. 1.",11.50%,82.12%,82.29%,0.63%,76.26%
38,126.jpg," | Model | Algorithms | Accuracy (p) | Misclassification rate (q = 1-p) | Precision | Recall | F1 score |
|---|---|---|---|---|---|---|
| M1 | LR | 0.93 | 0.07 | 0.97 | 0.89 | 0.93 |
| M2 | NB | 0.90 | 0.10 | 0.66 | 0.81 | 0.66 |
| M3 | DT | 0.92 | 0.08 | 0.86 | 0.45 | 0.66 |","An overview of our methodology to evaluate 6 biomedical tasks across 26 datasets in this paper. At first, we construct the prompt for each dataset. Then, we generate the response for each dataset using respective LLMs. Finally, depending on the task, we apply various evaluation techniques.",0.00%,-4.87%,79.58%,0.00%,35.18%
39,127.jpg," This table summarizes the main activities, results, and sections of the research project.","The success of these models is not solely determined by the quality of their output and their general-purpose nature (refer to Section 2 for more details), but primarily by the way users can interact with them. Fig. 1 elucidates this interaction. A user starts by defining a task. The user then interacts with the model by describing a task (e.g. writing a research paper, generating novel ideas for a market advertising, or debugging a Python code) via a prompt (i.e., a request made to the generative system), which is then sent to the model. The model analyses the prompt and responds with a textual output. Since the user expects a certain result, the result is compared with the actual output. The user is then able to iterate a new prompt to refine the output, thus engaging in a form of conversation with the system until the output does not match the expected result.",31.30%,87.48%,83.35%,0.00%,83.10%
40,128.jpg," Phase 1: The development strategy was **conceptual-to-empirical**. The research question was: ""What is the relationship between social media use and brand loyalty?"" The methods used were: **a structured literature review** to identify the key concepts and theories related to social media use and brand loyalty; **open coding** of social media data to identify the key themes and patterns related to social media use and brand loyalty; and **a hermeneutic literature review** to interpret the findings of the open coding and develop a conceptual framework for understanding the relationship between social media use and brand loyalty.

Phase 2: The development strategy was **empirical-to-conceptual**. The research question was: ""How does social media use influence brand loyalty?"" The methods used were: **open coding** of social media data to identify the key themes and patterns related to social media use and brand loyalty; and **a hermeneutic literature review** to interpret the findings of the open coding and develop a conceptual framework for understanding how social media use influences brand loyalty.

Phase 3: The development strategy was **empirical-to-conceptual**. The research question was: ""What are the key factors that influence brand loyalty in the social media era?"" The methods used were: **open coding** of social media data to identify the key themes and patterns related to social media use and brand loyalty; and **a hermeneutic literature review** to interpret the findings of the open coding and develop a conceptual framework for understanding the key factors that influence brand loyalty in the social media era.

Phase 4: The development strategy was **empirical-to-conceptual**. The research question was: ""How can social media be used to enhance brand loyalty?"" The methods used were: **open coding** of social media data to identify the key themes and patterns related to social media use and brand loyalty; and **a hermeneutic literature review** to interpret the findings of the open coding and develop a conceptual framework for understanding how social media can be used to enhance brand loyalty.","Fig. 2 presents the methodological steps for analysing the tasks performed by ChatGPT. First, we collected tweets related to ChatGPT; second, we used NER techniques to identify the tasks that users assigned to ChatGPT; third, we normalised the identified tasks; and finally, we clustered the tasks using BERTopic",27.44%,64.17%,79.63%,36.21%,78.81%
41,129.jpg," This table shows the dimensions and characteristics of digital transformation that were identified in each iteration of the research process. The first iteration was conceptual-to-empirical, the second iteration was empirical-to-conceptual, the third iteration was empirical-to-conceptual, and the fourth iteration was empirical-to-conceptual. The dimensions and characteristics that were identified in each iteration are listed in the table. The table also shows the number of dimensions and characteristics that were identified in each iteration.","Seppänen et al. (2013) underscored the importance of SSA in driving effective decision-making during emergencies. Drawing from a rich tapestry of literature on SSA in crisis management (Ödlund, 2010, Virrantaus et al., 2009, Waugh and Streib, 2006), they identified three pivotal factors influencing SSA within crisis response entities outlines in Fig. 1. The first, information needs or link content, bifurcates into critical information (vital for SSA development) and action-triggering information (ensuring proactive information dissemination). The second, link type, pertains to the modality of information communication. Trust, the third factor, is instrumental in gauging the quality of the link. Seppänen et al. (2013) concentrate on SSA within crisis agencies, overlooking public SSA. We identify this as a key gap, emphasizing the necessity of SSA between agencies and the public for effective crisis management.",27.39%,83.46%,80.62%,22.94%,74.72%
42,13.jpg," The flow chart describes the process of creating synthetic user profiles for an article recommendation system. The process starts by selecting a random number of users from the base profile. If the number of users is greater than 0, then a random number of sessions is created for each user. If the number of sessions is greater than 0, then a random number of interactions is created for each session. The interactions are then used to update the user profile. This process is repeated until the desired number of synthetic user profiles has been created.","As Fig. 2 shows, this study employs a comprehensive three-tiered methodology to dissect the dynamics of crisis communication on social media platforms. Starting with a thematic analysis of Facebook posts, the research unveils the content and essence of health agencies' communication. Progressing to a narrative analysis of informant interviews, it demystifies the underlying decision-making processes and the broader communication strategy. The final phase, a thematic analysis of Facebook comments, offers a lens into the real-world effectiveness of these strategies and the consequential development of public trust. Collectively, these phases provide highly relevant insights, capturing the intricate interplay between information, communication, and trust in a digital age.",30.90%,88.74%,82.55%,40.94%,81.53%
43,130.jpg," This diagram shows the relationships between different factors in organizational design. The factors are grouped into three categories: organizational design problem, equivocality, and uncertainty.

The organizational design problem is the problem of designing an organization that is both effective and efficient. The factors that contribute to the organizational design problem are:

* **Interpersonal relations and characteristics:** These are the relationships between the people in the organization, and the characteristics of those people.
* **Nature of tasks:** This is the type of work that is done in the organization.
* **Interdepartmental/interorganizational relations:** These are the relationships between the different departments or units in the organization, and the relationships between the organization and other organizations.

Equivocality is the degree of ambiguity or uncertainty in a situation. The factors that contribute to equivocality are:

* **Interdependence:** This is the degree to which the different parts of the organization are dependent on each other.
* **Differentiation:** This is the degree to which the different parts of the organization are different from each other.
* **Analyzability:** This is the degree to which the different parts of the organization can be understood and analyzed.
* **Variety:** This is the degree to which the different parts of the organization are different from each other.

Uncertainty is the degree of risk or unpredictability in a situation. The factors that contribute to uncertainty are:

* **Rules and programs:** These are the formal rules and procedures that govern the organization.
* **Hierarchical referral:** This is the degree to which decisions are made at the top of the organization and then passed down to the lower levels.
* **Goal setting:** This is the process of setting goals for the organization.

The diagram shows that the organizational design problem is affected by equivocality and uncertainty. Equivocality is affected by the nature of tasks, interdepartmental/interorganizational relations, and interpersonal relations and characteristics. Uncertainty is affected by rules and programs, hierarchical referral, and goal setting.

The diagram also shows that there are a number of factors that can be used to reduce equivocality and uncertainty. These factors are:

* **Supporting interpersonal development:** This is the process of helping the people in the organization to develop their skills and abilities.
* **Increasing the capacity to process information:** This is the process of giving the organization the resources it needs to process information.
* **Increasing information processing richness:** This is the process of making the information that is available to the organization more useful.
* **Creation of lateral resources:** This is the process of creating new resources that can be used to solve problems.
* **Time and job rotation:** This is the process of moving people around to different jobs and giving them new responsibilities.
* **Investment in information systems:** This is the process of giving the organization the technology it needs to process information.

The diagram can be used to help organizations understand the factors that affect their design. By understanding these factors, organizations can make better decisions about how to design themselves.","Aligned with prior studies (Pennycook et al., 2020, Velichety and Shrivastava, 2022), we classified information typologies that have been disseminated by health agencies to address misinformation. The authors analysed the posts to identify themes during the systematic analysis informed by Terry et al. (2017) and outlined in Fig. 3. Braun and Clarke (2006) emphasise that a theme captures an important aspect of the data in a patterned manner ?regardless of whether that theme captures the majority experience? (Scharp & Sanders, 2019, p. 1). While recognising a pattern might involve noting frequency, it's more about interpreting meaning than highlighting quantity. Braun and Clarke (2006) note there's no definitive threshold for how much of the data must evidence a theme for it to be considered valid. Essentially, the themes should address the research question, even if not explicitly mentioned in every instance.",42.05%,60.99%,77.49%,38.30%,80.63%
44,131.jpg," The figure shows the interactions between different components of an AI system in a hospital. The AI system interacts with the medical process, the patient, and the hospital's organizational resources. The AI system also interacts with the regulatory and compliance requirements. The arrows between the components show the direction of the interaction. For example, the arrow from the ""AI application"" component to the ""medical process"" component shows that the AI system interacts with the medical process by perceiving, replying, and aligning.","The agencies' communication team actively manages and moderates comments to curb misinformation. The comment moderation and filtering process is multi-tiered illustrated in Fig. 4.1.Automated Filtering: Initially, comments undergo automated filtering to catch and remove any that violate basic guidelines or contain profanities.2.Manual Review by Communication Team: Comments that pass the automated filter are then manually reviewed by the communication team. If misinformation appears unintentional and can be addressed, the team responds with a correction.3.Community Self-Moderation: Often, members of the online community themselves address and correct misinformation in comments. In such cases, the communication team may choose not to intervene, allowing the community's self-regulation to take precedence.4.Blocking Persistent Offenders: If a particular user consistently posts misinformation, especially if it seems intentional, the team takes the step of blocking that user to maintain the integrity of the information on the platform.",44.42%,89.97%,82.85%,26.34%,85.20%
45,132.jpg," The figure shows a taxonomy of management factors for the responsible use of AI in healthcare. The taxonomy is organized around three main themes: data management, AI technology management, and socio-ethical management. Each of these themes is further divided into sub-themes, and each sub-theme is divided into specific factors. For example, the data management theme is divided into the sub-themes of data quality, data accessibility, and data security. The data quality sub-theme is further divided into the factors of completeness, accuracy, and timeliness.

The taxonomy can be used to help organizations identify and manage the risks associated with the use of AI in healthcare. By understanding the different factors that can affect the responsible use of AI, organizations can take steps to mitigate these risks and ensure that AI is used in a way that is safe and ethical.","The ""SSA Dynamics Model"" in Fig. 5 outlines the complex interplay between information dissemination, communication strategies, and trust, essential for countering misinformation and enhancing SSA during crises. The model emphasize the fluidity of trust-building and SSA through ongoing interactions between the public and response agencies.",28.65%,89.91%,84.47%,39.09%,78.06%
46,133.jpg," The figure shows the identified 58 first-order concepts related to the management of AI in healthcare, grouped into 12 second-order themes, which in turn are grouped into four aggregate dimensions.",We used a self-developed Python app and collected a random sample of 765 posts and their linked comments published from 1 January to the end of April 2020 on the NSW1 and VIC2 Department of Health Facebook page. Table 2 presents a summary of our initial Facebook dataset.,11.72%,78.19%,83.90%,23.17%,68.61%
47,134.jpg," The figure shows the interactions between different components of an AI system in a healthcare setting. The components are:

- **AI application**: This is the component that uses AI to perform a specific task, such as diagnosing a disease or recommending a treatment.
- **Technical AI management**: This component is responsible for managing the technical aspects of the AI system, such as the data it uses and the algorithms it employs.
- **Integration management**: This component is responsible for integrating the AI system with the rest of the healthcare system, such as the electronic health record (EHR) system.
- **Contextual management**: This component is responsible for managing the context in which the AI system is used, such as the patient's medical history and the hospital's policies and procedures.
- **Patient requirements management**: This component is responsible for managing the patient's requirements, such as their preferences and their medical needs.
- **Medical process management**: This component is responsible for managing the medical process, such as the diagnosis, treatment, and monitoring of the patient.

The arrows between the components show the direction of the interactions. For example, the arrow from the AI application to the technical AI management component shows that the AI application sends data to the technical AI management component. The arrow from the technical AI management component to the integration management component shows that the technical AI management component sends data to the integration management component.

The dashed lines between the components show the relationships between the components. For example, the dashed line between the AI application and the patient requirements management component shows that the AI application is related to the patient requirements management component.

The dotted line between the contextual management component and the medical process management component shows that the contextual management component is related to the medical process management component.","To investigate official Facebook use against misinformation, we analysed 29 posts that directly addressed COVID-related uncertainties or misinformation. This targeted approach aimed to understand official strategies to counter misinformation during the pandemic. These posts aligned with the four major thematic areas of the COVID-19 infodemic identified by the ?Coronavirus Disease 2019 (COVID-19) Situation Report?85? (WHO, 2020). Table 3 showcases samples of these posts by Australian health agencies.",23.28%,79.46%,78.89%,36.17%,78.48%
48,135.jpg," Structural model of the relationships between game design affordances, activity satisfaction, and behavioral engagement.","We conducted nineteen semi-structured interviews, averaging an hour each, with professionals from the health agencies whose Facebook activities we studied. These interviews, held online via Zoom, aimed to deepen our understanding of the topic, gauge the extent of the problem, comprehend the organisational approach to using Facebook against COVID-19 misinformation, and benefit from the experts' tactical knowledge. All interview participants held roles in the public health sector, as shown in Table 4. They were recognised for their exceptional academic and hands-on knowledge in the field, qualifying them as experts. Although our engagement with these experts significantly influenced our interpretation of the study's results, it's essential to clarify that only data from six informants were incorporated into this study. These six (rows i to vi in Table 4) were directly involved in managing and operating public health social media communication and were recognised as online community administrators.",14.65%,91.64%,83.10%,0.00%,75.77%
49,136.jpg," Structural model of the relationships among game design affordances, activity satisfaction, behavioral engagement, and game usage. *p < .05.","To examine the proposed mediation pathway (H2), we used SPSS PROCESS macro model 4 (Hayes, 2017) with 5000 iterations. Shoppers? decision-making styles (coded as maximisers = 1 and satisficers = 0) served as the independent variable, while perceived content integration capability was the mediator variable, and perceived omnichannel interaction quality was the dependent variable. Results show that shoppers? decision-making styles had a positive effect on perceived content integration capability (ß = 0.600, t = 3.497, p < .001), which in turn positively influenced perceived omnichannel interaction quality (ß = 0.447, t = 6.945, p < .001). In line with H2, the indirect effect (ab) of shoppers? decision-making styles on perceived omnichannel interaction quality through perceived content integration capability was positive and significant. Thus, perceived content integration capability mediated the proposed relationship as the confidence interval excluded zero (ß = 0.268, 95% CI = [0.115, 0.445]), as presented in Fig. 2 and Table 6.",8.95%,81.14%,80.87%,0.04%,83.50%
50,137.jpg," Structural equation model of the relationships between game design affordances, activity satisfaction, and behavioral engagement. *p < .05; **p < .01; ***p < .001.","Manipulation checks were examined using a one-way ANOVA test and the MTS measures. Results show that maximisers showed higher mean MTS scores compared to satisficers (Mmaximisers = 6.04, SD = 0.57, Msatisficers = 4.81, SD = 0.77, F[1,127] = 234.681, p < .001). Therefore, the results confirm the effectiveness of the priming task used in Study 3. To assess the moderating effect of omnichannel configuration quality, we applied SPSS PROCESS macro model 5 (Hayes, 2017). Shoppers? decision-making styles (maximisers = 1 and satisficers = 0) were coded as the independent variable, with omnichannel configuration quality serving as the moderator variable, perceived content integration capability serving as the mediator variable and perceived omnichannel interaction quality serving as the dependent variable. Results confirm that the interaction between shoppers? decision-making styles and omnichannel configuration quality had a positive effect on perceived omnichannel interaction quality (ß = 0.288, t = 3.080, p < 0.001). Interestingly, decision-making styles had no effect on omnichannel interaction quality when configuration quality was low (ß = 0.007, p = .962, 95% CI [?0.297, 0.312] for ?1 SD, Mmaximisers = 4.23 Msatisficers = 4.24). However, when omnichannel configuration quality was high, maximisers perceived higher omnichannel interaction quality compared to satisficers (ß = 0.642, p < .001. 95% CI [0.324, 0.959] for  SD, Mmaximisers = 5.13, Msatisficers = 5.77). Therefore, controlling for the mediation effect, results confirm the conditional indirect pathway from shoppers? decision-making styles to perceived omnichannel interaction quality, as zero was excluded from the confidence intervals (ß = 0.143, CI [0.024, 0.315]). The higher the perceived omnichannel configuration quality was, the better the perceived omnichannel interaction quality was for maximisers compared to satisficers, as shown in Fig. 3 and Table 9.",11.95%,75.82%,78.18%,0.00%,81.97%
51,138.jpg," The table shows the correlation matrix of 16 variables. The variables are:
Game Creativity 1
Game Creativity 2
Game Achievability 1
Game Achievability 2
Game Immersibility 1
Game Immersibility 2
Game Immersibility 3
Competence Satisfaction 1
Competence Satisfaction 2
Competence Satisfaction 3
Autonomy Satisfaction 1
Autonomy Satisfaction 2
Autonomy Satisfaction 3
Autonomy Satisfaction 4
Relatedness Satisfaction 1
Relatedness Satisfaction 2
Relatedness Satisfaction 3
Relatedness Satisfaction 4
Game Continuance 1
Game Continuance 2
Game Usage 1

The correlation matrix shows the relationships between the variables. For example, the correlation between Game Creativity 1 and Game Creativity 2 is 0.76. This means that the two variables are positively correlated, meaning that as one variable increases, the other variable also tends to increase.

The correlation matrix can be used to identify relationships between variables and to explore the structure of the data.","The moderating effect of omnichannel configuration quality was tested using SPSS PROCESS macro model 5, as in Study 3. The results indicate a significant positive interaction between shoppers? decision-making styles and omnichannel configuration quality, positively impacting perceived omnichannel interaction quality (ß = 0.178, t = 2.168, p < 0.01). Notably, decision-making styles had no effect on omnichannel interaction quality when configuration quality was low (ß = ?0.050, p = .708, 95% CI [?0.314, 0.213] for ?1 SD). However, when omnichannel configuration quality was high, maximisers perceived better omnichannel interaction quality compared to satisficers (ß = 0.335, p < .001. 95% CI [0.102, 0.569] for  SD). Moreover, when controlling for the direct effect, the study?s results confirm full mediation. This outcome suggests that the relationship between shoppers? decision-making styles and perceived omnichannel interaction quality is entirely mediated by omnichannel configuration quality. The exclusion of zero from the confidence intervals (ß = 0.220, CI [0.108, 0.347]) substantiates the conditional indirect pathway from shoppers? decision-making styles to perceived omnichannel interaction quality. Thus, the findings of Study 4 confirm that omnichannel configuration quality showed a higher positive influence on maximisers than on satisficers, who reported higher omnichannel interaction quality (see Table 13 and Fig. 4). These findings support external validity of Studies 1, 2 and 3, as they build upon the broader and more general experiences of shoppers, providing a comprehensive understanding of the dynamics at play in digital omnichannel environments.",14.67%,70.04%,79.02%,22.87%,83.28%
52,139.jpg," Table 1. Correlation matrix and descriptive statistics for study variables (N = 296).

Note. * denotes p values < .05. Bolded and italicized numbers on the diagonal are positive square roots of the AVE values.","A total of 120 participants (Mage = 30.98, SDage = 8.14) were recruited by Qualtrics (see Table 4). Participants were randomly assigned to one of the two priming conditions (maximisers = 60 and satisficers = 60). The priming task and shopping task were the same as in Study 1. In this study, participants were asked to shop and select a new footwear product using a real-world omnichannel retailer. Perceived omnichannel interaction quality and MTS were measured as advised in Study 1. Perceived content integration capability was measured using two items adapted from Sun et al. (2020), such as ?I trust my ability to process the information I gather from different channels?. Participants responded to all items using a seven-point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree).",13.77%,85.89%,80.27%,3.22%,81.43%
53,14.jpg," ## **Plagiarism Checker**
**Input** : Flowchart images.
**Output** : Percentage of plagiarism.

### **1. Preprocessing**
The first step is to remove any arrows from the flowchart. This is done to simplify the image and make it easier to process. The next step is to dilate the image. This helps to fill in any small gaps or holes in the image and make the lines thicker.

### **2. Shape Detection**
The next step is to detect the shapes in the flowchart. This is done using the Canny edge detection algorithm. The Canny edge detection algorithm is a popular edge detection algorithm that uses a multi-stage process to detect edges in images.

### **3. Convert Flowchart to Directed Graph**
The next step is to convert the flowchart into a directed graph. A directed graph is a graph in which the edges have a direction. This is done by representing each shape in the flowchart as a node in the graph and each connection between two shapes as an edge in the graph.

### **4. Similarity Check**
The next step is to check the similarity between the two graphs. This is done by comparing the structure and shape of the graphs. The structure of a graph is defined by the number of nodes and edges in the graph and the way in which the nodes are connected. The shape of a graph is defined by the arrangement of the nodes and edges in the graph.

### **5. Display % of Plagiarism**
The final step is to display the percentage of plagiarism. This is done by calculating the percentage of nodes and edges that are similar between the two graphs.","A one-way ANOVA was used for the manipulation check using the MTS measures. Results show that maximisers scored higher in the MTS compared to satisficers (Mmaximisers = 5.95, SD = 0.42, Msatisficers = 4.30, SD = 0.52, F[1,118] = 359.280, p < 0.001). Therefore, the significant results of the manipulation checks confirm the success of the priming method used in this study. To examine the proposed mediation pathway (H2), we used SPSS PROCESS macro model 4 (Hayes, 2017) with 5000 iterations. Shoppers? decision-making styles (coded as maximisers = 1 and satisficers = 0) served as the independent variable, while perceived content integration capability was the mediator variable, and perceived omnichannel interaction quality was the dependent variable. Results show that shoppers? decision-making styles had a positive effect on perceived content integration capability (ß = 0.600, t = 3.497, p < .001), which in turn positively influenced perceived omnichannel interaction quality (ß = 0.447, t = 6.945, p < .001). In line with H2, the indirect effect (ab) of shoppers? decision-making styles on perceived omnichannel interaction quality through perceived content integration capability was positive and significant. Thus, perceived content integration capability mediated the proposed relationship as the confidence interval excluded zero (ß = 0.268, 95% CI = [0.115, 0.445]), as presented in Fig. 2 and Table 6.",38.74%,60.33%,77.85%,41.79%,88.46%
54,140.jpg, Table 1. Demographics of the participants (N = 550).,"Our study used stratified random sampling of 950 companies, establishing equal probability that any firm could be selected at any step during sampling. The companies were contacted by phone and e-mail to explain the study?s purpose and offer them the opportunity to receive the results once the study was finished. Analyzing the results in aggregate and promising confidentiality of responses increased the response rate (36.10%, 343 valid responses (see Table 4)) and reduced the possibility of desirability bias.",24.74%,64.51%,84.10%,0.03%,82.29%
55,141.jpg," From the table, we can see that:

- H1a is not supported since the p-value (0.56) > 0.05.
- H1b is not supported since the p-value (0.58) > 0.05.
- H2a is supported since the p-value (< 0.01) < 0.05.
- H2b is supported since the p-value (< 0.01) < 0.05.
- H2c is supported since the p-value (< 0.01) < 0.05.
- H3a is supported since the p-value (< 0.01) < 0.05.
- H3b is supported since the p-value (0.01) < 0.05.
- H4a is supported since the p-value (< 0.01) < 0.05.
- H4b is supported since the p-value (< 0.01) < 0.05.
- H4c is not supported since the p-value (0.83) > 0.05.
- H5 is supported since the p-value (< 0.01) < 0.05.","A comparative analysis of two groups of respondents was performed, the companies that returned the completed survey within three weeks of receiving it and the companies that returned the survey only after follow-up reminders. This test assumes that late respondents are similar to non-respondents (Armstrong & Overton, 1977). Comparing the characteristics of the firms with late vs. early respondents (Table 5) indicated no significant differences between early and late respondents.",12.15%,58.93%,78.96%,36.48%,68.17%
56,142.jpg, Moderator and mediation effects of game satisfaction on the relationships between independent variables and continuance intention and usage intention.,"To analyse the data, we followed a thematic approach to identifying relevant patterns and themes (Braun & Clarke, 2006). A theoretical coding process was carried out, using an abductive approach based on the framework of reference (Dubois & Gadde, 2002). The coding process was rather iterative, going back and forth between the theory and the collected data (Gioia et al., 2013). This method provides consistent and rigorous analysis across three iterative stages of data analysis, following the approach commonly undertaken in case studies (Yin, 2014). With the support of NVIVO software, data were coded to identify prominent themes that helped us to glean important insights from the phenomenon under observation. This process was conducted through a within-case analysis for each SME, followed by a cross-case analysis with the continuous identification of how SME resource orchestration supports AI implementation as the company makes its digital transformation (Beverland & Lindgreen, 2010). Fig. 1 depicts the data analysis process developed in three stages (Gioia et al., 2013).",17.94%,77.69%,80.71%,0.03%,75.92%
57,143.jpg," The figure shows the results of a study on the factors influencing the adoption of XR technology in organizations. The study used a structural equation model to analyze the relationships between the variables. The model was tested using data from a survey of 100 organizations.

The results show that perceived organizational value of XR, expected employee resistance to XR, and organizational support for XR are all significant predictors of organizational XR adoption intention. Additionally, the model shows that environmental factors, such as mimetic pressure and external support, have an indirect effect on organizational XR adoption intention through their influence on perceived organizational value of XR. Finally, the model shows that organizational factors, such as organizational support for XR and employee technology skills, have an indirect effect on organizational XR adoption intention through their influence on expected employee resistance to XR.

The study's findings suggest that organizations should consider the perceived organizational value of XR, expected employee resistance to XR, and organizational support for XR when making decisions about whether or not to adopt XR technology. Additionally, the findings suggest that organizations should focus on creating a supportive environment for XR adoption, including providing training and support for employees and ensuring that the technology is compatible with the organization's existing systems and processes.","DOI posits that an individual?s decision to adopt an innovation is a process consisting of five sequential stages (Rogers, 1983) (see Fig. 2). The DOI?s five stages have parallels to Prochaska?s transtheoretical model of behavior change (Prochaska et al., 1992) and McGuire?s hierarchy of effects (McGuire, 1989), supporting the assumption that these stages do exist (Rogers, 2003).",19.17%,54.79%,80.04%,38.68%,73.50%
58,144.jpg," The figure shows the relationships between the perceived organizational value of XR, the expected employee resistance to XR, and the organizational adoption intention of XR. The perceived organizational value of XR is positively related to the organizational adoption intention of XR. The expected employee resistance to XR is negatively related to the organizational adoption intention of XR. The conditions that moderate the relationships between the perceived organizational value of XR, the expected employee resistance to XR, and the organizational adoption intention of XR are also shown in the figure. These conditions include the cost of XR hardware and software, the availability of XR content, the level of employee training, and the organizational culture.","Table 3 shows the participating companies. Company A is an industrial technology provider with approximately 250 employees from various sectors, with a focus on engineering and software development. Their product relates to circular economy and recycling, and according to their own presentation, environmental sustainability is already embedded in their culture. Company B is an industrial software provider with approximately 400 employees who are mainly software developers due to their core B2B software product. Company B is making various sustainability efforts, such as offsetting its emissions, using green power, and switching to electric vehicles, but its employees have not been as involved in its sustainability strategy. Company C is a glass manufacturer with approximately 150 employees. The majority of these employees work in the production halls and often come from abroad or have little education. In administration, about 30 employees work in research and development, procurement, sales, marketing and process optimization. The company attaches great importance to the recycling and reuse of its glass products and environmentally compatible production, but is particularly interested in new approaches to creatively involve its employees and motivate them to adopt sustainable behavior. Finally, Company D is a web design agency with 30 employees, mainly software developers and UX designers. Environmental sustainability has gained importance for them as a key criterion of responsible and sustainable digitalization. However, the company is not yet engaged in efforts for sustainability.",22.35%,93.18%,81.75%,16.72%,82.94%
59,145.jpg," The table shows the frequency and percentage of respondents in a survey, grouped by gender, age, education, and organizational position.

The majority of respondents were male (80.1%), aged between 35 and 44 (25.7%), and had a bachelor's degree (62.1%). Most respondents were in middle management (24.8%) or top management (31.6%).",Details of the data collection and analysis process and participants of both studies are provided in the respective sections. Fig. 4 illustrates how the results of both studies in an integrated research process lead to an in-depth understanding of the challenges employees encounter when introducing and using green IS in the workplace.,31.00%,48.67%,82.54%,42.27%,65.46%
60,146.jpg," The table shows the frequency and percentage of companies in different industries and countries. The data is based on a survey of 120 companies.

The most common industry is automotive, which accounts for 25.1% of the companies. This is followed by aerospace and defence (18.4%), and construction (12.6%).

The most common country is Germany, which accounts for 29.2% of the companies. This is followed by the United States (16.7%), and the United Kingdom (12.5%).

The table also shows the frequency and percentage of companies in different size categories. The most common size category is 1-9 employees, which accounts for 58.3% of the companies. This is followed by 10-49 employees (22.5%), and 50-249 employees (10.8%).

The table also shows the frequency and percentage of companies in different age categories. The most common age category is 10-19 years, which accounts for 32.5% of the companies. This is followed by 20-29 years (21.7%), and 30-39 years (17.5%).","Consumer satisfaction is formed through a cognitive process that links prior expectations to perceived performance and confirms or disconfirms expectations concerning the actual performance (Morgeson, 2012). In digital services, specifically in the adoption of AI, performance expectancy is an important predictor of customer emotions, as higher levels of performance expectancy lead to increased usage intentions (Choi et al., 2011, Gursoy et al., 2019b). Thus, this research proposes that performance expectancy is the underlying mechanism explaining the effect of AI vs. autonomous choices on satisfaction. In particular, when AI makes the decision, there is a conflict between autonomy and technology that decreases consumers? performance expectations and, as a result, decreases satisfaction. Fig 1.Hypothesis 3 Performance expectancy mediates the effect of AI vs. autonomous choices on satisfaction.",28.83%,59.63%,78.70%,41.99%,78.87%
61,147.jpg," The table shows the correlation matrix of the variables in the model. All of the variables are significantly correlated with each other, with the exception of the variables USE and VALU, which have a non-significant correlation. The strongest correlations are between the variables AVE and MSV (r = 0.96), AVE and USE (r = 0.92), and MSV and USE (r = 0.91). The weakest correlations are between the variables USE and VALU (r = 0.06), EXSU and VALU (r = 0.07), and RESI and VALU (r = 0.08).",Survey Items for Satisfaction.,0.00%,43.91%,81.43%,0.00%,59.20%
62,148.jpg," The table shows the fit indices for the measurement model and the path model. The measurement model has a good fit, with a CFI of 0.959 and a SRMR of 0.050. The path model also has a good fit, with a CFI of 0.946 and a SRMR of 0.071. The PClose value for the path model is 0.227, which indicates that the model is not very close to the population model. However, the other fit indices suggest that the model is a good representation of the data.",Survey Items for Performance Expectancy.,2.51%,67.21%,82.25%,34.10%,61.40%
63,149.jpg," The table shows the results of a study that investigated the relationships between different variables related to employee resistance to XR adoption. The hypotheses tested, the corresponding relationships, the effect sizes, and the support for each hypothesis are presented in the table.

The results show that there is a significant negative relationship between expected employee resistance and XR adoption intention (H1a), as well as between expected employee resistance and perceived organizational value of XR (H1b). This means that as expected employee resistance increases, XR adoption intention and perceived organizational value of XR decrease. These findings support the notion that employees who are more resistant to XR are less likely to adopt it and perceive it as valuable.

Furthermore, the results indicate that perceived organizational value of XR has a significant positive relationship with organizational XR adoption (H2), suggesting that when employees perceive XR as valuable for the organization, they are more likely to adopt it. This finding highlights the importance of communicating the benefits and value of XR to employees to encourage its adoption.

Additionally, the study found that employee technology use skills have a significant negative relationship with expected employee resistance to XR (H3), indicating that employees with higher levels of technology skills are less likely to resist XR adoption. This suggests that providing employees with training and support to develop their technology skills may help reduce resistance to XR.

The results also reveal that organizational support has a significant negative relationship with expected employee resistance to XR (H4a) and a significant positive relationship with perceived organizational value of XR (H4b). This implies that when employees perceive high levels of organizational support for XR, they are less likely to resist its adoption and more likely to view it as valuable. These findings emphasize the importance of providing employees with the necessary resources and support to facilitate XR adoption.

Interestingly, the study found no significant relationship between external support and expected employee resistance to XR (H5) or between mimetic pressure and perceived organizational value of XR (H6). This suggests that external support and mimetic pressure may not play a significant role in shaping employee resistance to XR adoption or their perception of its organizational value.

Furthermore, the results indicate that compatibility has a significant positive relationship with expected employee resistance to XR (H7a) but not with perceived organizational value of XR (H7b). This implies that employees who perceive XR as compatible with their existing work practices and tasks are more likely to resist its adoption. However, the compatibility of XR with the organization's goals and values does not seem to influence employees' perceptions of its organizational value.

Finally, the study found that trialability has a significant negative relationship with expected employee resistance to XR (H8a) but not with perceived organizational value of XR (H8b). This suggests that employees who have the opportunity to experiment with and trial XR before adoption are less likely to resist it. However, the trialability of XR does not seem to influence employees' perceptions of its organizational value.

In conclusion, the study provides valuable insights into the factors that influence employee resistance to XR adoption and their perceptions of its organizational value. The findings highlight the importance of addressing employee concerns, providing training and support, and ensuring compatibility and trialability to encourage XR adoption in organizations.","In total, we conducted 21 interviews. The data collection reached saturation when the interview data showed no new information (Jacobson & Harrison, 2022). Table 3 describes the participants? demographics. We elaborated an interview guide with 13 questions inspired by the theoretical model tested in the previous studies. The average time for each interview was 20 min. Interviews were conducted face-to-face and mediated by videoconferencing technology (Zoom) in Portuguese by the last author, who transcribed, anonymized, and translated the scripts into English.",22.54%,85.83%,79.32%,35.88%,75.10%
64,15.jpg, The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram of the study selection process.,"Together, these theoretical foundations guide our research by providing a framework for understanding the dynamics of AI adoption, shareholder perceptions, and the organizational business contexts. They allow us to delve into the complex interplay between actors, roles, and business conditions within the context of B2B marketing, ultimately contributing to a deeper comprehension of the impact of AI adoption on shareholder reactions. This study theorizes that the focal firm's AI implementation for B2B marketing can be seen as an action delivered by the firm (i.e., performer) to present its IT proficiency and future business opportunities to its shareholders (i.e., audience). Following this logic, adopting the social actor perspective to explain how AI implementation for B2B marketing presents their underlying capabilities or competencies to shareholders, affecting shareholders' perception and influencing the firm's market value. In addition, this study further argues that the shareholders' reaction to the implementation of AI for B2B marketing relies on different business conditions, such as the characteristics of firms' industry type (i.e., industry dynamism) and operational environments (i.e., customer complexity). In this way, it offers an insightful lens for situating firms in a broader social landscape and investigating the relationships between their AI-enabled B2B marketing and shareholder reaction in different environments. The conceptual model of this study is illustrated in Fig. 1.",26.58%,77.14%,78.92%,0.00%,83.64%
65,150.jpg," **Mediation Path**                                 **Effect**
Mimetic pressure → Perceived organizational value of XR → Organizational XR adoption intention               0.206**
Organizational support → Expected employee resistance to XR → Organizational XR adoption intention        0.088**
Employee technology use skills → Expected employee resistance to XR → Organizational XR adoption intention  0.069*
Triability → Expected employee resistance to XR → Organizational XR adoption intention                       0.047*

Note: *** p < 0.001, ** p < 0.01, * p < 0.05","After the data collection, panel A of Table 1 presents the characteristics of the sample firms. For instance, the mean of the sample firms' net income and total assets are $3292.382 million and $ 139,398 million, respectively. At the same time, the mean of sales, total liabilities and employee number of the sample firms are $ 24,361.830 million, $ 11,933.500 million, and 66,487, respectively. Panel B and C of Table 1 show the distribution of the sample firms by industry (via its two-digit SIC codes) and year. It illustrates that AI-enabled B2B marketing activities are most popular in the services industry (i.e., SIC 70?89), which takes 56.18% of the total sample. The distribution panel shows that most of the firms only started using AI for their B2B marketing practices during 2016?2020, with 2017 as the peak year.",2.13%,42.62%,75.82%,14.63%,62.52%
66,151.jpg," The diagram shows how potential risks or opportunities can lead to different types of harm or benefit. The realization of a potential is not always straightforward and there may be other factors that contribute to the outcome. The potential for harm or benefit can be short-term or long-term, known or unknown, unavoidable or mitigatable, individual or collective, operational or strategic, constrained or open. The realization of the potential can be direct or indirect, random or detectable, reversible or irreversible, physical, financial, legal, reputational, personal, or environmental.","After performing the binary logistic regression model, we acquire the propensity score, which indicates the probability of implementing AI-enabled B2B marketing initiatives for all firms included in the model. Then, we use a nearest-neighbor one-on-one matching method to identify the control firms. To improve the matching quality, we set a predetermined caliper of 0.02, which measures the absolute distance between the control and treatment firms' propensity scores (Ye et al., 2020). As shown in Model 1 (pre-match model) of Table 2, the number of firms included in the regression model is 1423, consisting of 89 treatment firms collected from Factiva, 1334 potential control firms with the same 4-digit SIC codes as the treatment firms. 87 out of the 89 treatment firms are matched successfully through the above-mentioned matching procedures and criteria. Therefore, the total sample size for this research reached 174, including 87 treatment firms and 87 matched control firms. Model 1 shows that the coefficients of marketing efficiency are negatively significant, while the coefficients of firm size, liquidity and R&D intensity are positively significant. This result indicates that firms with lower marketing efficiency, larger firm size, higher firm liquidity and greater R&D intensity tend to be more likely to employ AI technology for their B2B marketing practices. Also, we further check the matching quality by comparing the results of pre-match and post-match logistic regressions. As shown in Model 2 (post-match model) in Table 2, there are no statistically significant predictors, thus indicating a satisfying matching quality is achieved.",18.24%,87.69%,78.87%,6.87%,83.55%
67,152.jpg," The table shows the ballpark risk of different activities. The risk is expressed in terms of the number of people harmed, hospitalized, or killed per year in the United States. The activities are food safety (general), food safety (salmonella), food safety (listeria monocytogenes), consumer goods, sports (soccer), and health (smoking).

The highest risk activity is health (smoking), which is estimated to cause 8.7-11.7 additional deaths per year in the United States. This is followed by food safety (general), which is estimated to cause 0.0009% of US citizens to die each year. The lowest risk activity is sports (soccer), which is estimated to cause 0.0001% of US citizens to die each year.","Table 4 shows the descriptive statistics, including means and standard deviations, and the correlations of all variables in Eq. (1). ",21.63%,83.08%,81.13%,36.94%,74.63%
68,153.jpg," This is a confusion matrix. It is used to evaluate the performance of a classification model. The rows represent the true labels of the data points, and the columns represent the predicted labels. The intersection of each row and column represents the number of data points that were correctly or incorrectly classified.

In this example, the model is trying to classify data points into three categories: 'bad', 'good', and 'other/neutral'. The 'bad' category has 100 data points, of which 90 were correctly classified as 'bad' and 10 were incorrectly classified as 'other/neutral'. The 'good' category has 100 data points, of which 95 were correctly classified as 'good' and 5 were incorrectly classified as 'other/neutral'. The 'other/neutral' category has 100 data points, of which 90 were correctly classified as 'other/neutral' and 10 were incorrectly classified as 'bad'.

The overall accuracy of the model is 95%, which means that 95% of the data points were correctly classified.","Table 5 presents the results of cross-sectional regression analysis with CAR over the event window (-1, 0) as the dependent variable. More specifically, model 1 is the basic model and only includes all control variables. In model 2, the direct effect of AI-enabled B2B marketing is introduced. The interactions between AI-enabled B2B marketing and industry dynamism and customer complexity are sequentially included in models 3 and 4. The F-tests (p < 0.05) show that these four models are significant, with adjusted R-squared values between 0.071 and 0.141. To test for multicollinearity, we calculate the full model's variance inflation factor (VIF). The maximum and mean values of VIF are 1.93 and 1.40 (much lower than the threshold of 10), thus suggesting that multicollinearity is not a concern in our models (Kennedy, 1998).",35.89%,64.06%,79.70%,46.67%,82.32%
69,154.jpg," This is a table that shows the number of recommendations that were good, bad, or neutral, as well as the type of content that was recommended. The data was collected from YouTube, search engines, Twitter, Facebook, TikTok, Amazon, and Instagram. The most common type of content that was recommended was misinformation, followed by political radicalization and child harm. The most common type of recommendation that was made was through search results, followed by autoplay and API graphs.","To facilitate our focus group discussion, we meticulously framed our research objectives to provide clear guidance on the purpose of our study, as outlined in Table 6. These predefined objectives served as a framework for our inquiries during the workshop. The discussion revolved around the examination of the three hypotheses central to our research. For Hypothesis 1, participants engaged in conversations addressing questions such as ""Could you share your thoughts on why shareholders generally react positively to AI implementation in B2B marketing?"" and ""What specific benefits or expectations do you associate with AI adoption in B2B marketing?"" Hypothesis 2 prompted discussions with questions such as ""From your perspective, how do industry dynamics and concerns affect your perception of AI adoption in B2B marketing?"" and ""Do you believe that firms in more dynamic industries have unique considerations when it comes to AI implementation? Please elaborate."" Hypothesis 3 guided participants to respond to queries like ""What challenges or complexities do you perceive in firms with more complex customer bases when implementing AI in B2B marketing?"" and ""Could you provide examples of situations where shareholders may have concerns about AI adoption in such firms?"".",13.24%,81.20%,80.38%,10.54%,77.84%
70,155.jpg," The diagram on the left shows a traditional principal-agent relationship, where the principal (e.g., a company) hires an agent (e.g., an employee) to perform a task on their behalf. The agent is expected to act in the best interests of the principal, but there is always the potential for the agent to act in their own self-interest instead.

The diagram on the right shows a trust-based principal-agent relationship, where the principal trusts the agent to act in their best interests, even without the threat of punishment or the promise of reward. This type of relationship is often built on a foundation of mutual trust and respect.

In both diagrams, the arrows between the principal and agent represent the flow of information and the direction of the relationship. The circle in the middle of the diagram on the right represents the self-interest of the agent, which is always present but can be mitigated by the trust between the principal and agent.","However, when firms take actions to recover from a service failure, such as chatbots resolving consumer issues, consumers are likely to focus on efforts made by an able, benevolent, and integral chatbot to resolve the issue. When consumers perceive chatbots to be trustworthy by their benevolence and integrity towards resolving the problem, consumers? intent to penalize the company may decline, resulting in less spread of nWOM. Hence, we hypothesize: H4a Perceived ability of chatbots encourages consumers to forgive firms for service failures. H4b Perceived benevolence of chatbots encourages consumers to forgive firms for service failures. H4c Perceived integrity of chatbots encourages consumers to forgive firms for service failures. H5a: Perceived ability of chatbots encourages consumers to reduce nWOM against firms in case of service failures.H5b: Perceived benevolence of chatbots encourages consumers to reduce nWOM against firms in case of service failures.H5c : Perceived integrity of chatbots encourages consumers to reduce nWOM against firms in case of service failures. In the above five sets of hypotheses, we discussed how chatbots' traits influence their perceived trustworthiness and how this trustworthiness influences consumers' willingness to forgive service providers and spread less nWOM. Corollary, the trustworthiness dimensions mediate chatbot traits and consumer response to service failure relationships. Hence, we hypothesize: H6 : Perceived ability, benevolence, and integrity mediate chatbot traits (i.e., privacy concerns, anthropomorphism, and perceived empathy) and customer outcome (i.e., forgiveness and nWOM) relationship. Fig. 1 presents the conceptual model.",24.40%,86.86%,80.87%,30.13%,87.08%
71,156.jpg," This figure illustrates the relationships between the principal, agent, and blockchain technology. The principal is the entity that hires the agent to perform a task. The agent is the entity that performs the task on behalf of the principal. The blockchain is a distributed ledger technology that can be used to record and track transactions.

The principal and agent have a self-interest in the relationship. The principal wants the agent to perform the task in a way that is beneficial to the principal. The agent wants to perform the task in a way that is beneficial to the agent.

The blockchain can be used to control the relationship between the principal and agent. The blockchain can be used to record and track the tasks that the agent has performed. The blockchain can also be used to track the payments that the principal has made to the agent.

The blockchain can help to ensure that the principal and agent are both acting in a trustworthy manner. The blockchain can help to ensure that the agent is performing the task in a way that is beneficial to the principal. The blockchain can also help to ensure that the principal is paying the agent in a timely manner.","Following a similar strategy as Study 1, in Study 2, we obtained 508 [Females= 260] filled-in questionnaires from UK-based participants [Median Age= 33.78 years; Median Income= £32,820]. In both studies, our final sample was skewed toward younger adults compared to the UK population. Table 1 presents the sample demographics of studies 1 and 2.",18.54%,48.56%,78.95%,35.93%,82.99%
72,157.jpg, 示意一個人使用手機，並因手機而分心，無法專注於手邊的任務。,"In Table 2, we present the descriptive statistics of the variables in the study. There are positive and statistically significant correlations between the antecedents (i.e., perceived privacy concern, anthropomorphic chatbots, and perceived empathy) and the mediators [i.e., perceived ability (rprivacy concerns, perceived ability=-0.21, p < 0.001; ranthropomorphic chatbots, perceived ability =0.25, p < 0.001; rperceived empathy, perceived ability=0.34, p < 0.001), perceived benevolence (rprivacy concerns, perceived benevolence=-0.06, p < 0.10; ranthropomorphic chatbots, perceived benevolence =0.28, p < 0.001; rperceived empathy, perceived beenvolence=0.24, p < 0.001), and perceived integrity (rprivacy concern, perceived intergrity=-0.05, p < 0.10; ranthropomorphic chatbots, perceived integrity =0.33, p < 0.001; rperceived empathy, perceived integrity=0.28, p < 0.001).",0.00%,8.84%,71.86%,0.00%,0.00%
73,158.jpg, Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram.,We tested hypotheses 1?6 using a structural equation model. We employed MPLUS 8.0 to test the structural model. Table 4 presents the results of the hypothesis tests.,0.00%,65.52%,81.79%,0.00%,45.08%
74,159.jpg," * **Antecedents**: 
 * Environmental Stimuli: e.g., device proximity (Dwyer et al., 2018) and notification settings (Wasmuth et al., 2022)
 * Psychological States: e.g., boredom (Lee et al., 2020) and online vigilance (Throuvala et al., 2020)
* **Contingency Factors**: 
 * Socio-Cultural Influences: e.g., group norms (e.g., Deng et al., 2020) and restriction policies (e.g., Cutino and Nees, 2017)
 * Individual Characteristics: e.g., personality (e.g., Toyama and Hayashi, 2021b) and age (e.g., Grewal et al., 2018)
* **Outcomes**: 
 * Psychological Consequences: e.g., anxiety (e.g., Cutino and Nees, 2017) and task performance (e.g., Lee et al., 2020)
 * Behavioral Consequences: e.g., increased time in-store (e.g., Grewal et al., 2018) and increased unplanned purchases (e.g., Sciandra et al., 2019)","We predicted through the sixth hypothesis that perceived ability, perceived benevolence, and perceived integrity act as mediators between the antecedents (perceived privacy concerns, anthropomorphism, and perceived empathy) and customer outcome (i.e., consumer forgiveness and nWOM). To test the mediation models, we employed Hayes? (2018) procedure and further employed a bootstrapping re-sample value of 5000. In Table 5, we present the results of the mediation analyses.",13.31%,73.83%,79.19%,36.23%,77.56%
75,16.jpg," The figure shows the software architecture of MOOSE. The framework consists of three main components: physics modules, machine learning modules, and a tensor library. The physics modules provide the ability to simulate physical systems, while the machine learning modules provide the ability to train and deploy machine learning models. The tensor library provides the necessary data structures and operations for both physics and machine learning computations.

The MOOSE framework is designed to be modular and extensible, allowing users to easily add new physics modules and machine learning models. The framework is also designed to be efficient, making it possible to simulate large-scale systems on high-performance computing platforms.

The MOOSE framework has been used to develop a number of applications, including:

* **BISON:** A multi-physics simulation code for nuclear reactor analysis
* **Griffin:** A fluid-structure interaction code for simulating the behavior of nuclear fuel rods
* **Marmot:** A thermal-hydraulics code for simulating the behavior of nuclear reactor coolant systems
* **Pronghorn:** A neutron transport code for simulating the behavior of nuclear reactor cores
* **Grizzly:** A multi-physics simulation code for simulating the behavior of nuclear waste repositories

The MOOSE framework is a powerful tool for simulating the behavior of complex physical systems. The framework is open-source and available for download from the MOOSE website.","Next, from Table 6, we can observe that the correlations of the antecedents and the mediators are positive and statistically significant, and the correlations of the mediators and the two outcome variables: consumer forgiveness and nWOM, are in the expected directions. These encouraging findings provide preliminary evidence in support of the study hypotheses.",41.31%,76.22%,80.44%,38.97%,79.98%
76,160.jpg," The table shows the research methods used in 28 papers. The most common research method was the survey, which was used in 9 papers. The second most common research method was the experiment, which was used in 9 papers. The third most common research method was the field experiment, which was used in 4 papers. The fourth most common research method was the observation, which was used in 2 papers. The fifth most common research methods were the eye tracking experiment, the interview, and the experience sampling, which were each used in 1 paper. The least common research method was the phone tracking, which was used in 1 paper.","Next, we tested the measurement model using MPLUS 8.0. The Study 2 measurement model reported a good fit (Chi-square/df= 2.83; RMSEA= 0.042; CFI= 0.955; TLI= 0.963). We also assessed the constructs? convergent and discriminant validities (Fornell & Larcker, 1981) that we report in Table 7.",17.19%,67.26%,80.24%,37.00%,77.01%
77,161.jpg," The structural model of the relationships among perceived gamification, negative affect, ease of imagination, existential authenticity, lack of focused attention, and behavioral intention (n = 284). *p < 0.05.","Finally, to test hypothesis six, we employed a strategy similar to Study 1, i.e., Hayes?s (2018) mediation procedure with a bootstrapping resample value 5000. We present the results of the mediation analyses in Table 8. The estimated path coefficient for the indirect effect of perceived privacy concerns on consumer forgiveness through perceived ability (Column 1 of Table 8) was statistically significant (? = -0.0089; LCI=-0.0122; UCI=-0.0056). Also, the estimated path coefficient for the indirect effect of perceived privacy concerns on nWOM through perceived ability (Column 2 of Table 8) was statistically significant (? = 0.0092; LCI=0.0044; UCI=0.0140). From Column 3 of Table 8, we observe that the estimated path coefficients for the indirect effects of chatbot anthropomorphism through perceived ability (? = 0.0125; LCI=0.0062; UCI=0.0188), perceived benevolence (? = 0.0208; LCI=0.0124; UCI=0.0292), and perceived integrity (? = 0.0298; LCI=0.0204; UCI=0.0392) on consumer forgiveness were statistically significant. Also, in Column 4 of Table 8, we observe that the estimated path coefficients for the indirect effects of anthropomorphism through perceived ability (? = -0.0127; LCI=-0.0178; UCI=-0.0076), perceived benevolence (? = -0.0175; LCI=-0.0246; UCI=-0.0104), and perceived integrity (? = -0.0352; LCI=-0.0481; UCI=-0.0223) on nWOM were statistically significant. Finally, in Column 5 of Table 8, we observe that the estimated path coefficients for the indirect effects of perceived empathy through perceived ability (? = 0.0193; LCI=0.0107; UCI=0.0279), perceived benevolence (? = 0.0261; LCI=0.0169; UCI=0.0353), and perceived integrity (? = 0.0292; LCI=0.0190; UCI=0.0394) on consumer forgiveness were statistically significant. Similarly, in Column 6 of Table 8, we observe that the estimated path coefficients for the indirect effects of perceived empathy through perceived ability (? = -0.0175; LCI=-0.0269; UCI=-0.0081), perceived benevolence (? = -0.0220; LCI=-0.0329; UCI=-0.0111), and perceived integrity (? = -0.0352; LCI=-0.0494; UCI=-0.0210) on nWOM were statistically significant.",23.29%,85.39%,77.09%,0.01%,74.99%
78,162.jpg," This is a table that shows the focus group, age, profession, and gender of the 20 participants. The participants are all between the ages of 18 and 61, and they are all students, except for one participant who is a consultant and one participant who is an assistant professor. There are 10 female participants and 10 male participants.","To detect a CV?s presence in SEM research, researchers should seriously consider explaining why a new CV should have a relationship with the dependent variables and/or a correlation with the independent variables. Researchers should consider using our proposed procedures under two conditions: with and without theoretical support. Fig. 1 illustrates the proposed process for identifying a new CV without theoretical support. Researchers should consider testing the measurement model. If the correlations between a new CV and the other variables are high, the researchers have not been able to separate out the CV?s effects on the other variables; thus, a CV has not been identified. If the correlations between a new CV and the other variables are low, researchers need to statistically compare structural models with CVs and those without CVs. If the path coefficient and R-squared value show significant differences between such models, then a new CV has not been identified. However, if the path coefficient and R-squared value do not indicate significant differences between such models, a CV has been identified.",22.03%,71.57%,80.83%,6.92%,75.99%
79,163.jpg, AMOS output for the structural model,"Fig. 2 illustrates the proposed process for identifying a new CV when there is theoretical support for it. When there is a theory that suggests including the CVs, they should be included in the initial research model. Theoretical support ensures that CVs will be in the measurement model. If the correlations between a new CV and the other variables are high, researchers should correlate the new CV with the independent variables and conduct an SEM analysis with and without the new CV. Otherwise, it is unnecessary to add this correlation, but the SEM analysis should still be performed. If the path coefficient and R-squared value with the new CV differ significantly from those without it, researchers should present the results without the new CV and provide post hoc alternative explanations (i.e., modify the supporting theoretical reasons). If the path coefficient and R-squared value with the new CV do not differ significantly from those without it, researchers should present the results with the new CV.",22.42%,78.28%,81.13%,0.00%,83.61%
80,164.jpg," The PRISMA flow diagram details the process of study selection. A total of 920 records were identified through a comprehensive search of electronic databases and other sources. After removing duplicates, 563 records were screened based on title and abstract, and 117 full-text articles were assessed for eligibility. Of these, 49 full-text articles were excluded for various reasons, and 68 studies were included in the final qualitative synthesis.","Data analysis followed Lee and Baskerville?s (2003) generalization framework, with iterations between four types of generalization between empirical and theoretical statements: generalizing from empirical data to empirical descriptions (EE), generalizing from empirical descriptions to theoretical statements (ET), generalizing from theory to descriptions of other settings (TE), and generalizing from concepts to theory (TT). While data analysis activities were highly iterative and intertwined with data collection activities (Klein and Myers, 1999, Walsham, 2006), our general approach was to progress from rich empirical descriptions to theoretical insights. In the first analysis step (EE), we coded the transcripts in MAXQDA. Codes ? abstract descriptions of intersubjective realities ? can originate from theory and data (Walsham, 2006). Drawing on Gioia et al.?s (2013) framework, we started with data-driven open coding to generate tentative empirical themes from the interview transcripts at the level of phrases or paragraphs. In a second step (ET), we compared the emerging empirical themes with existing theories to integrate them into higher-level conceptual categories via axial coding. Here, we conceptualized relationships between emergent categories by focusing on how categories are related and how they influence each other (Gioia et al., 2013). In a third step (TE), we further refined and linked these conceptual categories by revisiting the data, resulting in many iterations of a conceptual model with four aggregate dimensions and two manifestations each: people (innovator or facilitator), structure (informal or formal), task (idea generation or idea evaluation), and technology (representation or collaboration). Fig. 1 illustrates our coding process and further quotations illustrating the codes are used throughout Section 4.",20.08%,80.95%,80.05%,2.75%,78.85%
81,165.jpg," The table shows the codes, factors, and contexts investigated in the study. The codes are used to identify the specific factors that are being investigated. The factors are the different aspects of the technology, organization, and environment that are being studied. The contexts are the different areas in which the factors are being investigated.

The technology factors are continuous learning process, interoperable datasets, process reengineering, pilot project, new tasks for machine training, human-machine relationship, and intrinsic motivation.

The organizational factors are top management support, bottom-up commitment, regulatory frameworks, inter-organizational data collection and sharing, and citizens' trust in the system.

The environmental factors are relation with the supplier(s) and sharing experience.","Our empirical analysis reveals that creativity emerged when WealthTech?s ISD team members empathetically interacted, aligning their distinctive creative styles and contributions within formal and informal structural settings, while engaging in idea generation and evaluation using technologies for representation and collaboration. This synthesis of insights enriches the STS perspective on creativity emergence in ISD teams, as visually illustrated by the conceptual model in Fig. 2 and elaborated in detail below the model. The conceptual model and the detailed findings from WealthTech afford opportunities to advance theory on the phenomenon of interest: the emergence of creativity in ISD practices. In doing that, we follow Whetten's (1989) guidelines to explicate what concepts and relationships we consider to understand and explain the phenomenon of interest, how we articulate the relationships between the concepts as propositions, and why we choose to emphasize specific relationships and articulations.",38.23%,94.19%,81.91%,36.12%,81.94%
82,166.jpg," The table shows the codes developed in the study. These codes emerged from the grounded theory approach and were used to analyze the data collected from interviews with experts in the field of artificial intelligence (AI). The codes are organized according to the three contexts investigated in the study: technology, organization, and environment.

Under the technology context, the codes relate to the continuous learning process, interoperable datasets, new tasks for machine training, and the up-skilling of human agents. These codes capture the technological aspects of AI implementation, such as the need for AI systems to continuously learn and improve, the importance of interoperable datasets for effective AI training, the emergence of new tasks that require AI-powered automation, and the need for human agents to be up-skilled to work effectively with AI systems.

Under the organization context, the codes relate to human-machine relationships, regulatory frameworks, inter-organizational data collection and sharing, and citizens' trust in the system. These codes capture the organizational and social aspects of AI implementation, such as the need for effective human-machine collaboration, the importance of regulatory frameworks to govern AI development and deployment, the challenges of inter-organizational data collection and sharing, and the need to build citizens' trust in AI systems.

Under the environment context, the codes relate to the trustworthiness of AI systems and PSOs' activities, legal and political barriers, administrative silos, data-driven services, citizens' lack of awareness and fear of AI systems, increasing technology dependency, and knowledge diffusion. These codes capture the broader environmental factors that influence AI implementation, such as the need for trustworthy AI systems and PSOs, the legal and political barriers to AI adoption, the challenges of administrative silos, the increasing demand for data-driven services, the need to address citizens' lack of awareness and fear of AI systems, and the importance of technology diffusion to promote AI adoption.

Overall, the codes developed in the study provide a comprehensive framework for understanding the factors influencing the implementation of AI in the public sector. These codes can be used to guide future research and inform policy decisions related to AI implementation.","We studied in detail the creative practices of ISD teams at the WealthTech headquarters in Switzerland and various subsidiaries worldwide. The style of involvement was that of an embedded researcher having in-depth access to the research site (Walsham, 2006), including meeting rooms, in-house workstations, and the intranet. For nearly three years, from February 2013 to December 2015, the lead author spent several days weekly onsite in the Swiss offices, having access to an in-house workstation and intranet. There, the author conducted 32 interviews to get an in-depth understanding of creativity in ISD teams from a participant?s perspective (Walsham, 2006). In addition, the author attended meetings to do participant observations, in which a researcher participates actively in discussions, as opposed to acting as a passive outside observer (Walsham, 2006). In April 2014, the author also visited the UK offices to conduct 30 interviews and observe how ISD teams collaborated virtually. Table 1 provides an overview.",45.43%,81.78%,80.67%,41.43%,80.29%
83,167.jpg," **Case-specific factors**

Technological:
- Difficulties in implementing an explainable AI system
- Scarce availability of infrastructure
- Difficulties in having high quality interoperable data
- Difficulties in collecting new data

Organizational:
- Reluctance to change
- Lack of human resources
- Employees' mistrust of AI

Environmental:
- Citizens' mistrust of AI
- Complexity and dynamism of the regulatory framework
- Structural administrative silos

**AI-related factors**

Technological:
- Accurate and faster data analysis
- Creating a data-driven organization

Organizational:
- Re-qualification of employees
- Automation of simple cognitive tasks
- Augmenting the decision-making process

Environmental:
- Enhancing the provision of data driven services
- Ensuring adequate skills and technology advanced systems
- Ensuring trustworthiness and compliance with human rights guaranteed by norms
- Enhancing collaboration among PSOs

**Common factors**

- Requires AI-related factors
- Afford","We conducted interviews at WealthTech with 62 different ISD team members, ranging from 19 to 100 min. We took care to select a broad sample of participants to capture diverse perspectives, relying on a combination of theoretical sampling (seeking out different ISD team members), purposive sampling (seeking out diverse views), and snowballing (following referrals from other participants). We audio recorded and transcribed all 62 interviews except two, where the participants did not give permission to record, in which case we took notes. Of the 62 interviews, 39 were in German, in which case we translated the quotations into English. The remaining 23 interviews were in English. Table 2 provides an overview.",15.61%,63.70%,78.25%,40.30%,67.53%
84,168.jpg," The figure shows the structural model of the impact of mobile social media health information quality on WeChat dissemination. The five dimensions of mobile social media health information quality are: interactivity of mobile social media, reliability of mobile social media, content credibility of health information, content value of health information, and timeliness of health information. The WeChat dissemination index is the dependent variable.",The hypotheses are visualized in the research model in Fig. 1.,14.57%,67.24%,84.58%,36.51%,55.86%
85,169.jpg, The process of screening and selecting evaluation indicators for the study.,"For the scenario-based experiment (Study 2), we selected and adapted four conversations with KIM representing a 2 × 2 factorial design: two conversations were completed by KIM, while this was not the case for the other two. In the unsuccessful conversations, the chatbot responded twice with textual elements to restart the conversation. In addition, two conversations were short in terms of the length of the conversation itself and KIM?s responses presenting 2 correct recipes, while the other two conversations were long with a lengthy introduction by KIM, and a detailed textual presentation displaying 5?6 recipes (see Figure A1 in the appendix). The experiment was supported by a German market research institute and took place with 627 participants in February 2022. We asked the respondents to read one sample conversation with KIM thoroughly and to evaluate KIM in terms of the conversation ability score and the three item scales (perceived naturalness of KIM, performance of the chat, satisfaction with the interaction). To check manipulation, they were asked to indicate whether KIM made good recipe suggestions and answered straight to the point ? that is, short and concise. The participants were randomly assigned to one of the four scenarios (see Table 3).",36.45%,82.39%,82.92%,0.00%,86.80%
86,17.jpg," The diagram shows the process of building a decision tree model for a given dataset. The process starts with data preparation, where the data is cleaned and preprocessed. The data is then split into training and testing sets. The training set is used to build the decision tree model, while the testing set is used to evaluate the performance of the model.

The decision tree model is built using a machine learning algorithm. The algorithm takes the training data as input and builds a tree-like structure that represents the decision-making process. The tree is built by recursively splitting the data into smaller and smaller subsets until each subset contains only one type of data point.

Once the decision tree model is built, it can be used to make predictions on new data. The model takes a new data point as input and traverses the tree to find the leaf node that corresponds to the data point. The leaf node contains the predicted value for the data point.

Decision trees are a powerful machine learning algorithm that can be used to solve a variety of problems. They are relatively easy to understand and interpret, and they can be used to make accurate predictions. However, decision trees can also be complex and difficult to tune. It is important to carefully consider the data and the desired outcome before using a decision tree model.","Both samples consist of more females than males. In the usability study (Study 1), the percentage of females was higher (60.2%) than males (39.0%, 0.8% diverse). In the experiment (Study 2), the gender distribution was more balanced with 51.0% females and 47.7% males (1.3% diverse). The respondents are young, most often 20?25 years (Study 1: 67.5%, Study 2: 62.3%) or 26?30 (Study 1: 16.3%, Study 2: 17.3%) old. The share of respondents under 20 was more than twice the size in Study 2 (14.7% vs. 5.7%) reflecting the aim to include a younger age group. Over time, the claim of familiarity with the term chatbot increased from about two-thirds to three-quarters of the respondents, more often males. In both samples, less than half of the respondents already had experience of chatbot usage. The percentage was higher compared to an earlier study on the acceptance of chatbots (Rese et al., 2020). About two-thirds of the sample in Study 1 use Facebook Messenger, and about half of the sample Study 2. However, the chatbot, KIM, is largely unfamiliar to respondents, in particular in Study 1. With regard to diet preferences, more than half of the respondents follow no special regimen, whilst about a quarter to a third are vegetarians, among them significantly greater numbers of females. Female respondents search for recipes more frequently on the internet in both samples. In general, the time spent searching for a recipe was short. In Study 1, half of the respondents take up to 5 min and another third up to 10 min, whilst in Study 2 about a quarter need longer with up to 20 min and more (see Table 4). While Study 1 relied on a student sample, participants of Study 2 included students (28.2%), employees (43.5%), self-employed (5.9%), pupils (9.3%), and trainees (8.9%).",34.50%,47.04%,78.87%,35.86%,84.54%
87,170.jpg," The evaluation index system of mobile social media health information quality is constructed, as shown in Figure 1. The evaluation index system includes two一级指标, six二级指标, and 16三级指标. The一级指标 are user perception and objective data. The二级指标 are user metrics importance score, user perception weights, entropy method, objective weighting of empirical data, combined weighting based on entropy value correction G1, and mobile social media health information quality. The三级指标 are browsing behavior, interaction behavior, reading behavior, information credibility, information timeliness, information diversity, information aesthetics, information interaction, information relevance, information accuracy, information completeness, information organization, information structure, information integrity, and information security.","With regard to the dialogues, there were no significant differences between female and male participants. Most participants completed the recipe search, corresponding more or less to their food preferences. Often KIM asked two questions about recipe criteria (48.0%), including the ingredients. In 3.3% of the dialogues, KIM referred to all three criteria, whilst not mentioning them explicitly in 21.1% of the conversations. On average, KIM suggested 6 recipes to the user, of which a small number were inappropriate (18.2%). The search was quick, with KIM posting more messages and using more words. KIM?s messages usually included one emoji per dialogue. KIM had to restart the conversation once in almost every dialogue. This had a significantly greater frequency in dialogues with female users, who also received a lower percentage of correct recipes. In about a quarter of the cases, KIM failed to employ a correct greeting, either repeating the same message or omitting the greeting. In the evaluation by external reviewers, KIM received a mean conversational ability score above 0 = poor, machine-like, but still under 50. This score was termed as ?good conversationalist? and was achieved by 2 of the 6 chatbots evaluated (Shah et al., 2016). The users rated KIM slightly better on satisfaction, naturalness, and performance expectancy with a score above the average value of 4 (see Table 5; for a description of the variables, see Table A1 in the appendix).",10.08%,87.29%,78.12%,10.64%,72.82%
88,171.jpg, The figure shows the hierarchical structure of mobile social media health information quality evaluation index system.,"For hypotheses on task fulfilment, we relied on a comparison of users successfully and unsuccessfully completing the search for recipes with KIM in Study 1. We compared the mean values of task and conversation elements as well as evaluation criteria for the two groups. Since the group failing to complete the task was small, a non-parametric test ? the Mann-Whitney U Test ? was used (see Table 6).",19.71%,80.09%,83.10%,1.78%,74.31%
89,172.jpg," Rotated Component Matrixa

Extraction Method: Principal Component Analysis.

Rotation Method: Varimax with Kaiser Normalization.

a. 4 components extracted.","With regard to the perceived conversational ability of KIM, we transformed the independent variables with a box-cox transformation to compensate for the non-normal distribution of the data. We employed stepwise regression analysis to integrate into the model not only the independent variables concerning the hypotheses but also all available independent variables from Table A1. We chose this more exploratory approach because of the scarcity of research results on the human?chatbot conversation. We used the four internal and external dependent evaluation variables. Similar to the results from the mean comparison, the external evaluation with the conversational ability score and, to some extent, user satisfaction were shown to be suitable and provided higher R2. Taking the external evaluation ? the conversational ability score ? into account, H1b and H7b were confirmed, and a positive effect of a high percentage of correct recipe suggestions (0.212, p = 0.003) and a negative effect of the number of conversation restarts (-0.506, p = 0.000) were established. For user satisfaction, the positive effect of a high percentage of correct recipe suggestions (0.174, p = 0.043) was also found. Other hypotheses on the perceived conversational ability of KIM could not be proven. The internal evaluation criteria established negative effects for the number of messages, the number of words from the user, the number of greetings by KIM, and the number of wrong recipe suggestions. While the presence of a correct greeting did not show the hypothesized positive effect (H8b), additional greetings had a negative effect (Table 7).",1.21%,77.73%,79.13%,0.00%,78.11%
90,173.jpg," All the items show high reliability and validity. All the items have a CR value greater than 0.708, an AVE value greater than 0.5, and a loading value greater than 0.707.","This research has tried to identify a mix of instrumental (task-oriented) and social (small-talk) conversational elements that influence users? perception of a chatbot and their conversation with it. To investigate conversational ability, we used the task-based and text-based chatbot, KIM, by MAGGI Kochstudio to perform the task of finding an accurate recipe and to undertake an analysis of 123 unstructured chat records (Study 1). An overview of previous studies analysing text-based chatbot conversations supported the identification of relevant features and evaluation variables from usability research (Frøkjær et al., 2000). We focused on several task-based and text-based variables as well as two characteristics of conversational ability that potentially influence the success of KIM in this area. In addition, we relied on an external evaluation by three reviewers based on the conversational ability score (Shah et al., 2016) and a subjective user evaluation following the conversation with KIM using item scales describing user satisfaction (Hornbæk, 2006, Shawar and Atwel, 2007, Söderlund, 2022, Söderlund and Oikarinen, 2021). A scenario-based experiment (Study 2) was used to gain further insights into user evaluation of selected conversational elements (see Table 8). The usability study (Study 1) provided insights into several task-based and text features as well as characteristics of conversational ability, and it was used as a source for example dialogues. While chatbot usage could be monitored, the implementation period was rather long with data collected in person. The scenario-based experiment (Study 2) was quickly implemented but was restricted to a few features and did not include real experience with KIM.",11.69%,71.63%,79.86%,0.04%,80.89%
91,174.jpg," Section 2: Primary Indicators and Ratings
8. Secondary Indicators and Ratings
9. Dimensions
10. Reliability Analysis
11. Content Validity
12. Health Communication
13. V Health
14. C Health
15. R Health
16. I Health
17. Total Score",H1a.Involvement relates positively to the consumption level of tourist engagement in SMNs. H1b.Involvement relates positively to the contribution level of tourist engagement in SMNs. H1c.Involvement relates positively to the creation level of tourist engagement in SMNs. H2a Perceived anonymity relates positively to the consumption level of tourist engagement in SMNs.H2b Perceived anonymity relates positively to the contribution level of tourist engagement in SMNs. H2c Perceived anonymity relates positively to the creation level of tourist engagement in SMNs. H3a Consumption engagement in SMNs relates positively to cold BRQ. H3b Consumption engagement in SMNs relates positively to hot BRQ. H6a Cold BRQ on SMNs relates positively to trip decision-making. H6b Hot BRQ on SMNs relates positively to trip decision-making.H6a Cold BRQ on SMNs relates positively to trip decision-making. H6b Hot BRQ on SMNs relates positively to trip decision-making H4a Contribution engagement in SMNs relates positively to cold BRQ. H4b Contribution engagement in SMNs relates positively to hot BRQ. H5a :Creation engagement in SMNs relates positively to cold BRQ. H5b Creation engagement in SMNs relates positively to hot BRQ. H7a Cold BRQ on SMNs relates positively to cross-buying decisions.H7b Hot BRQ on SMNs relates positively to cross-buying decisions. Fig. 1 shows the theoretical model.,0.00%,49.85%,77.71%,0.00%,68.51%
92,175.jpg," The Unified Theory of Acceptance and Use of Technology (UTAUT2) is a model that explains how users' perceptions of a technology affect their intention to use it. The model was developed by Venkatesh, Thong, and Xu in 2012. It is an extension of the original UTAUT model, which was developed in 2003.

UTAUT2 includes seven constructs:
- **Performance expectancy:** The degree to which a user believes that using a technology will help them achieve their goals.
- **Effort expectancy:** The degree to which a user believes that using a technology will be easy.
- **Social influence:** The degree to which a user believes that others think they should use a technology.
- **Facilitating conditions:** The degree to which a user believes that the organization and technical infrastructure are in place to support the use of a technology.
- **Hedonic motivation:** The degree to which a user believes that using a technology will be enjoyable.
- **Price value:** The degree to which a user believes that the benefits of using a technology outweigh its costs.
- **Habit:** The degree to which a user is accustomed to using a technology.

UTAUT2 also includes four moderators:
- **Gender:** The user's gender.
- **Age:** The user's age.
- **Experience:** The user's experience with using technology.
- **Voluntariness:** The degree to which the user is required to use the technology.

UTAUT2 is a useful model for understanding how users' perceptions of a technology affect their intention to use it. The model can be used to identify the factors that are most likely to influence a user's decision to use a technology. This information can be used to design more effective marketing and training campaigns for new technologies.","The findings further demonstrated that the effect of involvement on creation (|?ß| = 0.146, p < 0.05), the effect of creation on hot BRQ (|?ß| = 0.203, p < 0.05), and the effect of cold BRQ on cross-buying decisions (|?ß| = 0.154, p < 0.05), were higher for group 2. The effect of hot BRQ on trip decision-making (|?ß| = 0.134, p < 0.05) was higher for group 1. Interestingly, no differences were found pertaining to the associations between involvement and consumption, brand involvement and contribution, consumption and cold BRQ, consumption and hot BRQ, contribution and cold BRQ, contribution and hot BRQ, creation and cold BRQ, cold BRQ and trip decision-making, and hot BRQ and cross-buying. Fig. 2, Fig. 3 present the findings of the structural models of groups 1 and 2.",20.26%,63.15%,77.83%,38.77%,77.22%
93,176.jpg," The image shows a neural network with one input layer, one output layer and one hidden layer. The input layer has 8 neurons, the hidden layer has 10 neurons and the output layer has 1 neuron.","The findings further demonstrated that the effect of involvement on creation (|?ß| = 0.146, p < 0.05), the effect of creation on hot BRQ (|?ß| = 0.203, p < 0.05), and the effect of cold BRQ on cross-buying decisions (|?ß| = 0.154, p < 0.05), were higher for group 2. The effect of hot BRQ on trip decision-making (|?ß| = 0.134, p < 0.05) was higher for group 1. Interestingly, no differences were found pertaining to the associations between involvement and consumption, brand involvement and contribution, consumption and cold BRQ, consumption and hot BRQ, contribution and cold BRQ, contribution and hot BRQ, creation and cold BRQ, cold BRQ and trip decision-making, and hot BRQ and cross-buying. Fig. 2, Fig. 3 present the findings of the structural models of groups 1 and 2.",14.46%,69.45%,79.45%,3.22%,80.12%
94,177.jpg," The table shows the distribution of the sample in terms of gender, age, education, employment status, and net monthly household income.

The sample is fairly evenly split in terms of gender, with 48.5% male and 51.5% female. The majority of the sample is aged between 25 and 44 (32.2%), followed by those aged between 45 and 64 (19.2%). The majority of the sample has a secondary education (47.3%), followed by those with a higher education (46.5%). The majority of the sample is employed (63.8%), followed by those who are actively seeking work (14.9%). The majority of the sample has a net monthly household income of less than 1100 Euros (25.1%).","In the U.S sample, 50.2% of the respondents were female, 43.4% in the age group of 26?41 years, 40.9% had earned a four-year degree, and 84.3% indicated that the trip was domestic. In the Chinese sample, 56.1% were female, 36.9% in the age group of 26?41, 65.6% had earned a four-year degree, and 97.3% indicated that the trip was domestic. Table 1 shows the demographic profiles of participants in both countries.",43.55%,91.26%,85.08%,43.64%,78.96%
95,178.jpg," The table shows the normality test results of the data. The first column is the item number, the second column is the sample size, the third column is the mean, the fourth column is the standard deviation, the fifth column is the Kolmogorov-Smirnov Z value, and the sixth column is the significance value. The significance value is the p-value of the Kolmogorov-Smirnov test. A significance value less than 0.05 indicates that the data is not normally distributed.

From the table, we can see that all of the p-values are less than 0.05, which indicates that the data is not normally distributed.","Establishing validity and reliability of the measures is essential prior to testing the structural model (Hair Jr et al., 2017; Henseler et al., 2009). We assessed the measurement models for the pooled data and then group 1 (U.S.) and group 2 (China). As displayed in Table 3, constructs were internally consistent since Cronbach?s alpha values were > the 0.70 threshold (Nunnally & Bernstein, 1994). The convergent validity criterion was verified, with average variance extracted (AVE) values all above 0.50 (Fornell & Bookstein, 1982).",38.29%,63.46%,80.79%,40.59%,82.17%
96,179.jpg," The table shows the results of a multiple regression analysis. The dependent variable is Use Intention and the independent variables are Performance Expectancy, Effort Expectancy, Subjective Norms, Facilitating Conditions, Hedonic Motivation, Price Value, Habit and Perceived Risk. The results show that all of the independent variables are significant predictors of Use Intention.","As shown in Table 4, discriminant validity was assessed by heterotrait-monotrait criterion (HTMT). The results indicated good discriminant validity as all HTMT values were below the threshold value of 0.90 (Voorhees et al., 2016). A multicollinearity test was employed via the value inflation factor (VIF). VIF values were less than the 5.0 threshold. Hence, multicollinearity was not an issue. The Harman's single-factor test was utilized to control the threat of common method variance. The results of the exploratory factor analysis (unrotated) showed that no single construct explained more than 44.9% of the observed variance. Common method variance did not seem a concern in this study since it was below 50?60% (Fuller et al., 2016).",29.74%,79.93%,81.38%,15.57%,79.12%
97,18.jpg, Two-stage optimization framework for the urban road network,"In the second stage, the structural models for group 1 and group 2 were gauged using SmartPLS 3 (Ringle et al., 2015). To assess the structural model, the R² of the endogenous variables was computed for the model?s explanatory power (Hair Jr et al., 2017). As reported in Table 5, consumption achieved R² value of 38.6% (pooled data), 32% (group 1), and 36.2% (group 2); contribution 44.3% (pooled), 38.5% (group 1), and 42.5% (group 2); creation 40.7% (pooled), 29.3% (group 1), and 43.8% (group 2); cold BRQ 27.7% (pooled), 17.9% (group 1), and 49.8% (group 2); hot BRQ 51.1% (pooled), 39.4% (group 1), and 49% (group 2); trip decision-making 23.9% (pooled), 20.2% (group 1), and 13.9% (group 2); and cross-buying 48.9% (pooled), 37.2% (group 1), and 54.9% (group 2). To assess the strength of the hypothesized relations, a bootstrapping test based on 5000 subsamples was performed. As hypothesized, brand involvement had a positive effect on ?consumption,? ?contribution? and ?creation? (H1a, ß = 0.612, p < 0.05; H1b, ß = 0.645, p < 0.05; H1c, ß = 0.603, p < 0.05, respectively). Accordingly, H1a, H1b, and H1c were verified. Additionally, perceived anonymity had a positive effect on ?consumption,? ?contribution? and ?creation? (H2a, ß = 0.038, p < 0.05; H2b, ß = 0.081, p < 0.05; H2c, ß = 0.119, p < 0.05, respectively). Hence, H2a, H2b, and H2c were confirmed. As hypothesized, consumption had a positive effect on both (H3a) cold and (H3b) hot BRQ (ß = 0.272, p < 0.05; ß = 0.221, p < 0.05, respectively). Contribution had a positive effect on both (H4a) cold and (H4b) hot BRQ (ß = 0.129, p < 0.05; ß = 0.281, p < 0.05, respectively). Additionally, creation had a positive effect on both (H5a) cold and (H5b) hot BRQ (ß = 0.170, p < 0.05; ß = 0.269, p < 0.05, respectively). Therefore, H3a, H3b, H4a, H4b, H5a, and H5b were verified. Cold BRQ had a positive effect on trip-decision making (H6a) and cross-buying (H6b) (ß = 0.057, p < 0.05; ß = 0.411, p < 0.05, respectively). Finally, hot BRQ had a significant positive effect on trip-decision making (H7a) and cross-buying (H7b) (ß = 0.448, p < 0.05; ß = 0.350, p < 0.05, respectively). Thus, H6a, H6b, H7a, and H7b were confirmed.",4.93%,40.18%,78.07%,0.00%,77.44%
98,180.jpg," The table shows the results of a multiple regression analysis. The dependent variable is Use Intention (UI). The independent variables are Performance Expectancy (PE), Effort Expectancy (EE), Subjective Norms (SN), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HAB), and Perceived Risk (PRISK).

The results show that all of the independent variables are significant predictors of Use Intention. The strongest predictors are Performance Expectancy, Effort Expectancy, and Subjective Norms. These variables are all positively related to Use Intention, meaning that as they increase, Use Intention also increases. The other significant predictors are Facilitating Conditions, Hedonic Motivation, and Price Value. These variables are also positively related to Use Intention. However, Habit and Perceived Risk are negatively related to Use Intention, meaning that as they increase, Use Intention decreases.

The table also shows the collinearity statistics for the independent variables. The tolerance values are all above 0.10, and the VIF values are all below 10.00, indicating that there is no significant collinearity between the independent variables.","A multigroup analysis (MGA) was performed to compare the differences between the model for group 1 with the model for group 2. Differences in the path coefficients between the two data sets are shown in Table 6. In the first step, we tested the measurement invariance of composite models (MICOM) (Henseler et al., 2016). Thus, we followed the three-step process. These steps were computing the ?configural invariance,? ?compositional invariance? and ?the equality of means and variances.? . As displayed in Table 5, the full measurement invariance was partially achieved in the comparison between group 1 and group 2 as step (1) was fully established, and step (2) and step (3) were partially established. The following step was related to the employment of a multigroup analysis to test the path coefficients in both groups. Parametric and nonparametric procedures (PLS-MGA and permutation) were applied to investigate group differences (Hair et al., 2018). As shown in Table 5, the findings indicated that significant differences existed in path coefficients between group 1 and group 2. In the relationship between perceived anonymity and ?consumption,? ?contribution? and ?creation? the effects were stronger for group 1 (|?ß| = 0.113, p < 0.05, |?ß| = 0.106, p < 0.05, |?ß| = 0.088, p < 0.05, respectively).",33.06%,91.35%,80.24%,34.30%,80.45%
99,181.jpg," Table 1. Measurement Model Results

Note: CA = Cronbach's alpha; AVE = average variance extracted; CR = composite reliability; PRI = perceived risk intention; HAB = habit; FCI = facilitating conditions; SN = subjective norm; EE = effort expectancy; PE = performance expectancy; LBRO = likelihood of behavior intention; VAL = valence.","H1: The accuracy in predicting the communication effectiveness of FtF is higher than the accuracy in predicting the effectiveness of email. H1a : The difference in the accuracy of effectiveness predictions between FtF and email is moderated by the closeness between the requester and the target individual. In Study 1, we address RQA and test the related Hypotheses 1 and 1a as seen in the conceptual model displayed in Fig. 1. In this study we try to replicate previous research and reestablish that FtF is more effective than email for approaching potential resource providers. ",1.77%,63.27%,78.77%,16.01%,74.75%
100,182.jpg, **. Correlation Matrix.**," In Study 2, we build on the findings of Study 1 and empirically investigate the effect of perceived effectiveness and other (irrational) factors on media choice (RQM and Hypotheses 2, 3, and 4) as seen in the conceptual model displayed in Fig. 3.  H2 : Perceived differences in the effectiveness of the medium (FtF vs. email) has an impact on the media selection decision. H2a The impact stated in H2 is higher for strangers than for friends. H3 : Perceived differences in the awkwardness and embarrassment of the medium (FtF vs. email) has an impact on the media selection decision. H3a The impact stated in H3 is higher for strangers than for friends. H4 : Perceived differences in the convenience of the medium (FtF vs. email) has an impact on the media selection decision. H4a The impact stated in H4 is higher for strangers than for friends.",0.00%,31.74%,81.75%,0.00%,60.61%
101,183.jpg," The table shows the results of a multiple regression analysis. The dependent variable is Use Intention (UI). The independent variables are Performance Expectancy (PE), Effort Expectancy (EE), Subjective Norms (SN), Facilitating Conditions (FC), Hedonic Motivation (HM), Price Value (PV), Habit (HAB), and Perceived Risk (RISK).

The results show that all of the independent variables are significant predictors of Use Intention. The strongest predictor is Performance Expectancy, followed by Effort Expectancy and Subjective Norms. The other variables are also significant predictors, but they have a smaller effect on Use Intention.

The table also shows the standardized coefficients for each independent variable. These coefficients show the relative strength of each variable in predicting Use Intention. Performance Expectancy has the strongest standardized coefficient, followed by Effort Expectancy and Subjective Norms. The other variables also have significant standardized coefficients, but they have a smaller effect on Use Intention.

Overall, the results of this study show that Performance Expectancy, Effort Expectancy, and Subjective Norms are the strongest predictors of Use Intention. The other variables are also significant predictors, but they have a smaller effect.","The two goals of Study1 were to contrast the accuracy of effectiveness prediction (the difference between actual and predicted effectiveness) for email and FtF requests (H1) and to verify whether this overestimation is moderated by the closeness level between the requesters and the targets (H1a). To test these hypotheses, a new variable (Prediction accuracy) was calculated by subtracting Actual effectiveness from Predicted effectiveness. The data was submitted to a univariate ANOVA1 with factors of Media (email vs. FtF ? Between-subjects) and Closeness (Strangers vs. Friends ? Between-subjects), and Prediction accuracy as the DV. We looked at the main effect of Media which was highly significant [F (1, 109) = 22.105, p < 0.001,  = 0.169] showing that the magnitude of inaccuracy was significantly larger in email compared to FtF when Closeness levels are collapsed (H1 supported). However, neither the interaction [F (1, 109) = 0.321, p = 0.572,   = 0.003] nor the main effect of Closeness (F (1, 109) = 0.028, p = 0.867,   <0.001) was significant. Hence, the pattern of overestimation is not different among Friends and Strangers groups, that is the closeness between requesters and targets does not improve the accuracy of predicted effectiveness (H1a is not supported). We also asked participants to report their own feeling about the task (Table 1). We looked at the ANOVAs to see if there are any differences across conditions in terms of participants? feelings about the task. As expected, we found that the Friends group perceive requesting to be easier and feel less awkward approaching a target individual compared to the Strangers group. However, the two groups do not predict any difference in the embarrassment after being rejected. No significant interaction emerged between Media and Closeness nor any main effect for Media was observed (see Table 1).",35.18%,94.15%,79.67%,24.89%,80.82%
102,184.jpg," The table shows the results of a path analysis. The path coefficients, standard deviations, t-values, p-values, and squared multiple correlations are reported.

The path coefficients represent the strength of the relationships between the variables. The standard deviations represent the amount of error in the estimates of the path coefficients. The t-values represent the ratio of the path coefficients to their standard deviations, and the p-values represent the probability of obtaining a t-value as large as or larger than the observed t-value, assuming that the null hypothesis is true. The squared multiple correlations represent the amount of variance in the dependent variable that is explained by the independent variables.

The results show that all of the path coefficients are significant at the 0.05 level, except for the path coefficient between H6 and PRI-VALUI, which is significant at the 0.06 level. This means that all of the independent variables have a significant effect on the dependent variable, except for H6. The squared multiple correlations show that the independent variables explain 36% of the variance in the dependent variable.","We asked our participants one open-ended and several Likert scale questions (Appendix E) to justify their choice of medium. A content analysis of the open-ended question revealed that our Likert questions covered all the reasons provided by participants. Then, a factor analysis was conducted on the reasoning data leading to two factors and two single items (Table 2).",27.93%,83.39%,81.57%,38.64%,74.03%
103,185.jpg," The table shows the training and testing results of a neural network with 10 inputs and 1 output. The inputs are: performance expectancy, effort expectancy, subjective norms, facilitating conditions, hedonic motivation, perceived risk, habit, and price value. The output is use intention. The training and testing results are both 0.95, which indicates that the neural network is able to learn the relationship between the inputs and the output.","To test hypotheses 2 through 4, an omnibus logistic regression was conducted (Table 3). All the variables were mean-centred and logistic regression was conducted using the bootstrap method with 5000 resampling. Although we have already observed that a substantial portion of requesters chose email, which is the suboptimal medium according to Study1, the expectation was that their reasons would be different when approaching Strangers vs. Friends. Hence, in the following analysis, we included the interaction terms between Closeness and each of the reasons in Table 2. As seen in Table 3, two significant interactions emerged supporting H3a and H4a but not H2a. Please note that the main effects of factors involved in these significant interaction terms (i.e., Closeness, AwkEmbar, and Convenience) are not interpretable at this stage.",35.98%,91.43%,82.16%,21.41%,82.06%
104,186.jpg," The table shows the relative importance of each of the ten criteria in the decision-making process. The criteria are listed in the first column, and the relative importance of each criterion is shown in the second column. The relative importance of a criterion is determined by its weight in the decision-making process. The higher the weight, the more important the criterion is.

The most important criterion is Performance expectancy, with a relative importance of 0.256. This is followed by Effort expectancy, with a relative importance of 0.175. The third most important criterion is Subjective norms, with a relative importance of 0.099. The fourth most important criterion is Facilitating conditions, with a relative importance of 0.052. The fifth most important criterion is Hedonic motivation, with a relative importance of 0.046. The sixth most important criterion is Price value, with a relative importance of 0.022. The seventh most important criterion is Habit, with a relative importance of 0.039. The eighth most important criterion is Perceived risk, with a relative importance of 0.017.

The table also shows the average relative importance of each of the ten criteria. The average relative importance is calculated by taking the sum of the relative importance of each criterion and dividing it by the number of criteria. The average relative importance of the ten criteria is 0.100.","However, we can conclude that Effectiveness does not play a significant role in media selection decision (H2 was not supported) as further confirmed in Table 4. To unpack the significant interaction terms of Closeness, data was split on Closeness levels (Friends vs. Strangers) and separate logistic regressions (bootstrap method with 5000 resampling) were performed for each subset of data (Table 4) with the decision outcome as the dependent variable and reasoning items explained above as independent variables. Although no predictor reached the significance level for the Friends group (H3 and H4 not supported for Friends), the biggest coefficient emerged for Effectiveness. On the other hand, email was more attractive for the Strangers group due to less awkwardness, less embarrassment, and the convenience of an email request (H3 and H4 supported for Strangers). A minority within this group picked the more effective medium for the right reason as shown by the marginally significant coefficient of Effectiveness.",32.79%,81.36%,79.99%,41.55%,83.45%
105,187.jpg," The table shows the results of the PLS-SEM and ANN analyses. The first column shows the relationship between the constructs, the second column shows the OLS standardized coefficient, the third column shows the PLS-SEM path coefficient, the fourth column shows the ANN normalized relative importance, the fifth column shows the OLS ranking based on standardized coefficients, the sixth column shows the PLS-SEM ranking based on path coefficients, and the seventh column shows the ANN ranking based on normalized relative importance.

The results show that the most important relationship in the PLS-SEM model is between PE and UI, with a path coefficient of 0.305. This means that a one-unit increase in PE leads to a 0.305-unit increase in UI. The second most important relationship is between EE and UI, with a path coefficient of 0.185. This means that a one-unit increase in EE leads to a 0.185-unit increase in UI. The third most important relationship is between FC and UI, with a path coefficient of 0.125. This means that a one-unit increase in FC leads to a 0.125-unit increase in UI.

The results of the ANN analysis show that the most important relationship is between SN and UI, with a normalized relative importance of 0.386. This means that SN has the strongest influence on UI. The second most important relationship is between PE and UI, with a normalized relative importance of 0.100. This means that PE has the second strongest influence on UI. The third most important relationship is between EE and UI, with a normalized relative importance of 0.068. This means that EE has the third strongest influence on UI.

The results of the PLS-SEM and ANN analyses are similar, with both models showing that PE, EE, and FC are the most important relationships. However, the ANN analysis also shows that SN is an important relationship, which is not shown in the PLS-SEM model. This suggests that the ANN model may be more accurate in predicting UI.","First, we contrasted the media decision by Male and Female participants by running a logistic regression (bootstrap with 5000 resampling) analysis with Closeness, Gender, and the interaction term of the two variables as the IVs and, the media decision outcome as the dependent variable. As shown in Table G1, no significant interaction effect emerged nor any main effect of Gender on media selection was detected.",33.02%,89.79%,79.65%,36.80%,80.24%
106,188.jpg, Moderator analyses examining the moderating role of perceived risk on the relationship between facilitating conditions and behavioral intention to use mobile payment services.,"Second, we looked at the gender effect of reasoning by running an ANOVA analysis which was explained in the main text. Lastly, we excluded the female participants from the data set and ran another logistic regression. The significant interaction terms in Table 3 were also significant in this analysis.",15.85%,87.37%,84.87%,14.51%,68.20%
107,189.jpg," The steps taken in the laddering method and the results obtained in each step are as follows:

**Step 1: Preference Consumption Distinction**
At this stage, the researcher asks the participant to describe a recent shopping experience. The goal is to understand the participant's preferences and consumption habits.

**Step 2: Probing**
The researcher then asks the participant to explain why they made the choices they did. This helps the researcher to understand the participant's underlying motivations.

**Step 3: Three Levels of Abstraction**
The researcher then asks the participant to identify the three most important reasons why they made the choices they did. This helps the researcher to identify the participant's core values.

**Step 4: Content Analysis**
The researcher then analyzes the data to identify common themes and patterns. This helps the researcher to develop a deeper understanding of the participant's values.

**Step 5: Implication Matrix**
The researcher then creates an implication matrix, which shows how the participant's values are related to each other. This helps the researcher to identify the participant's overall value system.

**Step 6: Hierarchical Value Map**
The researcher then creates a hierarchical value map, which shows the participant's values in a visual format. This helps the researcher to communicate the participant's values to others.","We asked our participants to contrast requesters? perspectives when they are approached FtF and via email. Two separate factor analyses were conducted on FtF and email perspective-taking measures with similar emerging factors as shown in Table A1. Cronbach?s alphas are reported separately for FtF questions and email questions. Separate repeated measure ANOVAs, Closeness (Cls vs. Str ? between-subject) × perspective-taking index (FtF vs. eml ? within-subject), were conducted for each of the three emerged indices and none of them resulted in a significant interaction between Closeness and the perspective-taking index. It suggests that Participants in both Closeness conditions predicted the same magnitude of change in targets? perspectives when moving from FtF to email. As shown in the last column of Table H1, participants, regardless of their closeness level to the target (main effect of media), acknowledged differences between FtF and email requests for some of the indices and Single measures. We tested whether requesters consider any of the significant indices/measures when predicting the effectiveness of each medium. Index 3 was positively correlated with requesters? prediction of both FtF effectiveness (r(106)= 0.401, p < .0001) and Email effectiveness (r(107)= 0.398, p < 0.001). Index 1 was correlated with Email prediction only (r(106)= 0.246, p = 0.011) but it wasn?t strong and vanished when data was split on the Closeness factor. Single1 was correlated both with Email (r(48)= 0.366, p = 0.010) and FtF (r(48)= 0.475, p = <0.001) predictions but Single2 was only correlated with FtF predictions (r(48)= 0.539, p = <0.001). These significant correlations offer possible causes that make requesters to perceive FtF as a more effective channel for reaching out to targets. However, as shown in Study 1, requesters? perception is not even nearly close to the real extent of the difference between the two channels.",27.39%,75.60%,78.44%,29.13%,82.83%
108,19.jpg," The figure shows a decision tree for choosing a semi-supervised learning method. The first step is to determine whether the data is labelled or not. If the data is labelled, then the next step is to determine whether the labelling is reliable. If the labelling is reliable, then semi-supervised learning can be used to improve the performance of the model. If the labelling is not reliable, then active learning can be used to improve the quality of the labels. 

If the data is not labelled, then the next step is to determine whether the data is abundant or scarce. If the data is abundant, then self-supervised learning can be used to learn from the unlabelled data. If the data is scarce, then few-shot learning can be used to learn from a small amount of labelled data.","H1 : There is a negative relationship between the transparency in SHAs? decision and recommendation algorithms and perceived creepiness. H2 : There is a negative relationship between the tangibility of SHAs and perceived creepiness. H3 : Perceived creepiness has a positive relationship with resistance to SHAs. H4 : Resistance has a negative relationship with the intention to use SHAs. H5 : Resistance has a negative relationship with perceived usefulness. H6 : Resistance has a negative relationship with perceived ease of use. H7 : Perceived usefulness has a positive relationship with intention to use SHAs.  H8: Perceived ease of use has a positive relationship with intention to use SHAs.Fig. 1 summarizes the derived conceptual model of this study, which will be tested in our subsequent studies.",14.81%,81.16%,81.34%,42.16%,85.87%
109,190.jpg," The figure shows the relationships between attributes, benefits, and values. The arrows indicate the direction of the relationship. For example, the arrow from the attribute ""portability"" to the benefit ""time saving"" indicates that portability leads to time saving. The values are at the end of the chain, and they are influenced by the benefits and attributes. For example, the value ""economic value"" is influenced by the benefits ""increased productivity"" and ""value for money"".","Fig. 3 displays the final consensus mental model. The final consensus mental model consisted of six constructs: four inhibitors and two enablers. As displayed in Fig. 3, participants? responses to SHAs were shaped by the interplay of the enablers perceived usefulness and perceived ease of use and the inhibitors perceived creepiness, privacy concerns, perceived loss of control, and perceived immaturity of technology. While most of these constructs have been extensively studied in research on technology acceptance and resistance, perceived creepiness emerged as a novel inhibitor for the context of SHAs (see e.g., Chouk and Mani, 2019, Mani and Chouk, 2019, Lee, 2020, or Pal et al., 2021 who examine the inhibiting role of constructs analogous to privacy concerns, loss of control, and perceived immaturity of technology in the context of smart technology). Moreover, the centrality measures indicate that perceived creepiness has the highest centrality of all the identified inhibitors. Thus, these findings offer initial evidence and a glimpse into the possibility that individuals who decide not to utilize SHAs may be influenced by a certain sense of creepiness with them.",41.62%,79.90%,82.51%,13.18%,82.99%
110,191.jpg," **Consumer benefits** are the specific advantages that a consumer perceives from using a product or service. These benefits can be tangible (e.g., a product that is durable or has a long warranty) or intangible (e.g., a product that makes the consumer feel good about themselves).

**Consumer values** are the beliefs and principles that guide a consumer's behavior. These values can be personal (e.g., a consumer who values honesty) or social (e.g., a consumer who values environmental sustainability).

The relationship between consumer benefits and consumer values is complex and multifaceted. On the one hand, consumer benefits are often derived from consumer values. For example, a consumer who values honesty may be more likely to purchase a product that is marketed as being honest and trustworthy. On the other hand, consumer values can also influence the way that consumers perceive benefits. For example, a consumer who values environmental sustainability may be more likely to perceive a product as being beneficial if it is marketed as being environmentally friendly.

Understanding the relationship between consumer benefits and consumer values is essential for marketers. By understanding what consumers value, marketers can develop products and services that appeal to those values and provide the benefits that consumers are looking for.

In the figure, the relationship between consumer benefits and consumer values is shown in a two-dimensional matrix. The matrix shows that there are six main types of consumer benefits: sensory, efficiency, assessment, discovery, security, and lifestyle. Each of these benefits is associated with a number of consumer values. For example, the sensory benefit of perceived aesthetics is associated with the consumer values of security and achievement.

The matrix can be used to help marketers understand the relationship between consumer benefits and consumer values. By understanding the benefits that consumers are looking for, marketers can develop products and services that appeal to those benefits and provide the value that consumers are seeking.","Next, we selected those items with high and distinct loadings on this one factor (= .80). In addition, we sorted out items that did not clearly load on the feeling of creepiness factor. The final item set included seven items (see Table 1).",13.20%,81.33%,80.06%,35.24%,69.45%
111,192.jpg," The figure shows the relationship between AR attributes, consumer benefits, and consumer values. 

AR attributes are the unique features of augmented reality that make it different from other technologies. These attributes include:

* Contextualization: AR can be used to provide users with information about their surroundings, such as historical facts or directions.
* Assortment: AR can be used to help users find products or services that they are looking for.
* Portability: AR devices are lightweight and portable, making them easy to use on the go.

Other AR attributes include customization, interactivity, socialization, reality congruence, informativeness, and shareability.

These AR attributes provide consumers with a variety of benefits, including:

* Sensory: AR can be used to create immersive and engaging experiences that stimulate the senses.
* Efficiency: AR can be used to help users complete tasks more quickly and easily.
* Assessment: AR can be used to help users make better decisions by providing them with more information.
* Discovery: AR can be used to help users discover new products or services.

These consumer benefits ultimately lead to a variety of consumer values, including:

* Sales: AR can be used to increase sales by providing consumers with a more engaging and informative shopping experience.
* Security: AR can be used to improve security by providing users with real-time information about their surroundings.
* Achievement: AR can be used to help users achieve their goals by providing them with the tools and information they need.
* Lifestyle: AR can be used to enhance users' lifestyles by providing them with new and innovative ways to interact with the world around them.
* Economy: AR can be used to save users money by providing them with more efficient ways to shop and complete tasks.
* Status: AR can be used to enhance users' social status by providing them with access to exclusive products and experiences.","Next, we examined the effect of perceived creepiness on resistance. This analysis showed that the creepier participants perceived the SHA, the higher their resistance to using the SHA, supporting H3 (OLS regression including all conditions: B = 0.57, SE =.03, p < .001, d = 1.11). Considering the downstream effects of resistance, we find that all regression paths were significant at p < .001, except for the relationship between ease of use and usage intention (p = .316). In support of H4, resistance exerted a direct negative effect on usage intentions (B = -0.22, SE =.05) and negatively affected perceived usefulness (H5; B = -0.60, SE =.03) and ease of use (H6; B = -0.20, SE =.03). In line with H7, perceived usefulness exerted a positive effect on usage intention (B = 0.71, SE =.03). However, since ease of use did not significantly affect usage intention (B = -0.03, SE =.03), H8 had to be rejected. Finally, we tested the significance of the indirect effects of transparency and tangibility on usage intention through the proposed mediators ? creepiness, resistance, perceived usefulness, and perceived ease of use. Except for the path through perceived ease of use, all indirect effects reached significance at 95% CI. Table 2 provides a detailed overview of all results.",13.66%,71.35%,78.29%,39.53%,82.72%
112,193.jpg," The figure shows the relationships between algorithmic injustice, trust in DA outcomes, discrimination, displacement of responsibility, and guilt.","Anchoring on the challenge-hindrance stressor framework, we apply the JD-R model to propose hypotheses development in the research model (Fig. 1). Specifically, we discuss that THS has a positive effect on workarounds, whereas TCS has a negative effect on workarounds. We also propose that support structures and trait resilience weaken the positive effect of THS on workarounds and strengthen the negative effects of TCS on workarounds.",16.30%,82.97%,83.70%,3.35%,71.22%
113,194.jpg," The table shows the descriptive statistics of the sample. The sample size is 122. The mean age of the respondents is 53.83 years old with a standard deviation of 11.72 years. 42 of the respondents are female (34.4%) and 80 are male (65.6%). 7.4% of the respondents have a high school education, 9.8% have a college education, 41.8% have a bachelor's degree, 34.4% have a master's degree, and 6.6% have a Ph.D. The mean firm size is 30,443 employees with a standard deviation of 14,822 employees. 7.4% of the respondents are in the consumer goods industry, 4.1% are in the producer goods industry, 47.5% are in the services industry, 10.7% are in the finance industry, 3.3% are in the utilities industry, and 27% are in other industries.","The results of the hypotheses testing about the research model were conducted by AMOS 28.0. We mean-centered the independent, moderator, and dependent variables to further decrease the possibility of multicollinearity. Fig. 2 showed the results of the main structural model. The R2 value for the dependent variable (workarounds) is 0.39. THS positively influenced workarounds (ß = 0.27, p < 0.01), whereas TCS negatively influenced workarounds (ß = -0.16, p < 0.05), supporting H1.",39.85%,59.24%,79.79%,40.68%,76.65%
114,195.jpg," Table 3.

Correlation matrix and descriptive statistics.","Table 2 showed the demographic information. Specifically, we ranked respondents by the interval between responses to the two rounds of questionnaires, then we compared the demographic information of the early and late 25% of respondents (Armstrong and Overton, 1977, Sivo et al., 2006). The results (Table 2) showed no significant differences in gender, age, education, and usage experience. Therefore, in line with prior literature (Benlian, 2020, Ke et al., 2021, Laumer et al., 2017), we find that, even with a relatively low response rate, the comparative analysis revealed that nonresponse bias was not problematic for the present research.",8.19%,73.25%,83.20%,0.00%,67.10%
115,196.jpg," The table shows the loadings and cross-loadings of the items in the measurement model. The loadings represent the correlations between the items and their respective latent constructs, while the cross-loadings represent the correlations between the items and the other latent constructs in the model.

As can be seen from the table, all of the items have high loadings on their respective latent constructs, and low cross-loadings on the other latent constructs. This indicates that the items are measuring what they are supposed to measure, and that they are not measuring anything else. This provides evidence for the convergent validity and discriminant validity of the measurement model.","The longitudinal two-wave design with a time lag between the independent and dependent variables ensured that common method bias (CMB) was alleviated (Sykes, 2015). We also applied the Harmon single-factor test (Podsakoff et al., 2003). We found 10 factors with eigenvalues over 1, and the first factor accounted for 26.75%, lower than the threshold of 40%. In addition, we conducted a fit comparison between the one-factor model and the measurement model (Flynn et al., 2010). The one-factor model fit (?2/df=11231.312/560 =20.06, RMSEA=0.226, SRMR=0.332, CFI=0.182, TLI=0.131) indicated a worse result than the fit of our measurement model (?2/df=1849.975/539 =3.43, RMSEA=0.0072, SRMR=0.051, CFI=0.900, TLI=0.918). Finally, we adopted the marker variable technique to further examine CMB (Malhotra et al., 2006). In particular, the selected marker variable should be unrelated to any other variable in the measurement model (Acharya et al., 2022). We chose the vision of continuity as the marker variable. It refers to the organizational vision of maintaining continuity amidst internal and external changes (Venus et al., 2019), which was an irrelevant variable with two items. Table 3 showed that the correlations between the marker variable and other latent variables were irrelevant. Then, we utilized the lowest positive correlation (r = 0.02) to adjust the correlations among constructs. The results indicated that the difference between unadjusted correlations and adjusted correlations was not significant. Hence, CMB might not be a concern.",53.94%,78.93%,81.25%,19.30%,86.42%
116,197.jpg," Logistic Regression Model (DV: Discrimination)

Main effects
Constant -9.673 (0.001)
Gender -0.791 (0.299)
Age -0.03 (0.265)
Stereotypic Beliefs 0.413 (0.127)
Algorithmic Injustice (0 = Just; 1 = Unjust) 3.867 (<0.001)
Displacement of Responsibility -0.064 (0.819)
Trust in DA Outcomes 1.431 (<0.001)

Main and interaction effects
-0.032 (0.995)
-0.499 (0.556)
-0.034 (0.246)
0.433 (0.142)
5.675 (0.092)
0.459 (0.512)
-0.880 (0.424)

Algorithmic Injustice x Displacement of Responsibility -0.688 (0.364)
Algorithmic Injustice x Trust in DA Outcomes 2.821 (0.021)

Omnibus Test of Model Coefficients x2 = 46.45 (<0.001) x2 = 53.16 (<0.001)

Cox and Snell R2 0.321 0.358
Nagelkerke R2 0.540 0.603
Goodness of fit (Hosmer and Lemeshow Test) x2 = 10.733 x2 = 1.383

Note: P-values are shown in parentheses.","We strategically selected 20 respondents from four subsidiaries of the electric company, all of whom had completed both phases of the questionnaires (as indicated in Table 4). Due to the constraints imposed by the COVID-19 pandemic, we conducted telephone interviews as the most practical and safe means of data collection. Each interview, on average, had a duration of approximately 10 min. Within the timeframe, we allocated around 7 min for specific questions tailored to each participant, while the remaining time was dedicated to providing a comprehensive introduction to the research background. While the interviews were relatively short, they were designed to be concise and focused, ensuring that participants? responses provided in-depth insights into their experiences and perspectives related to ES use, particularly in the context of TDS and support structures.",10.23%,31.29%,77.62%,34.70%,53.60%
117,198.jpg," **Antecedents and consequences of skepticism**

**Hypotheses:**
H1a: Web seal unreliability is positively related to skepticism.
H1b: Seal authority incredibility is positively related to skepticism.
H2a: Web seal unreliability is positively related to disbelief in web seal.
H2b: Seal authority incredibility is positively related to disbelief in web seal.
H3: Disbelief in web seal is negatively related to perceived assurance.
H4: Mistrust in seal authority is positively related to inferences of IS provider’s manipulative intent.
H5a: Disbelief in web seal mediates the relationship between web seal unreliability and perceived assurance.
H5b: Mistrust in seal authority mediates the relationship between seal authority incredibility and inferences of IS provider’s manipulative intent.","Fig. 1 provides an overview of the selection process and outcomes at each stage, specifically the identification, screening, eligibility, and inclusion of articles in the study. From the identification step, 5470 article records were retrieved. We removed 1177 duplicated records. In the next step, we review 4293 records, following an in-depth evaluation of 371 full-text articles. Finally, 120 articles met the inclusion criteria and were included in the final analysis. Following that, an extraction of detailed data from the included articles was carried out (see Appendix, Table A1).",10.78%,73.08%,79.26%,39.51%,72.40%
118,199.jpg," The figure shows the results of a study on the antecedents and consequences of skepticism. The study found that web seal unreliability, seal authority incredibility, and dispositional skepticism were all positively related to skepticism. In addition, perceived assurance was negatively related to skepticism. Mistrust in seal authority and mistrust in IS provider were both positively related to skepticism. Finally, inferences of IS provider's manipulative intent was positively related to mistrust in seal authority and mistrust in IS provider.","For the thematic analysis, we sought to construct a high-level framework as a template to organize emerging themes. Based on a hybrid view, integration of variance, and process thinking recommendations (Burton-Jones et al., 2015, de Guinea and Webster, 2017, Langley, 2007, Maitlis and Lawrence, 2007), for a systematic examination of the health data breaches, we developed a model (see Fig. 2). This model distinguishes between facilitators (the conditions that clear the paths for potential data breaches), data breach types, and impacts (an effect or influence that a data breach incident has on individuals and organizations/businesses). In developing our model and identifying antecedents, facilitators, and impacts, we were inspired by recent methodological developments in the IS field (Burton-Jones et al., 2015, de Guinea and Webster, 2017) and the notion of hybrid models. In our model, facilitators and data breach types (enablers) denote a process approach (see, for example, Maitlis and Lawrence, 2007), while studying impacts (consequences) pertains to variance approaches. As stated by de Guinea and Webster (2017, p. 159), a hybrid model is an ideal candidate for ?investigating events that trigger certain changes in states or outcomes?.",23.96%,76.07%,80.77%,12.47%,80.15%
119,20.jpg, The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram of the study selection process.,Graphical Abstract,0.00%,72.59%,78.15%,0.00%,32.56%
120,200.jpg," The table shows the discriminant validity of the constructs. All the constructs have an AVE value greater than 0.5, which indicates that they have good discriminant validity. In addition, the square root of the AVE value of each construct is greater than the correlations between the constructs, which also indicates that the constructs have good discriminant validity.", Fig. 1 depicts the proposed R2M2 model.,16.41%,67.65%,84.92%,34.50%,42.14%
121,201.jpg," The table shows the HTMT values between the constructs. All HTMT values are below the threshold of 0.85, indicating discriminant validity between the constructs.","According to the CKC model shown in Fig. 2 (NIST, 2014b), an attacker initially carries out reconnaissance to assess the potential benefits in terms of ransom (Oz et al., 2022) that they could gain from targeting an organization (NIST, 2022, Souppaya and Scarfone, 2013). Larger organizations, although better equipped with security technologies (Angst et al., 2017, Dalton et al., 1980, McLeod and Dolezel, 2018, Schlackl et al., 2022), would be severely impacted by a ransomware attack, as their IT-enabled supply chain may come to a standstill. To maintain seamless operations, a large organization will prefer to pay the ransom at times (Carroll & Stater, 2008). Therefore, this study proposes the following hypotheses: H1a: Larger organizations are more prone to ransomware attacks.",12.72%,68.26%,81.29%,0.88%,72.92%
122,202.jpg," The table shows the results of the hypotheses testing. The first column shows the hypothesis number, the second column shows the hypothesis, the third column shows the path, the fourth column shows the p-value, and the fifth column shows the support for the hypothesis.

The results show that all of the hypotheses are supported except for H4. H4 states that there is a positive relationship between perceived assurance and distrust in IS. However, the results show that there is no significant relationship between these two variables.

The other hypotheses are all supported. This means that there is a positive relationship between web seal unreliability and distrust in IS, a positive relationship between web seal unreliability and authority in IS, a positive relationship between authority in IS and distrust in IS, a positive relationship between perceived assurance and distrust in IS, a positive relationship between perceived assurance and manipulative intent, and a positive relationship between manipulative intent and distrust in IS.","This study used data from the University of Queensland, Australia, to explore the cyber resilience of organizations (Tsen et al., 2020). The dataset consisted of various features of 1473 organizations belonging to the critical and non-critical sectors in Australia, the USA, Canada, and Japan and their associated breaches due to ransomware attacks from 2004 to 2022. The dataset included firms with varying digital intensity, organizational size, network segmentation, EVSS, and CSR. We referred to Statista for data on the average financial losses (Lj) (Statista, 2022) from 2004 to 2022, which are derived from cyber-crime cases reported by the Internet Crime Complaint Center (IC3), which is a part of the FBI. Table 5 summarizes the statistics of the data.",32.78%,78.93%,80.32%,42.92%,76.88%
123,203.jpg," This timeline shows a two-period model of durable goods demand with costly information acquisition. In the first period, the firm sets a price for the durable good. Consumers can choose to purchase the durable good or not. In the second period, the firm sets a price for the related information. Consumers can choose to purchase the related information or not. After purchasing the related information, consumers can choose to purchase the durable good or not.",Table 6 shows the relationship between the dependent and independent variables.,23.93%,73.67%,85.43%,36.00%,75.96%
124,204.jpg," The table summarizes the conditions for the three regions in Figure 4. The first column lists the conditions, the second column lists the region in Figure 4 where the conditions hold, and the third and fourth columns list the signs of \(\frac{\partial \pi^H}{\partial B}\) and \(\frac{\partial \pi^L}{\partial B}\), respectively.","We used generalized linear models such as LR (Son et al., 2020) in the RRA module to test the hypotheses. Based on the chi-squared test, the goodness of fit of the model was revealed to be significant (p < 0.001) with a small deviance of 66.68. Table 8 reports the parameters of the M1 (LR) model that were significant at the 1% level. Substituting the values from Table 8 into Equation (1), Eq. (3) is determined, as follows (3) Table 8 and Eq. (3) indicate that larger organizations are 6.786 times more likely to face R attacks than NR attacks (p = 0.001), thereby supporting hypothesis H1a. Moreover, if organizations belong to a critical industry, their chances of facing ransomware attacks increase by 5.229 times compared to NR attacks (p = 0.001). This finding supports hypothesis H1b. Similarly, for each unit increase in digital intensity, the odds of R attacks are 13.573 times greater than those of NR attacks (p = 0.001). This result aligns with hypothesis H2a. However, if the network is segmented, the probability of occurrence of R attacks decreases to 0.042 times that of NR attacks (p = 0.001), thereby supporting hypotheses H2b. Table 8 also indicates that for each unit increase in vulnerabilities in the organizational environment, the odds of ransomware attacks increase by 2.734 times compared to other cyberattacks (p = 0.098). This finding supports hypothesis H3. In contrast, if information security governance is properly implemented in an organization, for an increase in every cybersecurity role, the probability of ransomware attacks decreases to 0.421 times that of non-ransomware attacks (p = 0.100), thereby supporting hypothesis H4.",30.59%,84.60%,78.01%,0.55%,83.17%
125,205.jpg," The table shows the conditions for which firm H and firm L will enter or exit the market. The conditions are based on the values of the parameters &beta;, &beta;_1, and &beta;_3. The table is divided into three regions, and the conditions for each region are shown in the table.

In Region I, both firm H and firm L will enter the market if &beta; < &beta;_1 and &beta; < B_3(&beta;_0). In this region, both firms will have positive profits.

In Region II, firm H will enter the market, but firm L will not. This is because &beta; > &beta;_1 and &beta; > B_3(&beta;_0). In this region, firm H will have positive profits, but firm L will have negative profits.

In Region III, neither firm H nor firm L will enter the market. This is because &beta; > &beta;_1 and &beta; < B_3(&beta;_0). In this region, both firms will have negative profits.","Next, in the RRA module, we used the M1, M2, and M3 models to classify attacks as R or NR. The performances of the three models were measured and compared on the test dataset using the accuracy, precision, recall, and F1-score, as shown in Table 9. It is evident from Table 9 that M1 was better than M2 and M3 because its accuracy, precision, recall, and F1-score were better than those of the other two models. Hence, this study further elaborates on the results of M1 (the LR model). Figs. 3(a) and 3(b) show that M1 could correctly classify or predict 27 out of 29 R and 293 out of 314 NR data points of the test dataset.",25.30%,69.34%,81.19%,44.30%,80.94%
126,206.jpg," The table summarizes the conditions for the three different regions in the diagram and the corresponding signs of the derivatives of profits with respect to the parameter \(B\). 

In region I, both firms have increasing profits as \(B\) increases. In region II, firm \(H\)'s profits increase with \(B\), while firm \(L\)'s profits decrease. In region III, both firms have decreasing profits as \(B\) increases.","Based on the method by Nickerson et al. (2013), combined with the steps of selected sub-methods, we compiled an integrated research design (cf. Fig. 1). While the research method is based on Nickerson et al. (2013), its application is inspired by Cledou et al. (2018) as they developed their taxonomy. This section outlines methodological steps, and section 4 describes the application of these steps, following the phases listed in Fig. 1: Planning & Data Collection, Taxonomy Construction, and Taxonomy Evaluation.2",20.90%,82.30%,81.11%,34.88%,81.02%
127,207.jpg," The figure shows the research model that was developed based on the literature review. The model proposes that BI system attributes (system quality, data quality, information quality, and service quality) have a direct effect on user satisfaction and an indirect effect on user performance. The indirect effect is mediated by self-efficacy, task complexity, routine use, and advanced use.","As described in section 3, we informed the taxonomy development from both a theoretical perspective and an empirical perspective. The Taxonomy Construction itself was performed in four iterations (cf. Fig. 2). The first iteration followed the conceptual-to-empirical approach. That is, we derived dimensions and characteristics from the 23 identified articles and examined 30 of the 210 digital innovation projects from the database to assess their empirical evidence.",24.19%,87.10%,83.45%,38.38%,77.65%
128,208.jpg," Table 1. Measurement scales and reliability analysis

Variable	Items	AVE	Cronbach's Alpha
Data Quality	The BI system is always up and running.	0.751	0.915
The BI system is easy to navigate.	0.830	
The BI system is able to handle all of the data that is needed by the business.	0.821	
The BI system is always accessible.	0.872	
The BI system is reliable.	0.857	
The BI system is secure.	0.846	
Information Quality	Data in the BI system is accurate.	0.831	0.901
Data in the BI system is complete.	0.845	
Data in the BI system is consistent.	0.868	
Data in the BI system is relevant.	0.855	
Data in the BI system is timely.	0.838	
Service Quality	My organization provides me with adequate training on how to use the BI system.	0.784	0.859
My organization provides me with the support that I need to use the BI system effectively.	0.822	
The BI system is easy to use.	0.857	
The BI system is user-friendly.	0.886	
The BI system is intuitive.	0.826	
User Satisfaction	I am satisfied with the BI system.	0.878	0.939
I would recommend the BI system to others.	0.872	
The BI system has helped me to be more productive in my job.	0.878	
The BI system has helped me to make better decisions in my job.	0.890	
The BI system has helped me to be more confident in my job.	0.871	
Individual Performance	Using the BI system, I am able to perform my job more effectively.	0.763	0.906
Using the BI system, I am able to perform my job more efficiently.	0.831	
Using the BI system, I am able to make better decisions in my job.	0.832	
Using the BI system, I am able to be more productive in my job.	0.827	
Using the BI system, I am able to work more collaboratively with others in my job.	0.781	
Self-Efficacy	I can easily find the information that I need in the BI system.	0.853	0.936
I can easily understand the information that is presented in the BI system.	0.875	
I can easily use the BI system to perform the tasks that I need to do.	0.909	
I am confident in my ability to use the BI system.	0.872	
I am able to use the BI system to solve problems and make decisions.	0.851	
Task-Specific Usefulness	The BI system helps me to do my job better.	0.820	0.946
The BI system helps me to save time in my job.	0.867	
The BI system helps me to be more accurate in my job.	0.835	
The BI system helps me to be more organized in my job.	0.852	
The BI system helps me to be more efficient in my job.	0.824	
Advanced Use	I use the BI system to create reports and presentations.	0.674	0.720
I use the BI system to perform ad-hoc queries.	0.726	
I use the BI system to build models and simulations.	0.566	
I use the BI system to develop and test hypotheses.	0.620	
I use the BI system to collaborate with others on projects.	0.678	
Perceived Playfulness	Playing around with the BI system is enjoyable.	0.813	0.885
Experimenting with the BI system is stimulating.	0.822	
Using the BI system is fun.	0.823	
The BI system makes me feel creative.	0.786	
The BI system allows me to be imaginative.	0.782	
Predicting Performance	Setting meaningful performance targets using the BI system	0.832	0.875
Creating effective marketing campaigns using the BI system	0","In the final iteration, the objective ending conditions described in Table 1 are fulfilled, so the final taxonomy is complete, empirically relevant, stable, and unambiguous. However, within the iterations of the Taxonomy Construction process, we made several adjustments to the taxonomy (i.e., additions, deletions, and relabeling of dimensions and characteristics). Changes along the development process were made when the research team concluded that the taxonomy?s explanatory character, conciseness, and robustness would increase given a particular adjustment. We also revised the taxonomy when analyzing the projects revealed that certain dimensions and/or characteristics had little empirical relevance, are invariant across the sample, or are not mutually exclusive (i.e., characteristics were hardly distinguishable from one another). The progression of adjustments in these iterations is shown in Fig. 4.",30.74%,76.56%,77.82%,36.61%,77.87%
129,209.jpg," **Correlation Matrix**

|  | Mean (S.D.) | SQ | DQ | IQ | SQa | US | IP | SE | TC | RU | AU |
|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|
| SQ | 5.32 (1.42) | 1 | .800 | .147** | .084 | .150** | -.022 | -.016 | .165** | .197** | .234** |
| DQ | 5.40 (1.13) |  | .787 | .111* | .034 | .137** | .081 | -.028 | .141** | .147** | .163** |
| IQ | 5.65 (1.36) |  |  | .848 | .044 | .135** | .160** | -.095 | -.038 | .027** | .255** |
| SQa | 5.29 (1.13) |  |  |  | 1 | .095 | -.002 | .034 | -.001 | .137** | .139** |
| US | 3.24 (1.26) |  |  |  |  | 1 | .084 | -.034 | .065 | .068 | -.085 |
| IP | 5.65 (0.98) |  |  |  |  |  | 1 | -.003 | .119** | .155** | .016 |
| SE | 5.26 (1.23) |  |  |  |  |  |  | 1 | -.002 | .045 | -.092 |
| TC | 5.09 (1.64) |  |  |  |  |  |  |  | 1 | .015 | -.001 |
| RU | 4.13 (1.13) |  |  |  |  |  |  |  |  | 1 | .047 |
| AU | 5.08 (0.98) |  |  |  |  |  |  |  |  |  | 1 |

**Note.** *p < .05, **p < .01.","While the OIPT from Galbraith (1973) offers a powerful mechanistic model to derive strategies to reduce uncertainty, the original theory had several limitations, which is why it has been further advanced over the past decades by several scholars. According to Haußmann et al. (2012), major limitations were, among others, the lack of considering individual information restrictions among stakeholders (e.g., Zmud, 1979), interpersonal characteristics (e.g., Bensaou & Venkatraman, 1995; Burke et al., 2001), inter-organizational relations (e.g., Fairbank et al., 2006). Consequently, subsequent studies (Burke et al., 2001; cf., Cooper & Wolfe, 2005; Daft & Lengel, 1986) successively advance the original theoretical framework from Galbraith, 1973, Galbraith, 1974 and incorporate external environment, interdepartmental relations, and technology as sources of uncertainty and equivocality. One relevant resulting advancement is the recognition that task uncertainty is not the only constraint to be reduced but also equivocality, which is defined as ?ambiguity, the existence of multiple and conflicting interpretations about an organizational situation? (Daft & Lengel, 1986, p. 556). Overall, Haußmann et al. (2012) propose an adapted framework of the OIPT, considering the limitations of previous scholars and incorporating helpful advancements (see Fig. 1).",0.00%,11.24%,73.04%,0.00%,73.94%
130,21.jpg," This is a diagram showing the professional capabilities required for HVAC design, as well as the tasks, dimensions, and scenarios involved.

The diagram shows that there are four main dimensions of HVAC design: recall, analysis, application, and coordination. Each of these dimensions is further divided into a number of tasks. For example, the recall dimension includes tasks such as creating a system inventory and developing design criteria. The analysis dimension includes tasks such as calculating loads and determining equipment capacity. The application dimension includes tasks such as selecting equipment and designing ductwork. The coordination dimension includes tasks such as coordinating with other trades and obtaining permits.

The diagram also shows that there are three main scenarios for HVAC design: new construction, renovation, and retrofit. Each of these scenarios has its own unique set of challenges and requirements. For example, new construction projects typically require a more comprehensive design than renovation or retrofit projects. Renovation projects often involve working with existing systems and constraints, while retrofit projects may require the use of new technologies and materials.

The diagram can be used to help HVAC designers identify the skills and knowledge they need to be successful. It can also be used to help designers understand the different tasks and dimensions involved in HVAC design.","Based on the management factors and the management practices, we iteratively developed the AIAMA model. Since our first model concept considers the AI management factors as the main source of task uncertainty and equivocality leading to information processing issues among stakeholders (i.e., guiding concept 1), we rely on the management factors from Fig. 2 as our model?s main building blocks. Building the basis of our AIAMA model, we adopted the management factors and transformed them into our model (i.e., describing what to manage), as presented in Fig. 4. Each factor consists of several dynamic constructs that can evolve via system changes. In the model, the constructs serve as abstract concepts inductively summarizing observations from reality into researchable objects and explaining the factors of AI application management (Bhattacherjee, 2012, Cronbach and Meehl, 1955).",31.23%,89.82%,81.10%,42.77%,80.89%
131,210.jpg," The table shows the results of a Mann-Whitney U test comparing the high and low advanced use subgroups on each of the 11 variables. The results show that the high advanced use subgroup has significantly higher means on all of the variables except for self-efficacy, where there was no significant difference between the groups.",Coding scheme based on literature analysis,3.84%,76.35%,81.92%,34.57%,53.42%
132,211.jpg," From the table, we can see that:

- H1a and H1b are supported.
- H2 is supported.
- H3a and H3b are supported.
- H4 is supported.
- H5a, H5b, and H5c are supported.
- H6a is not supported.
- H7a and H7b are not supported.
- H8 is not supported.
- H9a and H9b are not supported.
- H10a is supported.
- H11 is supported.
- H12a is supported.
- H13 is not supported.
- H14a and H14b are supported.",Coding scheme based on interview analysis,0.00%,54.48%,78.73%,0.00%,53.13%
133,212.jpg," The table shows the 95% confidence intervals for the standardized total effects of the independent variables (IVs) on the dependent variables (DVs). 

For example, the 95% confidence interval for the total effect of IV IQ on DV RU is [0.105, 0.210]. This means that we are 95% confident that the true total effect of IQ on RU falls between 0.105 and 0.210.

Similarly, the 95% confidence interval for the total effect of IV SQ on DV AU is [0.199, 0.272]. This means that we are 95% confident that the true total effect of SQ on AU falls between 0.199 and 0.272.",We illustrate the AIAMA model application by the management of outdated medical practices. Outdated medical practices may result from updated medical guidelines by healthcare authorities. We exemplify the model application by the case of a hospital having to react to changing medical guidelines as it affects their AI application.,18.18%,53.92%,80.53%,37.29%,69.71%
134,213.jpg," The table shows the results of a study that tested the hypotheses that (1) routine use of a technology (RU) has a positive effect on individual performance (IP), (2) advanced use of technology (AU) has a positive effect on IP, (3) the effect of RU on IP is moderated by task complexity (TC), and (4) the effect of AU on IP is moderated by TC.

The results show that:

* Hypothesis 1a is supported: RU has a positive effect on IP (b = 0.103, p = 0.042).
* Hypothesis 1b is supported: AU has a positive effect on IP (b = 0.200, p < 0.001).
* Hypothesis 2 is supported: The effect of RU on IP is moderated by TC (b = 0.219, p < 0.001).
* Hypothesis 3 is supported: The effect of AU on IP is moderated by TC (b = 0.229, p < 0.001).

In conclusion, the results of this study provide evidence that both RU and AU have a positive effect on IP, and that the effects of RU and AU on IP are moderated by TC.","Fig. 1 shows our research framework. Our study used SDT as the main theory with which to develop a research framework and formulate hypotheses. Past online game research has used SDT to explain player satisfaction (Sepehr & Head, 2018) and loyalty (Teng et al., 2022a). Our study is novel because no previous studies have examined the impacts of our three proposed game design elements. Our study is valuable because our findings can guide game makers to effectively boost player satisfaction by selecting the best elements to improve.",12.64%,66.80%,80.37%,38.86%,84.15%
135,214.jpg," This research focuses on the pre-implementation stage, whereas prior research has focused on the implementation and post-implementation stages.","We used LISREL v.8.53 software, which is a commonly used tool (Shareef et al., 2017, Shareef et al., 2020, Tsai and Bagozzi, 2014), to perform structural equation modeling analysis, which we then used to test our hypotheses. We set the significance at the typical.05 level. Fig. 2 shows the analytical results. Most of the study hypotheses are supported, with some exceptions. First, game creatability is not related to competence satisfaction, thus H1a is not supported. The reason for this may be that game creatability empowers players to unleash their creative potential but may also influence players? focus during gameplay. Specifically, players may prioritize showcasing their creativity rather than honing their playing skills, which is the key theoretical factor in competence satisfaction (Reer et al., 2022). Moreover, game creatability induces players to develop their own storylines. From a practical standpoint, different story endings are not always satisfactory. Players sometimes fail to reach their goal, which is unlikely to result in much competence satisfaction (Deci & Ryan, 2000). These reasons would explain the nonsignificant relationship between game creatability and competence satisfaction.",7.97%,80.46%,81.18%,0.01%,74.15%
136,215.jpg," This image shows the factors that contribute to status quo bias. The factors are divided into three categories: rational decision making, cognitive misperception, and psychological commitment. Each of these categories is then divided into two subcategories. The subcategories for rational decision making are transition costs and uncertainty. The subcategories for cognitive misperception are anchoring effects and sunk costs. The subcategories for psychological commitment are regret avoidance and control.","We further implemented two improvements. First, we omitted the skill level from the list of control variables because, in our theoretical model (Fig. 2), it was the only control variable that was significantly related to usage but not to continuance. Its removal simplifies the list of control variables. Second, we added a third item to assess game creatability: ?I play this online game to create my own characters,? and a third item to assess game achievability: ?I am confident I can achieve the goals of making in-game progress in this online game.? Both scales showed a higher level of reliability (a = .82 and .87), providing future scholars with three-item scales to assess the two concepts. Fig. 3 illustrates the results.",19.87%,88.27%,81.91%,18.89%,79.27%
137,216.jpg," The table shows the incidence of and assigned statements to each of the categories and factors in the study. The category with the highest incidence is psychological commitment, with 22 statements. The category with the lowest incidence is regret avoidance, with 0 statements. The factor with the highest incidence is control, with 27 statements. The factor with the lowest incidence is uncertainty (costs), with 2 statements.","Consistent with the literature (Islam et al., 2022, Sun et al., 2020, Sun et al., 2021), Table 2 lists the loadings and cross-loadings of the measurement items. The items loaded in our assumed factors provide preliminary evidence of our data validity.",28.83%,79.90%,82.74%,44.44%,80.70%
138,217.jpg," The diagram shows a taxonomy of workplace incivility. The taxonomy is based on the following criteria:
- The intent of the perpetrator
- The awareness of the perpetrator
- The harm caused by the behavior

The taxonomy is divided into two main categories: deliberate harm and incidental harm. Deliberate harm is defined as behavior that is intended to cause harm to another person. Incidental harm is defined as behavior that is not intended to cause harm, but does so anyway.

Deliberate harm is further divided into two subcategories: malice and ideals. Malice is defined as behavior that is intended to cause harm to another person for the sake of causing harm. Ideals is defined as behavior that is intended to cause harm to another person in order to achieve a goal.

Incidental harm is further divided into two subcategories: non-compliant and compliant. Non-compliant behavior is defined as behavior that violates the rules or norms of the workplace. Compliant behavior is defined as behavior that follows the rules or norms of the workplace.

The taxonomy can be used to help organizations understand and address workplace incivility. By understanding the different types of incivility, organizations can develop more effective strategies to prevent and address it.","All the Cronbach?s a values are.76 or larger, indicating sufficient reliability (Nunnally & Bernstein, 1994). All the composite reliability (CR) values are.81 or larger, and all the average variance extracted (AVE) values are.55 or larger. These results indicate acceptable reliability (Bagozzi & Yi, 1988). As shown in Appendix A, all the indicator loadings are.65 or higher, suggesting good convergent validity (Hair et al., 1998). As shown in Table 3, all the positive square roots of the AVE values exceed the associated correlations, indicating discriminant validity (Fornell & Larcker, 1981). To offer enhanced evidence of discriminant validity, we tested and found that all the 95% confidence intervals of the correlations are smaller than all the positive square roots of the AVE values. Psychometric properties may include reliability, validity, and model fit performance. Our measurement model has sufficiently good performance in model fit, i.e., CFI= .97, IFI= .97, NNFI= .96, SRMR= .05 (Bagozzi, 2010).",23.15%,67.15%,78.33%,40.56%,81.34%
139,218.jpg," The figure shows the relationships between the justification factors, survey topics, and VISTA categories. The justification factors are listed on the left, the survey topics are listed in the middle, and the VISTA categories are listed on the right. The arrows show the relationships between the three.

The justification factors are:
* Inadequate (Uni-dimensional) Classification
* Evolving Technologies
* Enhanced Training and Awareness
* Remote Work Environments
* Holistic Approach
* Adaptive Countermeasures

The survey topics are:
* Limitations of Current Taxonomies
* Unintentional Insider Threats
* Incomplete Coverage of Insider Behaviors
* Overcoming Simplification
* Insider Threats from Ex-Employee
* Need for Comprehensive & Adaptive Approaches
* Poor Security Awareness and Training
* Insiders Missing Privileges and Access

The VISTA categories are:
* C1 (Untrained)
* C1-C7
* C2 (Fallible)
* C3 (Disempowered)
* C7 (Malicious)","We obtained 546 complete responses through a two-wave data collection process. The data indicate that most of our participants were male (86.7%). This proportion is similar to that of the local player population, in which 83% of players are male (GNN, 2016). Most participants were aged = 30 years (82.1%), had a college/university level education or higher (91.4%), and had an income = NT $600,000.00 (79.5%). Most participants had played the focal game for = 5 years (91.1%), showed weekly gameplay hours of < 21 h (87.7%), and had a skill level of gold or below (91.7%). Table 4 shows the demographic profile of the participants. However, the total numbers shown in Table 4 are not always equal to the total collected sample size due to some missing values in the profile data.",30.77%,62.55%,78.67%,41.61%,76.65%
140,219.jpg,"  그림은 공급망에서의 정보 흐름을 나타낸다. 공급망에서 정보는 공급업체와 구매자 간에 공유된다. 공급업체는 구매자에게 제품이나 서비스에 대한 정보를 제공하고, 구매자는 공급업체에 주문 정보를 제공한다. 이러한 정보 공유는 공급망의 효율성과 효과성을 높이는 데 도움이 된다.

그러나 공급망에서 정보 공유는 또한 보안 위험을 초래할 수 있다. 공급업체와 구매자 간의 정보 공유는 해커가 공급망에 침투하고 데이터를 훔칠 수 있는 기회를 제공할 수 있다. 따라서 공급망에서 정보 공유를 할 때는 보안에 대한 고려가 필요하다.","As shown in Fig. 2 and Table 5, our structural model explains significant variances in the endogenous constructs: 57% in competence satisfaction, 49% in autonomy satisfaction, 26% in relatedness satisfaction, 49% in game continuance, and 11% in game usage. We suppose that 11% may be adequate, as game usage may be easily affected by schoolwork or workplace and family responsibilities. Moreover, the well-known phenomenon of the intentionsingle bondbehavior gap predicts a high discount in transforming intention to behavior (Fennis et al., 2011).",0.00%,29.49%,69.37%,0.00%,0.00%
141,22.jpg," The HVAC designer is prompted with a question about the exam. The prompt is generated by the LLM model, which is trained on a dataset of questions and answers about the exam. The HVAC designer then answers the question. The answer is evaluated by the LLM model, which provides feedback to the HVAC designer. The HVAC designer can then repeat the process up to 5 times.","We performed the usual bootstrapping process, i.e., 5000 resamplings at the typical significance level of.05 (Nusair et al., 2024). Although not all the path coefficients in our structural model have significant coefficients, the bootstrapping results indicate that all the mediations are significant. This is reasonable, as the bootstrapping method is designed to test the interactions of the path coefficients. Therefore, a single large path coefficient can result in a significant interaction among path coefficients. All the mediation coefficients are significant, justifying the importance of the chosen mediators in our model. Moreover, our model shows that most (but not all) paths have significant coefficients, giving game makers useful insights into game achievability and game immersibility, but not focusing on game creatability.",36.00%,81.36%,81.77%,21.84%,86.39%
142,220.jpg," The diagram shows the process of recognizing workarounds, assessing their impact, and choosing a brokering action.

The first step is to recognize that a workaround exists. This can be done by passively detecting workarounds, which means that the broker is not actively looking for workarounds but may become aware of them through other means, such as complaints from users. The broker can also actively sense workarounds, which means that they are actively looking for them.

Once the broker is aware of a workaround, they need to assess its impact on the network. This can be done by considering the following factors:

* **Exchange risk:** The risk that the workaround will be discovered and the broker will be held liable.
* **Information load:** The amount of information that the broker needs to process in order to understand the workaround.
* **Limited brokering capacity:** The broker's ability to handle the additional work that the workaround creates.
* **Political power:** The broker's ability to influence the people who are involved in the workaround.

Based on the assessment of the workaround's impact, the broker can then choose a brokering action. The three options are:

* **Protect and regain formal brokerage:** The broker tries to eliminate the workaround and restore formal brokerage.
* **Connect and maintain no brokering role:** The broker tries to connect the people who are involved in the workaround and maintain a no brokering role.
* **Passively detect and maintain no brokering role:** The broker does not actively look for workarounds, but if they become aware of one, they do not take any action.

The choice of brokering action will depend on the specific situation.","After the measurement model?s fit and validity were established, we moved to the path testing of our research model. The latent constructs were maintained in Amos instead of creating imputed sum constructs. The variances explained (R2) by the model in the dependent variables were 40 % for perceived organizational value of XR, 46 % for expected employee resistance to XR, and 44 % for organizational XR adoption intention. The overall hypothesis testing results can be seen in Fig. 2.",39.78%,66.05%,80.02%,38.95%,78.11%
143,221.jpg," The figure shows the hypothesized relationships among the study variables. The hypothesized relationships are:
H1: Privacy concerns have a negative effect on consumers’ SSSC sustainability.
H2: Disclosure intentions have a positive effect on consumers’ SSSC sustainability.
H3: Consumers’ SSSC sustainability has a positive effect on their purchase intentions.
H4: Privacy concerns have a negative effect on consumers’ disclosure intentions.
H5: Human safety concerns have a positive effect on consumers’ SSSC sustainability.
H6a: The negative relationship between privacy concerns and SSSC sustainability is weaker for consumers with higher levels of environmental performance.
H6b: The negative relationship between privacy concerns and SSSC sustainability is weaker for consumers with higher levels of fuel and system efficiency.
H7: Government subsidies have a positive effect on consumers’ SSSC sustainability.
H8: Age has a positive effect on consumers’ SSSC sustainability.","The analysis proceeded by (1) identifying and grouping relevant quotes relating to the examined variables and their relationships, (2) deriving descriptive informant-driven first-level concepts based on these quotes, and (3) developing more theoretical second-order themes (i.e., adoption manifestations and conditions affecting the relationships between the variables) from these initial descriptive concepts. The outcome of this analysis is presented in the form of a data structure (Fig. 3). The findings are also finally integrated with the earlier quantitative results (Fig. 4). Further evidence and quotes from the interviews justifying the development of the themes can be found in the data tables (see Appendix D).",18.97%,63.01%,81.22%,41.79%,73.32%
144,222.jpg," The figure shows the relationships between privacy concerns, sustainability, monetary incentives, and disclosure intentions. There are two types of privacy trade-offs: intuitive privacy trade-offs and external privacy trade-offs. Intuitive privacy trade-offs are the trade-offs that people make between their privacy and other factors, such as sustainability or monetary incentives. External privacy trade-offs are the trade-offs that people make between their privacy and the privacy of others.

The figure shows that privacy concerns are a key factor in people's decisions about whether or not to disclose their personal information. However, the impact of privacy concerns on disclosure intentions is moderated by the presence of monetary incentives and sustainability concerns. When monetary incentives are high, people are more likely to disclose their personal information, even if they have privacy concerns. Similarly, when sustainability concerns are high, people are more likely to disclose their personal information, even if they have privacy concerns.

The figure also shows that the type of privacy trade-off that people make is influenced by the context in which they are making the decision. For example, people are more likely to make intuitive privacy trade-offs when they are making decisions about their own personal information. However, they are more likely to make external privacy trade-offs when they are making decisions about the personal information of others.","In total, we received 213 complete responses, which were screened in multiple steps, resulting in 206 valid responses. We excluded 1 response from South Africa, as it was from outside of Europe, and removed 6 responses for inattentive responding (< 0.5 sd in the responses) or for responding at a speed that would be impossible to do attentively (< 5 min). The sample was then examined for missing data. We found two missing values for USE_3 (see Appendix C, Table C1), which were imputed with the median value of the item. Finally, we screened the remaining responses that failed the attention-trap question. We found sufficient variance in their answers, and the respondents took a sufficiently long enough time (> 10 min) to complete the survey. Demographic information about the respondents (n = 206) can be found in Table 1. As can be seen from the table, the survey responses were collected from top management (61), middle management (65), lower management (51), and experts (29). Table 2 depicts information about the companies of the respondents.",20.15%,65.07%,80.27%,43.21%,78.04%
145,223.jpg," This is a diagram of a between-subjects experiment with a 3x2 factorial design. The independent variables are brand (Amazon, Ford, Walmart, Nio) and sustainability (low, high). The dependent variable is purchase intention.

Participants will be randomly assigned to one of six conditions:
1. Control
2. Walmart (low sustainability)
3. Walmart (high sustainability)
4. Ford (low sustainability)
5. Ford (high sustainability)
6. Nio (low sustainability)
7. Nio (high sustainability)

In the first step, participants will be asked to provide informed consent. Then, they will be asked to complete a demographics survey. Next, they will be given a presurvey to measure their attitudes toward sustainability. After this, they will be provided with instructions and scenarios. Then, they will be asked to complete a manipulation check. Finally, they will be given a post-experimental survey to measure their purchase intention.","In total, we received 213 complete responses, which were screened in multiple steps, resulting in 206 valid responses. We excluded 1 response from South Africa, as it was from outside of Europe, and removed 6 responses for inattentive responding (< 0.5 sd in the responses) or for responding at a speed that would be impossible to do attentively (< 5 min). The sample was then examined for missing data. We found two missing values for USE_3 (see Appendix C, Table C1), which were imputed with the median value of the item. Finally, we screened the remaining responses that failed the attention-trap question. We found sufficient variance in their answers, and the respondents took a sufficiently long enough time (> 10 min) to complete the survey. Demographic information about the respondents (n = 206) can be found in Table 1. As can be seen from the table, the survey responses were collected from top management (61), middle management (65), lower management (51), and experts (29). Table 2 depicts information about the companies of the respondents.",14.85%,94.86%,81.28%,33.36%,76.11%
146,224.jpg," The figure shows the relationships among variables in the structural equation model. The model has three latent variables: perceived control (PC), general self-efficacy (GS), and depression (DI), and one observed variable: positive affect (PA). The model shows that PC has a positive effect on GS (β = 0.20, p < 0.05) and a negative effect on DI (β = -0.26, p < 0.05). GS has a positive effect on PA (β = 0.60, p < 0.05). DI has a negative effect on PA (β = -0.38, p < 0.05). The model also shows that there is a positive correlation between PC and PA (β = 0.34, p < 0.05).","We then proceeded with the CFA by evaluating the fully correlated measurement model. As the first step in assessing the indicator and construct validities, we examined the standardized item loadings for the constructs. All loadings were statistically significant (p < 0.001) and above the recommended 0.707 threshold (Fornell & Larcker, 1981), except for one RESI item, which had a loading of 0.689. Thus, this item was dropped from subsequent analyses. The constructs, items, and their means, standard deviations, and standardized factor loadings are presented in Appendix C (Table C1). Next, we used the Master Validity plug-in of Gaskin et al. (2019) to analyze the discriminant validity of our measurement model (Table 3). According to Fornell and Larcker (1981), the composite reliability (CR) value of all constructs should be above 0.7, the average variance extracted (AVE) should be above 0.5 and larger than the maximum shared variance (MSV), and the square root of each AVE should be larger than all other correlations with the other variables. The CR, AVE, MSV, and square root of the AVE (bolded in diagonal) are reported in Table 3. As can be seen, our data fit all the aforementioned criteria, indicating sufficient convergent and discriminant validity for our model.",24.44%,86.17%,82.54%,17.69%,79.61%
147,225.jpg," The figure shows the moderated mediation model. In this model, the independent variable is privacy concerns, the dependent variable is purchase intentions, and the mediator is disclosure intentions. The relationship between privacy concerns and purchase intentions is moderated by human safety and disclosure intentions.","We opted to evaluate the model fit for the measurement and path models using the CFI, SRMR, and RMSEA measures. This is in line with the recommendations of Hair et al. (2014), who suggested that model fit should be evaluated with at least one absolute fit measure (e.g., SRMR and RMSEA) and one incremental fit index (e.g., CFI). The suggested cutoffs for these fit measures are = 0.95 for CFI, = 0.08 for SRMR, and = 0.08 for RMSEA, along with > 0.05 for its PClose (Hair et al., 2014). As shown in Table 4, the measurement model?s values were excellent for all of these fit indices. The model?s normed chi-square (?2/df) was 1.515, which falls within the suggested range of 1?3 (Hair et al., 2014). The chi-square test was statistically significant (p < 0.05), indicating poor fit with the data; however, this is common with complex models and larger sample sizes (Schermelleh-Engel et al., 2003). Moreover, Hair et al. (2014) recommend that this measure should not be examined independently but in the context of other model fit measures. As all the other model fit indices were excellent, we can conclude that the measurement model fit the data well. After we moved on to the hypothesis-testing phase, we also evaluated the path model?s fit. Again, the measures were still excellent except for CFI (0.946), which was still close to excellent fit, but within acceptable range (= 0.9) nonetheless.",28.72%,69.34%,80.94%,0.63%,80.72%
148,226.jpg," The figure shows the structural equation model (SEM) for the effects of privacy concerns and environmental performance on purchase intentions, mediated by disclosure intentions. The model shows that privacy concerns have a negative effect on disclosure intentions, which in turn has a positive effect on purchase intentions. Additionally, environmental performance has a positive effect on disclosure intentions, which in turn has a positive effect on purchase intentions. Finally, the model shows that the effect of privacy concerns on purchase intentions is moderated by environmental performance. Specifically, the negative effect of privacy concerns on purchase intentions is weaker when environmental performance is high.","Four out of twelve of the paths were significant at the p < 0.001 level, two at the p < 0.01 level, and one at the p < 0.05 level. Seven out of the twelve hypotheses were thus supported (see Table 5). For the statistically significant paths, the effect directions (positive or negative) were as hypothesized in the research model (Fig. 1). Thus, the research model had overall empirical support.",23.41%,72.42%,82.03%,41.83%,74.34%
149,227.jpg," The figure shows the relationships among privacy concerns, fuel and system efficiency, disclosure intentions, and purchase intentions. There are two types of mediation effects: moderated mediation and conditional indirect effects. The details are as follows:

1. The relationship between privacy concerns and purchase intentions is mediated by disclosure intentions.
2. The relationship between fuel and system efficiency and purchase intentions is mediated by disclosure intentions.
3. The relationship between privacy concerns and purchase intentions is moderated by fuel and system efficiency.
4. The relationship between fuel and system efficiency and purchase intentions is moderated by privacy concerns.","Besides testing for the direct effects of each antecedent, we also carried out additional mediation analysis by testing for the indirect effects of the six TOE-based antecedents on organizational XR adoption intention via the perceived organizational value of XR and expected employee resistance to XR constructs. In addition, we tested whether the effect of expected employee resistance to XR on organizational XR adoption intention was mediated via the perceived organizational value of XR. This analysis was carried out using the latent mediation estimand and the Indirect Effects plugin created by Gaskin et al. (2020). The statistically significant paths are presented in Table 6. Mimetic pressure?s effect on organizational XR adoption intention was strongly mediated via the perceived organizational value of XR (ß = 0.206; p < 0.001). Organizational support (ß = 0.088; p < 0.01), employee technology use skills (ß = 0.069; p < 0.05), and trialability (ß = 0.047; p < 0.05) had a positive mediated effect on organizational XR adoption intention via expected employee resistance to XR. Other indirect effects were statistically insignificant (p > 0.05).",17.02%,78.45%,82.22%,18.64%,73.42%
150,228.jpg," Table 1 shows the descriptive statistics of the variables used in the analysis. The variables are privacy concerns (PC), fatal accident rate (FA), CO2 emissions (EV), fuel and system efficiency (FE), and government subsidy (GS). The mean value of PC is 5.22 for the control group and 4.22 for the low-level group. The mean value of FA is 5.23 for the control group and 3.91 for the low-level group. The mean value of EV is 3.20 for the control group and 2.02 for the low-level group. The mean value of FE is 4.96 for the control group and 2.88 for the low-level group. The mean value of GS is 3.92 for the control group and 2.24 for the low-level group.","We can differentiate between potentialities (encompassing both risks and utilities) seen as hypotheticals, and their actual outcomes (harms and benefits). Numerous aspects underpin both categories (refer to Fig. 1). Pinning down causality from potentialities to outcomes is notoriously challenging, as causality can never be proven, but only rejected (Pearl, 2009) (for reasons similar to those widely accepted for hypothesis-testing (Popper, 2002)). Not every risk results in harm, and not every harm traces back to a known risk. Our study, aiming to assess the frequency of ?good? and ?bad? recommendations from algorithms, primarily addresses the potentialities of risks and utilities presented on the left-side of Fig. 1.",23.17%,85.21%,79.72%,41.67%,80.89%
151,229.jpg," Table 1. Constructs, sample items, and Cronbach's alpha values","Table 1 displays socially accepted risk profiles (for sources, see S.I.2. Comparison data). In food safety, the U.S. Food and Drug Administration (FDA) maintains a Food Defect Levels Handbook, which specifies that it accepts around 7% of defect samples (mainly mold and insect-infestations). About 15% of U.S. citizens contract foodborne illnesses annually, while severe harm is much less common (some 3000 die each year). The U.S. Food Safety and Inspection Service (FSIS) accepts 7.5% of salmonella-positive chicken carcasses and ground beef samples, with harm levels below 0.5% and around 420 U.S. deaths annually. General consumer goods have a different profile. Tracking the number of faulty products sold per year, the U.S. Dept. of Commerce only recalls 0.1%. Their accumulation and frequent use hurts 2%- 4% of U.S. citizens annually. In between these extremes is the risk profile of sports. In soccer, 1 in 32 ball possessions leads to a potentially dangerous foul (3.1%), and 1.5% of U.S. players end up injured. Another extreme is the risk profile of cigarettes. While each cigarette is risky and severe harm levels are notably higher (some 10% die from lung cancer or cardiovascular disease), it is surprising to many that these trackable risks are not life-threatening to some 90% of smokers.",8.12%,76.37%,81.64%,0.00%,84.44%
152,23.jpg," This is a table that shows the different capabilities, scenarios, tasks, and questions that are used in the training of a mechanical engineer. The table is divided into four sections: Recall, Analysis, Design Calculation, and Application. Each section contains a list of tasks that are related to the corresponding capability. For example, the Recall section contains tasks such as ""Review shop drawings and equipment submittals"" and ""What does a system manual typically include?"". The Analysis section contains tasks such as ""Create HVAC zoning and sensor locations"" and ""What's the MOST accurate statement about the task of sequencing heating and cooling?"". The Design Calculation section contains tasks such as ""Calculate building heat loss/gain"" and ""Calculate the total pressure of a SW5I centrifugal fan, given the fan static pressure (48.80 in. water or 1194 Pa) and outlet velocity (2800 fpm or 14.2 m/s), for standard air at a specified speed."". The Application section contains tasks such as ""Collaborate with acoustical engineer"" and ""To maintain NC-40 office noise levels, an HVAC designer, collaborating with an acoustical engineer, should use a duct silencer due to excessive AHU supply fan noise. The BEST position for the duct silencer is"".","As our analysis aims at bridging measures from computer science, information science, medicine, and the psychological and social sciences, we prioritized expected values and simple conditional probabilities over higher-order meta-analytic statistics (Higgins et al., 2019, Petticrew and Roberts, 2008, Uman, 2011). We still achieve the meta-analytic goal of systematically synthesizing independent studies to calculate an overall effect (Egger and Smith, 1997, Shorten and Shorten, 2013). Table 2 presents the simple framework that conditions algorithmic recommendation output on different kinds of input. While all included studies (N = 151) report the percentage of ?bad? recommendations (first column: X%, Y%, or Z%), we only obtain data on ?good? recommendations for 62 studies (see Table 3). This means that we can distinguish between ?bad? and ?not bad? recommendations for all 151 audits (which is what we will do for most of our analyses), and distinguish between ?good?, ?other/neutral?, and ?bad? for a subgroup of studies (see section ?Recommending utility content?).",32.02%,82.79%,78.85%,43.64%,83.05%
153,230.jpg," The image shows the results of a mediation analysis. The analysis was conducted using SPSS and AMOS. The results show that there is a significant indirect effect of privacy concerns on purchase intentions, mediated by disclosure intentions. This means that privacy concerns lead to disclosure intentions, which in turn lead to purchase intentions. The results also show that there is a significant direct effect of privacy concerns on purchase intentions. This means that privacy concerns also have a direct effect on purchase intentions, in addition to the indirect effect mediated by disclosure intentions.","As our analysis aims at bridging measures from computer science, information science, medicine, and the psychological and social sciences, we prioritized expected values and simple conditional probabilities over higher-order meta-analytic statistics (Higgins et al., 2019, Petticrew and Roberts, 2008, Uman, 2011). We still achieve the meta-analytic goal of systematically synthesizing independent studies to calculate an overall effect (Egger and Smith, 1997, Shorten and Shorten, 2013). Table 2 presents the simple framework that conditions algorithmic recommendation output on different kinds of input. While all included studies (N = 151) report the percentage of ?bad? recommendations (first column: X%, Y%, or Z%), we only obtain data on ?good? recommendations for 62 studies (see Table 3). This means that we can distinguish between ?bad? and ?not bad? recommendations for all 151 audits (which is what we will do for most of our analyses), and distinguish between ?good?, ?other/neutral?, and ?bad? for a subgroup of studies (see section ?Recommending utility content?).",15.76%,76.31%,80.55%,22.87%,81.27%
154,231.jpg," The table shows the results of a path analysis. The path analysis was used to test a model of the relationships between government subsidies (GS), privacy concerns (PC), disclosure intentions (DI), and purchase intentions (PI). The results show that there is a significant positive relationship between GS and DI (β = 0.525, p < 0.001), a significant positive relationship between DI and PI (β = 0.399, p < 0.001), and a significant negative relationship between PC and DI (β = -0.297, p < 0.001). The results also show that there is no significant relationship between PC and PI (β = -0.047, p = 0.296). These results support hypotheses H7, H8, and H1, but do not support hypothesis H2.","To date, the agency theory has been mostly applied to explain the intentions and motivations for BCT adoption in the supply chain, and consequently, has been categorized as a theory in the antecedents group (Zhu et al., 2022). Treiblmaier (2018) postulated BCT-induced transformations in the principal-agent relationship as follows (see Fig. 1). Without blockchain (Fig. 1, left), the agent is the only one which has access to information from external sources, such as customers, suppliers, or partners. The agent, guided by its own self-interests, processes this information, and then acts on behalf of the principal. This asymmetry of information requires the principal's trust in the agent. By integrating the blockchain into this relationship (Fig. 1, right), information flows become transparent and accessible to the principal, reducing the principal's required trust in the agent. Additionally, smart contracts can mitigate issues of information asymmetry through better control of contract execution and supervision (Chang & Chen, 2020). As a result, the interactions and relations among supply chain participants may be improved.",26.74%,77.89%,79.98%,28.48%,76.68%
155,232.jpg," The table shows the variable names, descriptions, means, standard deviations, minimums, and maximums for the variables used in the analysis.","We can argue that BCT, by virtue of its underlying characteristics, can reduce the asymmetry of information as well as opportunistic behaviors of agents. This result is in line with prior works in the research stream on BCT in SCM (Grosse et al., 2021, Treiblmaier, 2018, Zhu et al., 2022) as well as the one related to BCT adoption in IS (Upadhyay, 2020). However, as our findings do not concur with the blockchain-induced transformations postulated by Treiblmaier (2018), especially when it comes to trust (see Fig. 2), we initiate a discussion about the impacts on principal-agent relationships. Our study indicates that BCT only minimize the need for trust of the principal towards the agents. This result is consistent with Brookbanks and Parry (2022), who highlighted that BCT does not remove the importance of trust in established supply chains. This can be explained by the complexity of relationships between supply chain members, which implies that agents' behaviors cannot be fully automatized. In line with Tönnissen and Teuteberg (2020), our results do not lead to any examples of disintermediation as a whole, but key activities of agents have been disintermediated. This suggests that BCT are more complements than substitutes of inter-organizational trust in established supply chains. Moreover, the need for trust in SCM could have shape the extensive use of permissioned blockchains (Vadgama & Tasca, 2021), which place trust in known participants of the network rather than into the technology itself. This brings to the fore the trustless myth of blockchain (Meyers & Keymolen, 2023). Paradoxically, we note that the control mechanisms inherent in BCT can decrease the agent's trust in the principal (Fig. 2). We explain in the following this finding by connecting the unique features of BCT to information asymmetry and opportunistic behaviors.",32.89%,87.02%,79.30%,0.00%,84.85%
156,233.jpg," The correlation matrix shows the correlation coefficients between the different variables. The correlation coefficient is a measure of the strength of the linear relationship between two variables. It can range from -1 to 1, with a value of 1 indicating a perfect positive correlation, a value of -1 indicating a perfect negative correlation, and a value of 0 indicating no correlation.

In this case, the correlation matrix shows that there are some strong positive correlations between some of the variables. For example, the correlation coefficient between the variables ""LogDailyInvestors"" and ""Twitter"" is 0.158, which indicates that there is a positive relationship between the two variables. This means that as the value of the variable ""LogDailyInvestors"" increases, the value of the variable ""Twitter"" also tends to increase.

There are also some strong negative correlations between some of the variables. For example, the correlation coefficient between the variables ""LogDailyInvestors"" and ""Days Available"" is -0.430, which indicates that there is a negative relationship between the two variables. This means that as the value of the variable ""LogDailyInvestors"" increases, the value of the variable ""Days Available"" tends to decrease.

It is important to note that the correlation matrix only shows the strength of the linear relationship between two variables. It does not show the direction of the relationship or the causality between the variables. For example, the correlation matrix shows that there is a positive correlation between the variables ""LogDailyInvestors"" and ""Twitter"", but it does not show whether the relationship is driven by ""LogDailyInvestors"" influencing ""Twitter"" or by ""Twitter"" influencing ""LogDailyInvestors"".","First, we found 920 articles from the three databases based on search keywords, and after extracting only those from journals and conference papers, we were left with 563 articles. Second, we removed the duplicated studies, and researchers reviewed each journal, and conference proceedings that were recognized as potentially relevant and eliminated the others, therefore, 442 articles were removed. At the end of the next phase, the papers that did not fit the criteria were extracted, and 68 articles were obtained. Based on the PRISMA statement, Fig. 1 describes the methodology used to conduct this systematic review. PRISMA stands for ""preferred reporting items for systematic literature review and meta-analysis"" (Moher et al., 2009).",32.70%,76.87%,79.51%,39.56%,77.96%
157,234.jpg," Table 4 shows the results of the main regressions. In unreported results, we find that the number of unique visitors to the website and the number of shares on social media are not significant in any specification. We do not report these results in the interest of brevity. In unreported results, we find that the number of unique visitors to the website and the number of shares on social media are not significant in any specification. We do not report these results in the interest of brevity.

Columns 1 and 2 show the results of a regression of the natural logarithm of daily unique visitors to the campaign website on the natural logarithm of the number of investors and the binary treatment variable indicating whether the observation is from the treatment group. We find that the number of investors is positively and significantly associated with the number of unique visitors in both models. The estimated coefficient on the treatment variable in column 2 is negative and significant, suggesting that the treatment group received fewer unique visitors than the control group. However, this result is not robust to controlling for time-varying and campaign-specific controls in columns 3-6.

In columns 3 and 4, we add time-varying controls to the regression. We find that the number of investors remains positively and significantly associated with the number of unique visitors, and the treatment variable is no longer significant. We also find that the number of unique visitors is positively and significantly associated with the number of Facebook discussions and negatively and significantly associated with the number of Twitter discussions. These results suggest that the campaign was more successful in attracting unique visitors through Facebook than through Twitter.

In columns 5 and 6, we add campaign-specific controls to the regression. We find that the number of investors remains positively and significantly associated with the number of unique visitors, and the treatment variable is no longer significant. We also find that the number of unique visitors is positively and significantly associated with the number of press releases and the number of television appearances. These results suggest that the campaign was more successful in attracting unique visitors through press releases and television appearances than through other campaign activities.","This results in a CSD loop, as illustrated in Fig. 1, where individuals engage with the mobile device to distract themselves from the real world or unwelcome emotional states, but where the act of using the mobile device can cause further negative outcomes (e.g. anxiety). However, increased appetite for distraction is insufficiently explained by rationalist cognitive processing theories commonly applied by researchers explaining drivers, process mechanisms, and outcomes.",29.24%,90.53%,79.98%,36.39%,78.44%
158,235.jpg," The table shows the descriptive statistics of the number of posts on Twitter, Facebook, and Discussion at the end of the first stage and at the end of the last stage. The results show that there is a significant increase in the number of posts on Twitter and Facebook from the first stage to the last stage. There is also a significant increase in the number of posts on Discussion from the first stage to the last stage. These results suggest that the intervention was effective in increasing the number of posts on social media.","From the initial 880 articles, Covidence removed duplicates (n = 273), after which two of the researchers screened the abstracts of the remaining articles (n = 607) to assess suitability. During this process the abstracts were examined based on the eligibility criteria as outlined in Fig. 2. From this process 101 articles were determined as eligible from the abstract and these articles were read in full by two researchers. The full text review showed 77 articles did not meet the eligibility criteria, resulting in the final sample of 23 articles (see Fig. 3).",42.76%,66.31%,82.41%,47.37%,83.06%
159,236.jpg," Table 4 shows the results of the negative binomial and OLS regressions. Because the dependent variable in the negative binomial regression is a count variable, exponentiated coefficients are reported. For ease of interpretation, the exponentiated coefficients are multiplied by 100. The results show that the availability of daily investor information has a negative impact on daily trading volume. The interaction between daily investor information and days available has a positive impact on daily trading volume. This suggests that the negative impact of daily investor information on daily trading volume is mitigated as the number of days available increases.

The control variables show that the number of days available, equity offered, management fees, and target have a positive impact on daily trading volume. In addition, the number of patents and the technology sector have a negative impact on daily trading volume. These findings are consistent with the previous literature.","To assist in better understanding this phenomenon, we draw on Cognitive-Affective Personality System (CAPS) theory (Mischel & Shoda, 1995), as a unifying theory to clarify our conjectures and develop research propositions that are encapsulated in our theoretical CSD framework (Fig. 5). In this framework we outline that the dynamic input of both external environmental stimuli and internally generated psychological states act as the key antecedents of CSD which lead to behavioral and psychological consequences for the customer. Socio-cultural and individual factors are embedded to condition these impacts. Although presented separately in our framework, the environmental stimuli and psychological states are considered interdependent components of a dynamic system.",24.61%,85.19%,81.32%,43.24%,82.44%
160,237.jpg, Table 6: First-stage and reduced-form estimates.,"Drawing from the research questions, a range of keywords were identified for the database searches, including ?mobile phone,? ?smart phone,? ?augmented reality,? ?distraction,? and ?multi-tasking.? In line with prior studies (Agarwal et al., 2019, Borges et al., 2021, Yan et al., 2021), Web of Science (WoS) and Scopus were identified as appropriate databases to obtain relevant and reliable journal articles across disciplines (see Table 1). To determine the eligibility of articles, inclusion and exclusion criteria were created in line with PRISMA guidelines (Massaro et al., 2016, Moher et al., 2009, Tranfield et al., 2003), see Fig. 2. Inclusion criteria included i) journal articles investigated mobile or AR technologies, and distraction or multi-tasking, involving customers (i.e., consumption context), ii) journal articles published in English, iii) journal articles published in high-ranking outlets (Scimago Q1 or Q2), iv) journal articles published since 2016 (to ensure the technology studied is up to date), v) journal articles empirical in nature, and vi) full-text versions accessible. The full exclusion criteria included i) publications merely mentioning distraction or multi-tasking, ii) publications where the focus was not on a customer-related experience, iii) publications in other than high-ranking journal articles, and iv) non-empirical research articles.",7.00%,69.83%,79.65%,0.00%,79.87%
161,238.jpg," Table 6 shows the results of the first-stage and last-stage models. Because the dependent variable in the first-stage model is binary, we use a probit model, while in the last stage, we use OLS regression since the dependent variable is continuous. We cluster standard errors at the campaign level. We find that the coefficient on the main independent variable LogLagDailyInvestors is negative and statistically significant in both the first-stage and last-stage models, suggesting that the greater the number of previous daily investors, the lower the probability of success and the lower the amount raised. Among the campaign-specific controls, we find that the number of days available for the campaign is positively associated with the probability of success but not with the amount raised. In contrast, the equity offered is negatively associated with the probability of success but positively associated with the amount raised. These results suggest that campaigns with more days available have a higher chance of reaching their goal, while campaigns that offer more equity are less likely to succeed but raise more money when they do succeed. The remaining campaign-specific controls are not statistically significant.","To test the model's hypotheses, a bootstrapping method, using SmartPLS with 5000 subsamples, was used (Hair et al., 2011). R² values (coefficient of determination) are a measure of the predictive ability of a structural model. R² values higher than 0.25, as a rule of thumb, indicate a model has moderate explanatory power (Hair et al., 2011). The results of the tests of the hypotheses and the R² values are shown in Fig. 2.",39.50%,78.55%,82.48%,41.18%,78.71%
162,239.jpg," The figure shows the hypothesized relationships between the three types of perceived value (informational, entertainment, and social) and attitude toward digitally supported museum visits. It also shows the hypothesized relationship between attitude toward digitally supported museum visits and museum visitor continuance intention to use social media. Finally, it shows the hypothesized moderating effects of age and gender on the relationships between perceived value, attitude, and continuance intention.","Three focus groups were conducted (between 5 and 8 participants per session) to examine specific issues (Krueger, 2014). The sessions lasted between 60 and 90 min. The participants were recruited in Spain, following a non-probabilistic, purposive approach. The composition of a focus group should have a certain degree of homogeneity to avoid huge differences in opinion emerging, but it should also be diverse enough to promote discussion and generate useful information (Phillippi & Lauderdale, 2018). As prior knowledge of cultural events can influence participants? perceptions and evaluation of an experience (Lobuono et al., 2016), we selected people with similar levels of knowledge about the cultural event under consideration, but with different characteristics in terms of age, gender, and willingness to adopt new technologies. The focus groups were run until the saturation criterion was met (Malterud et al., 2016). The composition of the focus groups is shown in Table 1.",28.66%,88.50%,82.11%,14.19%,76.86%
163,24.jpg," The first image is an astronaut riding a horse in a photorealistic style. The second image is a group of astronauts on the moon in the 1980s. The first image is more realistic, while the second image is more stylized.","Analyses of the reliability and convergent validity of the scales were conducted using SmartPLS 4.0 software. The factorial loadings of the indicators exceeded the minimum recommended level of 0.70 (except one, see Appendix B; Hair et al., 2011). The composite reliability of the constructs and the average variance extracted (AVE) values were also higher than the recommended minimum levels (Hair et al., 2011) (see Appendix B). Discriminant validity was assessed based on the criteria of Fornell and Larcker (1981) and heterotrait-monotrait ratios (Kline, 2011), with both approaches returning satisfactory values (see Table 3).",27.74%,37.10%,80.47%,11.96%,86.85%
164,240.jpg," Study 1: Semi-structured interviews with museums professionals (N=25). Thematic analysis was conducted on the interview data using an open, axial, and selective coding approach.
Study 2: Intercept survey with museum visitors (N=110). Descriptive statistics were used to analyze the survey data.
Study 3: Online survey with museum visitors (N=260). Structural equation modeling was used to analyze the survey data.

The findings of the three studies suggest that social media is an important tool for museums to engage with visitors and enhance their experiences. Social media can be used to provide visitors with information about the museum, its collections, and its programs. It can also be used to create a sense of community among visitors and to encourage them to return to the museum. In addition, social media can be used to track visitor behavior and to measure the effectiveness of museum marketing campaigns.

The findings of this study have implications for museum professionals who are responsible for developing and implementing social media strategies. The study provides evidence that social media can be an effective tool for museums to achieve their goals. However, the study also suggests that museums need to be strategic in their use of social media and that they need to develop a clear plan for how they will use social media to achieve their goals.","As the research aims to advance the current academic knowledge on an under-debated issue ? i.e., how AI systems are twisted with public organizational actions and agents ?, we act on two levels. First, not viewing ?the world with a blank slate? (van de Ven et al., 2015, p. 2), we adopted as a mode of inquiry abductive reasoning (Timmermans & Tavory, 2012), as this logic appears to be particularly suitable to shed light on the uncertain, dynamic, and interconnected phenomena (Sætre & Van De Ven, 2021). Thus, we built on the TOE framework (DePietro et al., 1990) to revise the empirical phenomenon (Timmermans & Tavory, 2012), highlighting the factors associated with the implementation of AI. This framework allows us to cast light on the complex system of actors, actions, and interactions in which the technological artifact is entwined (Barley, 2020, Majchrzak et al., 2016), shedding lights on previously undistinguishable features. Fig. 2 illustrates the coding structure for the first round of interviews.",33.27%,81.75%,80.50%,44.04%,82.34%
165,241.jpg," The image shows the relationships between the actor, technology affordances, and outcomes in the context of social network services (SNS). The actor is the user of the SNS, and the technology affordances are the features of the SNS that enable the user to interact with the service and other users. The outcomes are the results of the interaction between the actor and the technology affordances. The outcomes can be positive or negative, and they can have a variety of effects on the actor, including their self-esteem, their relationships with others, and their overall well-being. The context characteristics moderate the relationship between the actor, technology affordances, and outcomes. The context characteristics include the social and cultural norms that influence the use of SNS, as well as the individual differences of the actor.","Moreover, going through the last step described by Timmermans and Tavory (2012) ? alternative casing ? the TACT (Majchrzak & Markus, 2013) has been adopted as additional theoretical lens to complement the previous findings and deeply understand how AI is entwined with organizational agents, enabling, or obstructing, novel actions (Treem & Leonardi, 2012). Finally, we observed the presence of recurrent patterns in affordances and constraints, based on the features of the cases analyzed. In other words, we observed if and which features of the cases (type of AI, status of the project, geographical extent, etc.) may influence the presence or absence of certain affordances and constraints. The initial relationships were then refined via replication logic ? frequently revising each case to compare and verify the occurrence of specific constructs, relationships, and logics. Fig. 3 depicts the coding scheme of the second round of data gathered.",61.60%,92.51%,82.39%,47.79%,85.80%
166,242.jpg," The figure shows the structural equation model (SEM) results. The model explains 31.9% of the variance in satisfaction, 9.2% of the variance in positive affect, and 5.3% of the variance in feeling of control. All the hypothesized paths are significant (p < 0.05). The results support the mediating role of social connectedness and feeling of control in the relationship between affordances actualization and satisfaction.","This dual perspective allows us to shape a conceptual framework (Fig. 4) that links social organizing to the specificities of AI (Faraj & Azad, 2012). The framework offers a novel perspective and enriches the current debate on the implementation of AI, shedding light on the relationships between the contexts observed and explaining the complex and multifaceted factors and features behind AI implementation. As reported in the framework, our data show that AI implementation requires several factors to be present in a PSO. These factors can be divided into common implementation factors, which AI shares with any type of technological implementation, and AI-related factors, which are instead specific to AI. These latter factors afford or constrain new actions for PSOs. Finally, these relationships are mutated by a set of case-specific features affecting the entire implementation process, such as the type of AI technology and the user of the AI system.",40.55%,74.77%,82.02%,14.90%,79.65%
167,243.jpg," **Self-Oriented Affordances**

Item | OAA | SAA | Median | SD |
------- | -------- | -------- | -------- | -------- |
1 | 4.00 | 4.66 | 5.00 | 0.54 |
2 | 4.00 | 4.33 | 4.00 | 0.67 |
3 | 3.67 | 3.74 | 4.00 | 1.16 |
4 | 3.42 | 2.84 | 3.00 | 1.23 |
5 | 2.42 | 2.23 | 2.00 | 1.19 |

**Other-Oriented Affordances**

Item | OAA | SAA | Median | SD |
------- | -------- | -------- | -------- | -------- |
1 | 3.55 | 3.23 | 4.00 | 1.09 |
2 | 3.32 | 3.18 | 4.00 | 1.17 |
3 | 4.18 | 3.56 | 4.00 | 0.83 |
4 | 3.35 | 2.68 | 4.00 | 1.18 |
5 | 3.48 | 3.48 | 3.50 | 1.20 |
6 | 3.22 | 3.22 | 4.00 | 1.26 |
7 | 3.84 | 3.26 | 4.00 | 1.26 |",H1: The dissemination and influence of health information in mobile social media are positively correlated with the information's quality. H2: The dissemination and influence of mobile social media are positively connected with its interactiveness. H3: The dissemination and influence of mobile social media are positively correlated with its dependability. H4: The dissemination and influence of health information are positively correlated with the accuracy of its content. H5: The dissemination and influence of health information are correlated with the information's content value. The conceptual model of our study is presented in Fig. 1.,0.00%,10.53%,76.50%,0.00%,28.11%
168,244.jpg," The table shows the weights and variance inflation factors (VIFs) for the items in the others-oriented AA and self-oriented AA scales. The weights are used to calculate the scores for each scale, and the VIFs indicate the amount of collinearity between the items.

For the others-oriented AA scale, the items with the highest weights are OAA2 (weight = 0.415) and OAA7 (weight = 0.540). This indicates that these items are the most important in determining the overall score for the scale. The VIFs for the items in this scale are all below 2.0, which indicates that there is no significant collinearity between the items.

For the self-oriented AA scale, the items with the highest weights are SAA3 (weight = 0.478) and SAA5 (weight = 0.617). This indicates that these items are the most important in determining the overall score for the scale. The VIFs for the items in this scale are all below 2.0, which indicates that there is no significant collinearity between the items.

Overall, the weights and VIFs for the items in the others-oriented AA and self-oriented AA scales indicate that the scales are reliable and valid.","(1) Filtering of indicators : To ensure the scientific rigor and validity of our evaluation system, we recruited 684 mobile social media users who met specific inclusion criteria. These criteria included: (1) having utilized mobile social media, (2) being concerned about health information, (3) taking part freely, (4) being under the age of 18, and (5) having strong reading and comprehension abilities independently. The questionnaires were displayed on the ?Wenjuanxing? online professional survey platform, convenience sampling was employed, and we utilized popular platforms such as QQ and WeChat to distribute the questionnaires. Users of general mobile social media for health information made up the distribution population. To prevent duplicate responses, each user was limited to completing the questionnaire only once. The flow of indicator screening is shown in Fig. 2:",33.53%,85.72%,79.45%,40.04%,78.82%
169,245.jpg," The figure above illustrates the process of users' continuance and discontinuance of STSFs. Users may start using a STSF due to various reasons, such as curiosity, recommendations from friends, or the need to stay connected with others. Once they start using a STSF, they may continue using it if they find it useful and enjoyable. However, if they do not find the STSF to be useful or enjoyable, they may discontinue use. There are two possible states after discontinuance. The user may either stay away from STSFs altogether or switch to other platforms.","First, based on a preliminary survey of 684 valid questionnaires on the importance rating of user perception indicators, the weights of the first- and second-level indicators were calculated using the factor analysis method, and the importance ranking of the second-level indicators was obtained. The ranking reflects the advantages of user subjective weighting in the ranking of the second-level indicators; Then, based on empirical research sample data, the entropy method is used to calculate the objective weights of secondary indicators, and the objective weights of secondary indicators are used as the basis for calculating the importance ratio between indicators; Finally, the G1 combination weighting method was used to calculate the combination weights of the mobile social media health information quality evaluation index system [41,69,70]. Implement combination weighting, as shown in Fig. 3.",24.29%,80.27%,81.91%,29.30%,82.70%
170,246.jpg," The figure shows the relationships between expectancy violations, responses to expectancy violations, and switching behavior.","Based on the responses to the questionnaire on the importance of users' mobile social media health information quality evaluation indicators, factor analysis was conducted using SPSS, and the weights of each question item indicator and public factor were calculated using the variance interpretation rate. The entropy method was chosen to calculate the objective weights of mobile social media health information quality evaluation indicators based on empirical data of mobile social media (represented by WeChat official accounts), and the combination weights were calculated using the entropy-modified G1 combination assignment method (see Table 4 and Fig. 4).",21.17%,81.06%,83.62%,0.27%,71.72%
171,247.jpg," The figure shows the research model of the study. The model consists of three parts:
1. The antecedents of intermittent discontinuance: This part of the model examines the factors that lead to intermittent discontinuance. The model proposes that negative expectancy violations in the current STSF and positive expectancy violations in the competitive STSF are the main antecedents of intermittent discontinuance. 
2. The mediating role of negative expectancy violations in competitive STSF: This part of the model examines the mediating role of negative expectancy violations in competitive STSF in the relationship between negative expectancy violations in the current STSF and intermittent discontinuance. The model proposes that negative expectancy violations in competitive STSF mediate the relationship between negative expectancy violations in the current STSF and intermittent discontinuance. 
3. The moderating role of mooring effects: This part of the model examines the moderating role of mooring effects in the relationship between negative expectancy violations in the current STSF and intermittent discontinuance. The model proposes that mooring effects moderate the relationship between negative expectancy violations in the current STSF and intermittent discontinuance.","In this study, exploratory factor analysis and confirmatory factor analysis were used to test the dimensions and items of the evaluation index system for structural validity, convergent validity and discriminant validity. The KMO value of the indicator system is 0.922 > 0.7, Bartlett's test: X2 is 2817.8, p < 0.001, which shows that the indicator system has good structural validity. Table 2 demonstrates the rotated factor loading coefficients after adjustment and deletion in this study. The four factors extracted were named by combining the connotation of each variable; factor 1 was the value of health information content, factor 2 was the reliability of mobile social media, factor 3 was the trustworthiness of health information content, and factor 4 was the interactivity of mobile social media. From the results of principal component extraction, it can be seen that the cumulative explained variance of the extracted 19 question items is 77.31 %, which indicates that the four factors extracted from the 19 question items have a better explanation for the original data.",45.16%,92.93%,80.82%,48.31%,79.68%
172,248.jpg," The figure shows the results of a study on the effects of expectancy violations on customer satisfaction and loyalty in the context of mobile service usage. The study used a structural equation modeling (SEM) approach to analyze the data. The model includes two second-order constructs: positive expectancy violations and negative expectancy violations. Positive expectancy violations are hypothesized to have a positive effect on customer satisfaction and loyalty, while negative expectancy violations are hypothesized to have a negative effect on customer satisfaction and loyalty. The results of the study support the hypotheses.","According to the four-factor model derived from the exploratory factor analysis, the validated factor analysis of the mobile social media health information quality evaluation scale was conducted on the survey questionnaire data using Amos23 software. The results showed (see Table 3) that the factor loadings of the dimensions of interactivity of mobile social media, reliability of mobile social media, the trustworthiness of health information content, and value of health information content ranged from 0.748 to 0.883, 0.781 to 0.908, 0.771 to 0.858, and 0.693 to 0.911, respectively, which were all greater than 0.6, indicating that each of their latent variables corresponding to the AVEs of each dimension were 0.6987, 0.7162, 0.6686, and 0.7118, all of which were greater than 0.6; the combined reliability CRs were 0.8737, 0.9095, 0.8896, and 0.9516, all of which were greater than 0.8, indicating ideal convergent validity.",35.98%,90.36%,81.84%,28.55%,80.37%
173,249.jpg, Table 1. Demographic characteristics of WeChat users (N = 380),"The final weight and the scores of the four dimensions of the WeChat official accounts' health information quality evaluation system were derived to give a quantitative evaluation of the current situation of WeChat official accounts' health information quality; see Table 5. The total average score of WeChat official accounts health information quality in this empirical study was 48.24, among which the score of the WeChat official accounts subject interaction dimension was 45.20, the score of the WeChat official accounts reliability dimension was 59.95, the score of the content credibility dimension was 28.32, and the score of the information content value dimension was 53.55. It is clear that the quality of health information on WeChat official accounts is generally low, especially the lowest score of content credibility, and the future improvement of health information quality on WeChat official accounts should be promoted from the aspect of content credibility.",16.08%,44.08%,83.21%,0.00%,66.06%
174,25.jpg," The timeline of the development of various NLP and CV models. NLP models are shown in orange, and CV models are shown in green. The vertical lines indicate the approximate time of the introduction of each model.",Fig. 1 shows the proposed conceptual model for the research hypotheses.,20.95%,82.85%,87.87%,36.94%,73.91%
175,250.jpg," All items show good standardized loadings, ranging from 0.827 to 0.970. All items also have good outer VIF values, ranging from 1.551 to 6.444. All items also have good AVE values, ranging from 0.763 to 0.950. All items also have good CR values, ranging from 0.906 to 0.970. All items also have good Cronbach's Alpha values, ranging from 0.826 to 0.954.","The choice of the number of hidden layers depends on the complexity of the problem. ANN models with one hidden layer (shallow ANNs) are sufficient to model any continuous function, whereas ANNs with two hidden layers can be used to model even discontinuous functions [111]. ANN models with two or more hidden layers are called deep ANN models, as they enable deep learning and modelling of more complex relationships ([98]; Kalinic et al., 2021), but they also require more data for training and testing. Although Kalinic et al. (2021) proved that, in the case of relatively simple research models, the introduction of a second hidden layer does not deliver any improvement in terms of accuracy, in our case, the high number of predictors and nonlinear relationships led us to select the deep learning ANN approach (Alharbi and Sohaib, 2020; [98]). This provided two hidden layers in the neural network and, thus, a greater degree of precision. We determined the number of neurons in hidden layers using simulation software?SPSS v20 [95,112]?and we used sigmoid as an activation function in both hidden and output layers [[50], [96]]. The deep learning ANN model is presented in Fig. 2.",2.62%,75.63%,79.08%,4.19%,71.87%
176,251.jpg," The correlation matrix shows the correlation coefficients between different variables. We can see that the correlation coefficient between ALAT and USAF is -0.135, which means that there is a negative correlation between ALAT and USAF.","To minimise the drop-out rate, we included information about the purpose of the research in the survey, together with a statement guaranteeing the anonymity of the respondents. In addition, we offered some small incentives, such as mugs and umbrellas featuring the university logo, and, at the end of the survey, awarded these to randomly selected participants. To reduce the occurrence of missing values, the participants were required to give their responses to all questions/statements before they could progress to the next page/end of the survey (otherwise, a notification would pop up). Regarding the sample characteristics, 51.5 % were women, 76 % of the total were under 45 years of age, 46.5 % had studied at the higher education level and 63.8 % were in employed work. Table 4 presents the sample characteristics.",20.01%,70.16%,81.84%,2.68%,74.42%
177,252.jpg," | USAF | TRAF | CODI |
|---|---|---|
| INOV | 1.151 |  |
| SOOV | 1.274 |  |
| SYOV | 1.342 |  |
| SUNC |  | 1.799 |
| TRAC |  | 1.799 |
| ALAT |  | 2.487 |
| PROC |  | 1.234 |
| USAF | 3.163 | 1.088 |","Prior to conducting further data analyses, we assessed the multivariate assumptions of normality, linearity, multicollinearity and homoscedasticity [98]. The results of the one-sample Kolmogorov-Smirnov test [99], presented in Table 5, indicate the absence of normal distribution [99], since all 2-tailed asymptotic significance values were 0.000?that is, less than 0.05 [82,96]. Hence, we opted for PLS-SEM in this study because it has been shown to be robust under conditions of non-normality [100].",0.00%,-1.50%,75.64%,0.00%,56.25%
178,253.jpg," The table shows the results of a PLS path model analysis. The model has three formative constructs: Intermittent Discontinuance (INDI), Short Break Discontinuance (SBDi), and Suspended Discontinuance (SUDi). The constructs are measured by multiple items. The table shows the weights, t-values, loadings, t-values, and VIF values for each item. The weights represent the strength of the relationship between the item and the construct. The t-values represent the significance of the relationship. The loadings represent the correlation between the item and the construct. The t-values represent the significance of the correlation. The VIF values represent the amount of collinearity between the items.

The results show that all of the items are significant (p < 0.05) and have a positive relationship with their respective constructs. The VIF values are all below 10, which indicates that there is no significant collinearity between the items.

Overall, the results of the PLS path model analysis support the validity and reliability of the measurement model.","We performed an ANOVA to test the linearity of the relationships between variables [100,82], the results of which are presented in Table 6. The results show that there are linear relationships between the dependent (use intention) and independent variables, since all p-values are below 0.05. However, seven out of the eight relationships reveal a statistically significant deviation from linearity, which justified the use of the ANN model?a nonlinear artificial intelligence technique that reflects the structure and operation of the human brain. The only exception was the relationship between price value and use intention, albeit its p-value (0.054) was very close to the significance threshold (0.05).",67.09%,95.41%,84.27%,50.10%,83.88%
179,254.jpg," **Hypothesis 1 (H1):** There is a positive relationship between perceived innovativeness and USAFs.
**Result:** Supported. The path coefficient between perceived innovativeness and USAFs is 0.504, which is significant at the 0.001 level. The t-value is 8.940, and the p-value is 0.000. The effect size (f2) is 0.265.

**Hypothesis 2 (H2):** There is a positive relationship between perceived service quality and USAFs.
**Result:** Supported. The path coefficient between perceived service quality and USAFs is 0.197, which is significant at the 0.001 level. The t-value is 3.671, and the p-value is 0.000. The effect size (f2) is 0.044.

**Hypothesis 3 (H3):** There is a positive relationship between perceived social support and USAFs.
**Result:** Not supported. The path coefficient between perceived social support and USAFs is 0.073, which is not significant at the 0.05 level. The t-value is 1.498, and the p-value is 0.134. The effect size (f2) is 0.006.

**Hypothesis 4 (H4):** There is a positive relationship between USAFs and intention to use.
**Result:** Supported. The path coefficient between USAFs and intention to use is 0.466, which is significant at the 0.001 level. The t-value is 8.839, and the p-value is 0.000. The effect size (f2) is 0.295.

**Hypothesis 5 (H5):** There is a positive relationship between perceived relative advantage and intention to use.
**Result:** Supported. The path coefficient between perceived relative advantage and intention to use is 0.131, which is significant at the 0.05 level. The t-value is 2.335, and the p-value is 0.020. The effect size (f2) is 0.020.

**Hypothesis 6 (H6):** There is a positive relationship between attitude toward using and intention to use.
**Result:** Not supported. The path coefficient between attitude toward using and intention to use is 0.117, which is not significant at the 0.05 level. The t-value is 1.510, and the p-value is 0.131. The effect size (f2) is 0.008.

**Hypothesis 7 (H7):** There is a positive relationship between perceived self-efficacy and TRAF.
**Result:** Supported. The path coefficient between perceived self-efficacy and TRAF is 0.277, which is significant at the 0.001 level. The t-value is 5.529, and the p-value is 0.000. The effect size (f2) is 0.123.

**Hypothesis 8 (H8):** There is a positive relationship between TRAF and intention to use.
**Result:** Supported. The path coefficient between TRAF and intention to use is 0.596, which is significant at the 0.001 level. The t-value is 13.932, and the p-value is 0.000. The effect size (f2) is 0.570.

**Hypothesis 9 (H9):** There is a negative relationship between TRAF and intention to quit.
**Result:** Supported. The path coefficient between TRAF and intention to quit is -0.224, which is significant at the 0.05 level. The t-value is 2.491, and the p-value is 0.013. The effect size (f2) is 0.023.","Multicollinearity is a problem of high correlation between independent variables [101]. The results of the multicollinearity test performed on our model (see Table 7) indicate that there were no issues of multicollinearity, since all the Variance Inflation Factor (VIF) values were in the range of 1.065?3.562 (i.e., less than 10), and the tolerances were all higher than 0.10 [93,99].",24.25%,76.34%,77.96%,34.67%,76.39%
180,255.jpg," The table shows the results of a mediation analysis. The independent variables are sunk costs and transition costs. The mediating variable is transition fatigue. The dependent variable is intermittent discontinuance.

The results show that sunk costs have a significant positive effect on transition fatigue (path coefficient = 0.233, p < 0.05) and transition fatigue has a significant positive effect on intermittent discontinuance (path coefficient = 0.184, p < 0.05). The indirect effect of sunk costs on intermittent discontinuance through transition fatigue is 0.043 (path coefficient = 0.233 * 0.184). The direct effect of sunk costs on intermittent discontinuance is 0.134 (path coefficient = 0.134). The total effect of sunk costs on intermittent discontinuance is 0.177 (path coefficient = 0.233 + 0.134).

The results also show that transition costs have a significant positive effect on transition fatigue (path coefficient = 0.133, p < 0.05) and transition fatigue has a significant positive effect on intermittent discontinuance (path coefficient = 0.241, p < 0.05). The indirect effect of transition costs on intermittent discontinuance through transition fatigue is 0.032 (path coefficient = 0.133 * 0.241). The direct effect of transition costs on intermittent discontinuance is 0.089 (path coefficient = 0.089). The total effect of transition costs on intermittent discontinuance is 0.121 (path coefficient = 0.133 + 0.089).

The results of the mediation analysis show that transition fatigue mediates the relationship between sunk costs and intermittent discontinuance and the relationship between transition costs and intermittent discontinuance.","We evaluated the measurement model by analysing its reliability and convergent and discriminant validity. The reliability analysis included three indicators of internal consistency: Cronbach's alpha (CA; [102]), the Rho coefficient and composite reliability (CR; [103]). The values for all three tests were above the recommended minimum value of 0.7. We assessed CR using average variance extracted (AVE). The AVE indicates the amount of variance a variable obtains from its indicators relative to the amount of variance caused by measurement error. All the AVE values were above the recommended minimum value of 0.5 [104]. Table 8 lists these values for each variable, along with the mean of each item and the outer loadings (i.e., the loads estimated for the relationships in reflective measurement models).",25.69%,89.57%,79.64%,39.14%,73.12%
181,256.jpg," The table shows the results of an experiment to measure the consistency and coverage of different machine learning algorithms. The algorithms were trained on a dataset of medical records and then tested on a separate dataset. The results show that the algorithms generally have high consistency and coverage, with some algorithms performing better than others. The best-performing algorithm is TEOV, which has a consistency of 0.719 and a coverage of 0.877. The worst-performing algorithm is USAF, which has a consistency of 0.361 and a coverage of 0.701.","In this study, the presence of nonresponse bias was detected. To address this issue, we conducted a multigroup analysis, following the approach outlined by Hair et al. [105]. We compared the group of respondents who completed the survey promptly (within the first 5 days of issue) with the group of respondents who completed it later. The results indicated that there were no statistically significant differences between these two groups in terms of all variables (p > 0.05). Consequently, it can be concluded that the potential impact of nonresponse bias on the sample data is likely to be minimal or negligible [106]. Next, we assessed discriminant validity by comparing the squared AVE with the intercorrelation scores. Discriminant validity is achieved if the squared AVE of a variable is greater than the intercorrelation with other variables [107]. We further checked discriminant validity by applying the heterotrait-monotrait (HTMT) ratio. Henseler et al. [108] suggest that a HTMT ratio score above 0.90 indicates a discriminant validity issue. The HTMT ratio scores were all below the threshold, indicating that discriminant validity was achieved (see Table 9).",35.34%,80.26%,82.80%,17.82%,80.71%
182,257.jpg," The table shows the results of the analysis of the eight configurations. The first column is the configuration name, and the second to ninth columns are the conditions that are being analyzed. The symbol ""●"" means that the condition is present, and the symbol ""x"" means that the condition is not present. The last two rows show the overall solution consistency and coverage.

The best configuration is HC6b, which has the highest overall solution consistency and coverage. The worst configuration is HC7, which has the lowest overall solution consistency and coverage.","First, we tested the research hypotheses by comparative analysis of the coefficients obtained by OLS, using IBM SPSS v20 as a simulation tool. The results, presented in Table 10, confirm that none of the initial eight hypotheses derived from the extended UTAUT2 model could be rejected?that is, that all eight predictors have a statistically significant influence on the dependent variable (use intention). The most influential predictors according to the OLS findings are performance expectancy (ßPE?UI=0.321, p-value=0.000), effort expectancy (ßEE?UI=0.177, p-value=0.000) and facilitating conditions (ßFC?UI=0.150, p-value=0.000), followed by hedonic motivation (ßHM?UI=0.120, p-value=0.000), subjective norms (ßSN?UI=0.100, p-value=0.000) and habit (ßHAB?UI=0.112, p-value=0.000). The least influential predictors as per OLS are risk (ßPRISK?UI= -0.084, p-value=0.000) and price value (ßPRI-VAL?UI=0.065, p-value=0.006). We assessed the quality of the OLS model using the values of adjusted R2 (which was 0.724) and normalised root mean squared error (RMSE), which was 0.1292. Both values are acceptable, meaning that OLS can be accepted as a valid baseline model.",37.90%,70.38%,80.02%,25.79%,85.25%
183,258.jpg," The table shows the results of the experiment. The first column is the configuration of the system. The second column is the condition of the system. The third column is the result of the experiment. The fourth column is the number of times the experiment was repeated. The fifth column is the percentage of times the experiment was successful.

The results show that the system is most likely to be in a state of intermittent discontinuance when the configuration is LC1a and the condition is USAF. The system is least likely to be in a state of intermittent discontinuance when the configuration is LC3b and the condition is PRCO.","We also assessed the predictive ability of the model by determining the squared multiple correlation coefficient (R2). The R2 value for use intention was 0.708, meaning that it explains a high proportion of the variance of the model. Furthermore, we examined the standardised root mean square residual (SRMR) value [108] to test the difference between the observed correlation and the predicted correlation as an indicator of model fit. A value of less than 0.08 is considered acceptable. The model proposed in this study yielded a value below this threshold (0.046). We also evaluated effect size (f2) after reviewing research from Chin [109], who indicated that f2 values of 0.02?0.15, 0.15?0.35 and 0.35 or higher suggest that an independent or exogenous latent variable has a small, moderate or large effect, respectively, on a dependent latent variable. The relationship between the variables in the present study was found to exert a significant effect, and the lowest value with regard to f2 pertained to the relationship between perceived value and use intention. Finally, we assessed the predictive relevance of the model using Stone-Geisser's Q2 value. According to Chin [109], a model demonstrates good predictive relevance when its Q2 value is greater than zero. Thus, the present value can be considered adequate. Table 10 summarises all of these results.",53.16%,84.83%,81.24%,22.32%,85.59%
184,259.jpg," Note: This table chronicles the summary statistics of the dataset employed in this study disaggregated on a monthly basis. The observations refer to the number of trades, while the mean value reflects the average price of token-changing hands denominated in US Dollars. The remaining columns are standard statistical moments used to describe data.","One of the potential problems associated with ANNs is overfitting [86], which occurs when the model ?memorises? data from the training sample and loses the ability to generalise when used with previously unseen data. To avoid this problem, we performed 10-fold cross validation [[95], [115]]. A common measure of the prediction accuracy of ANN models is RMSE [91,99] (Table 11). The low RMSE values presented in Table 11 indicate good reliability and high prediction accuracy for the proposed model [101,50]. Finally, we further evaluated the performance of the ANN model by determining its goodness-of-fit coefficient R2 [116,100], using the following formula: where   is the variance of the desired output. The value R2 = 0.957 indicates that the ANN acceptance model explains 95.7 % of the variance of use intention (model output), which is a significant improvement on the PLS-SEM results.",49.37%,77.14%,80.45%,10.84%,80.08%
185,26.jpg," The image shows the Transformer based architecture for various NLP tasks. The tasks include text prediction, classification, entailment, similarity, and multiple choice. The architecture consists of a Transformer encoder, a feed-forward network, and a linear layer. The Transformer encoder is used to encode the input text into a sequence of vectors. The feed-forward network is used to apply a non-linear transformation to the encoded vectors. The linear layer is used to compute the logits for the classification task.","Finally, we performed a sensitivity analysis of the ANN model to determine the importance of each predictor. The importance of a predictor measures the significance of the changes in the output caused by changes in different predictors [62]. The normalised importance is calculated by dividing the importance values of each predictor by the largest importance value [91]. Values for the relative and normalised importance of the ANN model are presented in Table 12.",40.40%,81.53%,83.00%,46.15%,78.70%
186,260.jpg," The table shows the results of the Benford's Law analysis for the reported COVID-19 deaths in the United States from January 2020 to September 2021. The results show that the data is not consistent with Benford's Law, which suggests that the data may have been manipulated.","The most significant predictor of use intention is PE (average importance: 0.256), followed by EE (0.175), FC (0.134) and HM (0.104), which is in line with SEM-PLS findings. Next, the ANN model predicts that SN (0.099) has a more significant impact than HAB (0.089), which differs from SEM-PLS results. Finally, the two least influential predictors were PRISK (0.087) and PRI-VAL (0.057), which was also predicted by SEM-PLS findings. These minor differences between the ANN and SEM-PLS findings can be explained by the higher prediction accuracy of the ANN model and its capacity to consider any nonlinear relationships among the variables [[50], [62]]. A detailed comparison of OLS, SEM-PLS and ANN findings is presented in Table 13 [117].",28.92%,56.27%,81.89%,11.77%,80.18%
187,261.jpg, Note: This table reports the results of the statistical significance of the trade size-clustering under Nadini et al.'s [30] dataset.,"Table 14 presents a comparison of similar research studies related to m-payment that have employed the UTAUT2 model as a theoretical framework. As can be seen, the results are aligned with the recent proposals of Al-Okaily et al. [120] and Migliore et al. (2020), which reinforces the generalisability of the findings obtained.",39.40%,87.88%,85.85%,11.44%,80.93%
188,262.jpg," The table shows the results of fitting the power law distribution to the data set. The first column is the time period, the second column is the alpha-MLE, the third column is the p-value, and the fourth column is the alpha-Levy range. The alpha-MLE is the maximum likelihood estimate of the alpha parameter of the power law distribution. The p-value is the probability of obtaining a test statistic as extreme as, or more extreme than, the observed test statistic, assuming that the null hypothesis is true. The alpha-Levy range is the range of values of alpha for which the power law distribution is a good fit to the data.

For the data set in question, the alpha-MLE ranges from 1.46 to 3.41, and the p-value is less than 0.05 for all time periods. This suggests that the power law distribution is a good fit to the data set.","Laddering approaches may be hard or soft: hard laddering involves a structured questionnaire that includes open-ended questions to be filled out by participants (e.g., in large-scale surveys; [78]), while soft laddering uses one-on-one interviews that promote in-depth conversation and enable concept clarification [31]. Given that AR is new to many consumers, participants might have difficulty articulating abstract experiences because relevant terminologies (e.g., contextual embedding) might not yet be established in consumer language. This consideration was the core reason for choosing soft laddering in this study. Fig. 1 summarizes our laddering approach.",10.32%,85.63%,80.06%,38.74%,82.81%
189,263.jpg, Note: This table pools the p-values of the statistical tests conducted in this study and applies a combination of p-value tests to account for the possibility of a Type II error.,"Based on the implication matrix, we developed an HVM that graphically represents the most dominant MECs (Fig. 2). The HVM is displayed as a tree diagram with the hierarchical level of the elements (from left to right), their relationships (arrows), and the strength of the links (arrow thickness). We found five dominant pathways in the HVM: the first and second?assortment and contextualization?facilitated a higher sense of perceived aesthetics and inspiration, respectively, which were important for users to achieve better choice-making, value for money, and perceived product fit. These benefits are critical for satisfying the underlying values of lifestyle, status and social influence, and security and safety. Third, shareability also facilitated inspiration, thereby reducing ?fear of missing out? (colloquially abbreviated as FOMO) and adding to the sense of safety. The fourth important route originated from portability and led to saving time and increasing productivity, thus satisfying economic values. Finally, the fifth theme emerged from reality congruence: consumers experiencing better value for money and a sense of accomplishment.",39.17%,75.85%,80.50%,0.76%,86.32%
190,264.jpg," The critical values for the mean absolute deviation statistic for the first digit distribution are as follows:

- 0.000-0.006: Close conformity
- 0.006-0.012: Acceptable conformity
- 0.012-0.015: Marginally acceptable conformity
- Above 0.015: Nonconformity

These critical values can be used to determine whether a given distribution of first digits is significantly different from a uniform distribution.","To validate the assignment of benefits and values into broader categories, we applied an established procedure from the qualitative literature: a sorting task.1 Card sorting is a qualitative research method used to group, label, and describe information based on feedback from customers or users. Through an international research agency, we surveyed 411 adults (male = 292, female = 119 average age of 37 years) who had bought something using AR apps within the last three months and who were familiar with AR technology. Since sorting tasks require high cognitive effort among respondents, respondents sorted either benefits (n = 208) or values (n = 203). We asked about the benefit of using a ?typical? AR shopping app, such as IKEA or Sephora (which were mentioned frequently in the main study). The question provided four options representing broader categories to which the benefits belonged, and respondents were asked to select the most appropriate category for each benefit. We applied the same procedure for status, achievement, lifestyle, economy, and safety (SALES). Since respondents assigned the constructs to the proposed categories (see Appendixes E and F for details), the results validated our classification (See Appendixes E & F). Fig. 3 summarizes the taxonomies into sensory, efficiency, assessment, and discovery benefits (SEAD) and the values into SALES.",18.36%,75.44%,78.50%,3.16%,72.59%
191,265.jpg," The table shows the critical values for the Mean Absolute Deviation test for the second digit distribution. The critical values are used to determine whether a sample of data is close to a normal distribution. If the sample mean absolute deviation is less than or equal to the critical value, then the sample is considered to be close to a normal distribution. If the sample mean absolute deviation is greater than the critical value, then the sample is considered to be not close to a normal distribution.

The table shows that the critical value for a sample size of 10 is 0.08. This means that if the sample mean absolute deviation is less than or equal to 0.08, then the sample is considered to be close to a normal distribution. If the sample mean absolute deviation is greater than 0.08, then the sample is considered to be not close to a normal distribution.","The comprehensive model also extends prior work from a value perspective, specifically extant research grounded in uses and gratifications or technology acceptance research, which has revealed the importance of utilitarian benefits?a construct that covers how ?useful? or ?practical? consumers consider an AR app (e.g., [[47], [137]]). Our study extends these findings through the additional explanation of why some apps are more or less useful than others, for example, by proposing time savings as a specific benefit, thereby contributing significantly to the IS literature. The same appears for hedonic benefits, such as aesthetics and inspiration [15]. From a broader perspective, the authors deduce two subframeworks, SEAD (benefits) and SALES (values), which group certain variables into broader categories. As shown in Fig. 4, the framework can be used in a parsimonious way to explain how AR marketing impacts value. These benefit and value categories may serve as generic sets of variables for researchers to include in their theories. Overall, this research enhances existing understandings of AR technology use behavior and provides a framework to guide future research in this area.",25.48%,85.95%,79.77%,39.41%,87.22%
192,266.jpg," | NFT | Mean Volume | Median Volume | Mean Std. Dev. | Minimum | Maximum | Begin Date | End Date |
| - | - | - | - | - | - | - | - |
| Axie Infinity | 16,108,439 | 16,084,349 | 5,783,438 | 0 | 180,232 | 7-2-2020 | 11-5-2022 |
| Art Blocks | 534 | 261,300 | 614,466 | 5 | 3,572 | 11-5-2020 | 11-5-2022 |
| Avatar | 847 | 115,000 | 343,750 | 0 | 8477 | 7-2-2021 | 11-5-2022 |
| Bastard Gas Punks V2 | 5582 | 48,582 | 55,281 | 0 | 3665 | 3-30-2021 | 11-5-2022 |
| Blitze | 365 | 10,566 | 15,058 | 0 | 317 | 11-7-2021 | 11-5-2022 |
| Blockchain Cuties | 834 | 990,404 | 3,251,297 | 0 | 126,556 | 28-8-2018 | 11-5-2022 |
| Brave Frontiers Heroes | 517 | 0 | 0 | 0 | 1115 | 11-7-2021 | 11-5-2022 |
| ChainGuardians | 1094 | 133,861 | 225,137 | 0 | 5669 | 24-1-2020 | 11-5-2022 |
| Chiliz | 1043 | 2,987 | 9,076 | 0 | 337 | 7-7-2019 | 11-5-2022 |
| Coinbase Wizards | 1524 | 10,000 | 17,283 | 0 | 5337 | 18-9-2018 | 11-5-2022 |
| Crypto Space Commanders | 1502 | 115,292 | 260,402 | 0 | 417 | 5-5-2019 | 11-5-2022 |
| Crypto Stamp Edition | 1278 | 3,639 | 7,584 | 0 | 180 | 24-2-2018 | 11-5-2022 |
| CryptoKitties | 1598 | 138,148 | 290,958 | 0 | 52,451 | 22-11-2017 | 11-5-2022 |
| CryptoPunks | 1788 | 358,331 | 685,076 | 0 | 30,987 | 21-6-2017 | 11-5-2022 |
| CryptoVoxels | 1440 | 905,078 | 2,821,257 | 0 | 118 | 5-6-2018 | 11-5-2022 |
| CyberKongz | 1074 | 3,454 | 13,851 | 0 | 100 | 15-4-2021 | 11-5-2022 |
| Dark Country | 230 | 0 | 0 | 0 | 678 | 2","While the potential existence of injustice in the recommendations provided by algorithms has been known for some time, the literature has only recently raised concerns about the harmful effects of algorithmic injustice against particular groups or individuals [7]. The data revolution has been driven by advanced analytical tools, resulting in the widespread ?datafication? of the ?information civilization? or ?surveillance society? [7]. Central to this trend is the growing enhancement in data, which encourages firms to utilize these data in their decision-making processes [88,89]. Algorithms are used to identify patterns or correlations within datasets, serving as indicators to categorize a subject as a member of a particular group [90]. These classifications are created based on ?probabilistic assumptions? [91] that ?deindividualize? the subjects [92]. For example, a firm may promote its employees based on their gender rather than their qualifications, resulting in discriminatory decisions against disadvantaged groups. This highlights the potential discriminatory implications of implicit assumptions during programming and the use of biased data to train algorithms [93]. In a hiring situation, algorithms that are used to predict job performance may rely heavily on factors such as gender and race, without adequately considering the applicants? actual performance [94]. Such discrimination is often unexpected and unintentional, as most decision makers assume algorithms to be inherently objective. Nonetheless, decision making based on algorithms may lead to using socially unjust recommendations, even if the computing process is well-intentioned [95]. Therefore, according to the obedience to authority perspective, decision makers are influenced by information or advice provided by an authority (in this case, algorithms). Consequently, if the recommendation is unjust, the user's decision may also be unjust, potentially resulting in discriminatory decisions. We hypothesize as follows (Fig. 1): H1: Algorithmic injustice in DA tools is positively associated with making discriminatory decisions. H2: Making a discriminatory decision based on an unjust algorithmic recommendation does not result in guilt. H3. Displacement of responsibility strengthens the effect of algorithmic injustice on making a discriminatory decision. H4. Displacement of responsibility moderates the effect of making a discriminatory decision when using unjust algorithmic recommendations on guilt, such that for high levels of displacement of responsibility, making discriminatory decisions does not result in guilt. H5. Trust in DA outcomes strengthens the effect of algorithmic injustice on making a discriminatory decision. H6. Trust in DA outcomes moderates the effect of making a discriminatory decision when using unjust algorithmic recommendations on guilt, such that for high levels of trust, making discriminatory decisions does not result in guilt.",0.00%,-3.73%,72.24%,0.00%,39.00%
193,267.jpg," The table shows the summary statistics of the number of transactions per year from 2017 to 2021. The number of transactions increased from 2017 to 2019, then decreased from 2019 to 2021. The mean number of transactions per year was 669,444.60. The median number of transactions per year was 494,950. The mean-median difference was 174,494.60. The skewness of the distribution of the number of transactions per year was 172.96. The minimum number of transactions per year was 1,020.00. The maximum number of transactions per year was 7,501,893.00.","In the experiment, participants were presented with a recommendation provided by DA and were asked to make a decision regarding the promotion of a candidate to a sales manager position. The decision specifically focused on choosing between a female candidate and a male candidate. This scenario aimed to simulate the use of algorithms in real-world promotion decisions, which have significant implications for individuals? lives and careers [108]. To ensure that the sample met the requirements of our study, we included a screening question asking participants about their roles in their respective firms. Participants who did not have managerial roles were excluded from the analysis. This decision was based on the understanding that managers are typically the primary users of DA tools and are responsible for accepting or rejecting the recommendations provided by such tools. The characteristics of the final sample are presented in Table 2.",33.16%,80.46%,79.63%,23.41%,76.76%
194,268.jpg," The table shows the yearly statistics of nonfungible.com transactions from 2017 to 2022. The number of transactions (N) increased from 193 in 2017 to 151 in 2022. The mean transaction value (Mean) also increased from $3957.554 in 2017 to $63,323.954 in 2022. The median transaction value (Median) increased from $4.000 in 2017 to $45,875.000 in 2022. The mean-median difference (Mean-Median) also increased from $3953.554 in 2017 to $17,448.954 in 2022. The skewness (Skewness) of the distribution of transaction values increased from 3.003 in 2017 to 0.648 in 2022. The minimum transaction value (Minimum) decreased from 0 in 2017 to $18,982 in 2022. The maximum transaction value (Maximum) increased from $52,457 in 2017 to $164,066 in 2022. The sum of all transaction values (Sum) increased from $763,808 in 2017 to $9561.917 in 2022.

The increase in the number of transactions and the mean transaction value indicates that the nonfungible.com market is growing. The increase in the mean-median difference and the skewness of the distribution of transaction values indicates that the market is becoming more concentrated, with a few large transactions accounting for a large share of the total value. The decrease in the minimum transaction value indicates that the market is becoming more accessible to smaller buyers. The increase in the maximum transaction value indicates that the market is willing to pay high prices for rare and valuable items. The increase in the sum of all transaction values indicates that the market is generating a significant amount of economic activity.","SPSS 26 was used to conduct the analyses. Cronbach's alphas, composite reliabilities, correlations, descriptive statistics, and the square roots of average variance extracted (diagonal values) are provided in Table 3. The item loadings associated with the constructs are presented in Table 4. To evaluate the presence of common method bias, we performed a marker-variable analysis [110]. We used extraversion as the marker variable, as it is theoretically not related to the variables in the research model. The average correlation between the marker variable and the main variables in the research model was 0.018, suggesting that common method bias was unlikely to exist in the data. To examine the manipulation check for ?just recommendation? versus ?unjust recommendation against women,? we used ANOVA. The results showed that participants in the ?just recommendation? treatment group reported a mean score of 4.35 (SD = 1.53), whereas participants in the ?unjust recommendation? treatment group reported a mean score of 5.31 (SD = 1.45) for the manipulation check question assessing the perception of unjust treatment toward the female candidate. The difference between the two groups was statistically significant (P < 0.05). Therefore, the manipulation of algorithmic injustice was successful.",53.67%,73.04%,79.53%,46.99%,79.01%
195,269.jpg," | NFT | N | Chi-squared | P-value | MAD | MAD conformity |
| - | - | - | - | - | - |
| SuperRare | 16,469 | 34.07 | 0.00% | 0.0034 | Close |
| Brave Frontier Heroes | 8410 | 106.54 | 0.00% | 0.0100 | Acceptable |
| Crypto Space Commanders | 15,581 | 134.60 | 0.00% | 0.0089 | Acceptable |
| Gods Unchained | 640946 | 640.46 | 0.00% | 0.0075 | Acceptable |
| Footballkitties | 1640 | 34.01 | 0.00% | 0.0075 | Acceptable |
| Known Origin | 1603 | 103.92 | 0.00% | 0.0092 | Acceptable |
| Makers Place | 851 | 43.63 | 0.00% | 0.0065 | Acceptable |
| My Crypto Heroes | 42378 | 7246.62 | 0.00% | 0.0123 | Acceptable |
| PixelChain | 1577 | 6.00 | 64.87% | 0.0060 | Acceptable |
| Art Blocks | 5524 | 220.22 | 0.00% | 0.0138 | Marginally acc. |
| Decentravids | 3616 | 551.54 | 0.00% | 0.0216 | Marginally acc. |
| F1 Delta Time | 2331 | 26.55 | 0.00% | 0.0129 | Marginally acc. |
| Axie Infinity | 26282 | 1,991.99 | 0.00% | 0.0424 | Nonconformity |
| Universe | 16664 | 45.09 | 0.00% | 0.0129 | Marginally acc. |
| War Riders | 3105 | 84.57 | 0.00% | 0.0227 | Nonconformity |
| AxieLand | 39222 | 214.92 | 0.00% | 0.0052 | Nonconformity |
| Bastard Gas Punks V2 | 1297 | 26.28 | 0.00% | 0.0227 | Nonconformity |
| Blockchain Cuties | 27349 | 366.25 | 0.00% | 0.0211 | Nonconformity |
| ChainGuardians | 8645 | 156.96 | 0.00% | 0.0266 | Nonconformity |
| Cheese Wizards | 2298 | 238.15 | 0.00% | 0.0352 | Nonconformity |
| Chibi | 1254 | 142.43 | 0.00% | 0.0211 | Nonconformity |
| Chibi Assault | 2399 | 161.28 | 0.00% | 0.0198 | Nonconformity |
| Crypto Stamps Edition | 2623 | 103.56 | 0.00% | 0.0212 | Nonconformity |
| Cryptovoxels | 3874 | 1174.22 | 0.00% | 0.0824 | Nonconformity |
| CryptoKitties | 299485 | 2559.27 | 0.00% | 0.0283 | Nonconformity |
| CryptoPunks | 2231 | 1168.68 | 0.00%","SPSS 26 was used to conduct the analyses. Cronbach's alphas, composite reliabilities, correlations, descriptive statistics, and the square roots of average variance extracted (diagonal values) are provided in Table 3. The item loadings associated with the constructs are presented in Table 4. To evaluate the presence of common method bias, we performed a marker-variable analysis [110]. We used extraversion as the marker variable, as it is theoretically not related to the variables in the research model. The average correlation between the marker variable and the main variables in the research model was 0.018, suggesting that common method bias was unlikely to exist in the data. To examine the manipulation check for ?just recommendation? versus ?unjust recommendation against women,? we used ANOVA. The results showed that participants in the ?just recommendation? treatment group reported a mean score of 4.35 (SD = 1.53), whereas participants in the ?unjust recommendation? treatment group reported a mean score of 5.31 (SD = 1.45) for the manipulation check question assessing the perception of unjust treatment toward the female candidate. The difference between the two groups was statistically significant (P < 0.05). Therefore, the manipulation of algorithmic injustice was successful.",0.15%,2.39%,76.41%,0.00%,41.61%
196,27.jpg,Error: Invalid response from model,"We performed a hierarchical logistic regression analysis in SPSS 26 to examine (1) the impact of algorithmic injustice on making a discriminatory decision and (2) the moderating role of displacement of responsibility and trust in DA outcomes on that association. The results, presented in Table 5, indicated that the impact of algorithmic injustice on discrimination (H1) was statistically significant (P < 0.001). While the moderating effect of displacement of responsibility on the relationship between algorithmic injustice and discrimination (H3) was not significant, trust in DA outcomes showed a significant moderating effect on the impact of algorithmic injustice on discrimination (P < 0.05) (H5). The findings also demonstrated that the control variables did not have a significant impact on making algorithmically informed, discriminatory decisions. Next, we ran an analysis of covariance (ANCOVA) to examine the effect of discrimination on the perception of guilt (H2). The results indicated that the effect was not significant (P = 0.19), supporting H2. We then added two interaction terms to the ANCOVA model to test the moderating effect of displacement of responsibility on the relation between discrimination and guilt (H4) as well as the moderating impact of trust in DA outcomes on that association (H6). The results showed that while H4 was not significant (P = 0.87), H6 was marginally significant (P = 0.08). In summary, while H1, H2, and H5 were supported, H3 and H4 were not supported, and H6 was partially supported.",0.69%,60.00%,78.11%,0.00%,81.29%
197,270.jpg," | NFT | N | Chi-squared | P-value | MAD | MAD conformity |
| - | - | - | - | - | - |
| Art Blocks | 8743 | 23.26 | 0.56% | 0.0045 | Close |
| Axie Infinity | 171127 | 1130.71 | 0.00% | 0.0075 | Close |
| Battle Infinity | 6247 | 64.47 | 0.00% | 0.0089 | Close |
| Brave Frontiers | 8410 | 21.45 | 1.00% | 0.0037 | Close |
| Crypto Space Commanders | 15581 | 76.25 | 0.00% | 0.0056 | Close |
| CryptoKitties | 1091521 | 1616.08 | 0.00% | 0.0029 | Close |
| CryptoPunks | 7262 | 50.47 | 0.00% | 0.0071 | Close |
| CryptoSpells | 2645 | 15.33 | 8.24% | 0.0052 | Close |
| CryptoVoxels | 3344 | 47.39 | 0.00% | 0.0056 | Close |
| Decentraland | 1603 | 24.64 | 0.00% | 0.0078 | Close |
| Enjin Coin | 69464 | 123.25 | 0.11% | 0.0028 | Close |
| Gods Unchained | 18743 | 228.00 | 0.00% | 0.0042 | Close |
| Illuvium | 11991 | 128.20 | 0.00% | 0.0077 | Close |
| Knights of Origin | 2177 | 11.91 | 18.5% | 0.0020 | Close |
| MegaCryptoPolis | 3105 | 36.43 | 0.00% | 0.0054 | Close |
| Makers Place | 8591 | 102.61 | 0.03% | 0.0071 | Close |
| My Crypto Heroes | 3735 | 30.66 | 2.83% | 0.0059 | Close |
| NBA Top Shot | 290485 | 168.93 | 0.00% | 0.0019 | Close |
| Sorare | 19003 | 169.37 | 0.00% | 0.0078 | Close |
| Splinterlands | 290048 | 128.15 | 0.00% | 0.0027 | Close |
| SuperRare | 2231 | 18.15 | 6.38% | 0.0090 | Close |
| The Sandbox | 16469 | 155.36 | 0.00% | 0.0081 | Close |
| Chiliz | 5645 | 148.86 | 0.00% | 0.0090 | Acceptable |
| FIFA World Cup Qatar 2022 | 10965 | 19.65 | 0.00% | 0.0088 | Acceptable |
| PSG | 2799 | 35.98 | 0.00% | 0.0137 | Acceptable |
| Dark Country | 1663 | 17.94 | 3.59% | 0.0099 | Marginally acc. |
| Pixies Official | 1648 | 33.42 | 0.00% | 0.","This study employs the theoretical framing of situational skepticism and signaling theory to develop a model explaining the emergence of consumer skepticism toward web seals (refer to Fig. 2 and Table 2). We argue that consumers rely on their topic and agent knowledge structures to elaborate on a web seal's unreliability (Section 3.1.1) and the seal authority's motives and credibility (Section 3.1.2), resulting in disbelief in the web seal and mistrust in the seal authority.",0.89%,6.02%,74.07%,33.50%,33.40%
198,271.jpg," The table shows the results of a study that measured the conformity of different non-fungible tokens (NFTs) to the MAD Conformity Index. The index is a measure of how well an NFT conforms to the standards of the market. The higher the index, the more conformed the NFT is.

The study found that the average conformity of NFTs is 0.58. This means that, on average, NFTs are only moderately conformed to the market standards. However, there is a wide range of conformity among NFTs, with some NFTs being highly conformed and others being very non-conformed.

The most conformed NFT is CryptoPunks, with a conformity index of 0.99. This means that CryptoPunks are very similar to the other NFTs in the market. The least conformed NFT is Art Blocks Curated, with a conformity index of 0.01. This means that Art Blocks Curated is very different from the other NFTs in the market.

The study also found that there is a positive correlation between conformity and price. This means that, on average, NFTs that are more conformed to the market standards are also more expensive. However, there are some exceptions to this trend, with some highly conformed NFTs being relatively inexpensive and some very non-conformed NFTs being relatively expensive.

The study's findings suggest that conformity is an important factor to consider when investing in NFTs. NFTs that are more conformed to the market standards are more likely to be accepted by the market and, therefore, more likely to increase in value. However, investors should also be aware that there are some exceptions to this trend and that some very non-conformed NFTs may also be valuable.","We used the Amos software to assess the significance of the structural path estimates in our theoretical model. Fig. 4 presents the model testing results. The model explains 74.9 % of the variance in mistrust in the IS provider, 61.1 % in perceived assurance, 56.5 % in inferences of manipulative intent, 51.5 % in disbelief in web seal, and 57.3 % in mistrust in seal authority.",31.03%,71.97%,79.96%,37.24%,76.14%
199,272.jpg," The table shows the results of the Pearson's Chi-squared test for the years 2017 to 2021. The test is used to determine whether there is a significant difference between the observed and expected frequencies of a categorical variable. In this case, the categorical variable is the conformity of NFT transactions to the power law distribution.

The results show that the p-value for each year is 0.00%, which means that there is a significant difference between the observed and expected frequencies of the variable. This suggests that the power law distribution is not a good fit for the data.

The table also shows the MAD (mean absolute deviation) and MAD conformity for each year. The MAD is a measure of the variability of the data, and the MAD conformity is a measure of how well the data conforms to the power law distribution.

The results show that the MAD is relatively small for each year, which suggests that the data is not very variable. However, the MAD conformity is also relatively small for each year, which suggests that the data does not conform well to the power law distribution.

Overall, the results of the Pearson's Chi-squared test, the MAD, and the MAD conformity suggest that the power law distribution is not a good fit for the data.","Second, we performed a confirmatory factor analysis (CFA). We examined the reliability, convergent validity, and discriminant validity of the latent reflective constructs. The composite reliability (CR) was above the recommended 0.70 threshold [91]. The average variance extracted (AVE) for all constructs exceeded the suggested 0.50 threshold [28], thereby demonstrating good reliability and internal consistency (Table 4), except for our control variable, social desirability (AVE = 0.475). Because social desirability slightly falls below the 0.50 threshold but has a CR of 0.729, we kept social desirability in our model (cf. [28]). All indicators loaded significantly on their latent constructs, and standardized loadings exceeded the required minimum of 0.700, indicating good convergent validity, except for social desirability (Appendix D). In addition, we tested the discriminant validity of the constructs. Since the square root of the AVE of each construct exceeded the squared interconstruct correlations, each construct explained more variance in its indicators than it shared with other constructs (Table 4). ",41.85%,78.80%,81.78%,43.21%,85.74%
200,273.jpg," The table shows the results of a Pearson's chi-squared test for each year from 2017 to 2021. The test is used to determine whether there is a significant difference between the observed and expected frequencies of a categorical variable. In this case, the categorical variable is whether or not an NFT transaction conforms to the MAD. The results show that there is a significant difference between the observed and expected frequencies of conformity to the MAD in each year, with p-values of 0.00 for all years. This suggests that the MAD is not a good predictor of whether or not an NFT transaction will conform to the MAD.","In addition, all heterotrait?monotrait (HTMT) ratios of correlations (Table 5) were below the 0.85 threshold [36], suggesting no discriminant validity problems. We also examined variance inflation factor (VIF) values to test for multicollinearity in our data. The highest VIF value was between inferences of manipulative intent and mistrust in seal authority (i.e., 3.146), falling below a threshold of 5.0, suggesting that our data are not subject to a severe multicollinearity issue [62]. To assess model fit, we used four metrics [70]: the ?²/degrees of freedom (df) ratio, the root mean squared error of approximation (RMSEA), the comparative fit index (CFI), and the Tucker?Lewis index (TLI). Common thresholds for acceptable model fit are ?²/df < 3, RMSEA < 0.80, CFI and TLI > 0.90 [32,42]. The CFA model yielded an acceptable model fit (?²/df = 2.329; RMSEA = 0.042; CFI = 0.943; TLI = 0.939).",32.37%,62.53%,80.10%,36.71%,84.55%
201,274.jpg," The Pearson's Chi-squared test is a statistical test that is used to determine whether the distribution of a set of data fits a particular distribution. In this case, the data is the distribution of the first digits of NFT prices. The null hypothesis is that the distribution of the first digits of NFT prices follows Benford's law. The alternative hypothesis is that the distribution of the first digits of NFT prices does not follow Benford's law.

The results of the Pearson's Chi-squared test show that the p-value for each year is less than 0.05, which means that the null hypothesis is rejected. This means that the distribution of the first digits of NFT prices does not follow Benford's law.

This finding is consistent with the findings of other studies that have found that the distribution of the first digits of prices in financial markets does not follow Benford's law. This suggests that there may be some underlying factors that are causing the distribution of prices to deviate from Benford's law.",This study set out to identify and empirically test the antecedents and consequences of skepticism toward web seals. We conducted an online experiment to test the proposed hypotheses in a cloud service market context. Our results support the harmful effects of skepticism toward consumers? perception of IS providers (Table 6) and particularly emphasize the central role of seal authority incredibility in the nomological net. This study uncovers skepticism as a critical boundary condition for the effectiveness of web seals because skepticism can lead to the opposite effect of what is intended with them.,41.61%,91.05%,81.65%,43.92%,79.87%
202,275.jpg," The table shows the results of a Pearson's chi-squared test for the null hypothesis that the distribution of MAD conformities is the same across years. The test statistic is 126.44, with 5 degrees of freedom, and the p-value is 0.000. This suggests that the null hypothesis can be rejected, and that the distribution of MAD conformities is not the same across years.

To further investigate this, we can look at the standardized residuals. The standardized residuals are the differences between the observed and expected frequencies, divided by the square root of the expected frequencies. They can be used to identify which years have significantly different distributions of MAD conformities.

The standardized residuals for 2017, 2018, and 2019 are all positive, which suggests that there are more MAD conformities than expected in these years. The standardized residuals for 2020, 2021, and 2022 are all negative, which suggests that there are fewer MAD conformities than expected in these years.

This suggests that there is a trend of decreasing MAD conformities over time. This could be due to a number of factors, such as the increasing popularity of NFTs, the increasing availability of information about NFTs, and the increasing sophistication of NFT traders.","The entire sequence of events of our model is described in Fig. 1. In the first period, both firms set their prices and consumers choose the product that maximizes their utility. After the purchase, the sales volume information and ratings information are generated. In the second period, the disclosed information is visible to consumers, thereby influencing the purchase. The disclosure of the two types of information generates three scenarios, namely sales information only, ratings information only, and both types of information.",47.93%,93.93%,82.08%,41.67%,73.25%
203,276.jpg," The diagram shows a tree-like hierarchy with a national headquarters at the top, with three intermediate offices below it. Each of the intermediate offices has three regional units below it. There is also an international department at the same level as the intermediate offices, but it is not part of the main hierarchy.","The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table 1 for details).",17.71%,76.21%,82.88%,40.88%,77.83%
204,277.jpg," The image shows how social capital can be accumulated through different strategies at different levels of the organization. At the strategic level, the focus is on supra-local representation of the organization and accrual of structural social capital. This can be achieved through strategic alignment via organizational guidelines and workshops. At the tactical level, the focus is on overarching social media activities and orchestration, as well as accrual of structural and relational social capital. This can be achieved through central supervision, coordination, and distribution of resources. At the operational level, the focus is on building local communities and partnerships, as well as accrual of relational and cognitive social capital. This can be achieved through decentralized programs for system innovation. The organization can accumulate social capital by focusing on these different strategies at different levels.","We have the following findings, which are consistent with previous ones:1 Firm H sets a higher price in the first period than in the second period when  . Firm L always sets a higher price in the second period than in the first period. 2 Firm H always sets a higher price than firm L in the first period but a lower price in the second period. 3 The profit of firm H is higher than that of firm L ( ) only when the price effect is weak?that is,  . As the price effect strengthens, the profit difference decreases ( ). Here, . 4 As the brand reputation gap increases, the profit difference increases ( ) except when, and this effect is mitigated as the price effect increases ( ). 5 The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table C1 for details).",25.76%,77.67%,80.29%,33.58%,73.30%
205,278.jpg," The table shows the interview participants' profiles. In total, 18 interviews were conducted with staff and volunteers of an international humanitarian NGO. The participants were from three different units of the organization: regional, intermediate, and national. They had varying years of experience in social media management and different organizational relationships with the NGO. The interviews lasted between 30 and 170 minutes.","We have the following major findings, which are basically consistent with previous ones: 1 Firm H always sets a higher price than firm L in the first period but a lower price in the second period. 2 The profit of firm H is higher than that of firm L ( ) only when the price effect is weak?that is,  . Here,  3 The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table D1 for details).",25.62%,74.56%,81.71%,24.53%,70.54%
206,279.jpg," The table shows the results of a two-step hierarchical regression analysis. In the first step, the main effects of crowdfunding platform type and prosocial motivation were entered into the model. In the second step, the interaction between crowdfunding platform type and prosocial motivation was added to the model. The results show that the interaction between crowdfunding platform type and prosocial motivation was significant, indicating that the relationship between crowdfunding platform type and prosocial motivation was moderated by prosocial motivation.","Our BI system success model is depicted in Fig. 1. The model defines system quality, information quality, and service quality as important attributes in an individual user's evaluation of a system leading to actual use of the system and to user satisfaction [5,19,28,126,132]. System quality refers to the usability, availability, reliability, and response time of the system as experienced by its users, whereas information quality reflects the content and format of the system's outputs to ensure they are useful, relevant, and understandable to users in their decision making [29,40]. Service quality reflects the support users receive when using the IS, such as the ability for a support team to respond reliably and timeously to user queries [29]. Recent work has also drawn attention to the importance of data quality in IS success [23,43]. Data quality reflects the completeness and integrity of the underlying data captured and stored in a system [80,116], and is specifically relevant in the BI context [60,117]. User satisfaction is the affective response of the user toward the system, such as whether it is found enjoyable or frustrating to use [11,103]. Use and user satisfaction then have consequences for user performance [59,90,120], such as perceived contribution to individual problem solving, decision making, and productivity [53,58,92,99,109]. However, the measurement of use has been oversimplified in many IS success studies [90], and we therefore distinguish between routine and advanced system use. Fig. 1 also depicts our incorporation of user self-efficacy, along with the direct and moderating effects of task complexity on the links between BI use and performance.",34.53%,85.71%,81.22%,5.32%,81.70%
207,28.jpg," In this image, a user provides a disinformation headline to a language model (LLM). The LLM then generates a script based on the headline. The script is then used to generate a news article. This process can be repeated multiple times, with each iteration resulting in a new news article.",The dimensionality of the constructs was confirmed through an initial exploratory principal components analysis with varimax rotation. The results (Table 4) did not suggest that any items needed to be dropped as all factor loadings were above 0.60 and average variance extracted values were all above 0.50 (Table 4). ,11.39%,73.13%,81.75%,41.33%,81.82%
208,280.jpg," From the results of the hierarchical regression analysis, we can see that in Step 1, the control variables of gender, age, and income were entered into the model. The results showed that none of these variables were significantly related to the dependent variable of crowdfunding amount. In Step 2, the main effect of crowdfunding platform type was added to the model. The results showed that crowdfunding platform type was significantly related to crowdfunding amount, with projects on Kickstarter receiving more funding than projects on GoFundMe. In Step 3, the main effect of project type was added to the model. The results showed that project type was significantly related to crowdfunding amount, with creative projects receiving more funding than other types of projects. In Step 4, the interaction effect of crowdfunding platform type and project type was added to the model. The results showed that the interaction effect was significant, with creative projects on Kickstarter receiving more funding than creative projects on GoFundMe."," Moreover, as per Harman's one factor test, the first factor did not account for a majority of variance, only 14.5 %, suggesting no concerns over common method bias [67,94]. In addition, we considered the marker variable approach to test for common method bias [79]. We selected self-reported experience in management accounting as a marker because it is likely to be subject to a similar disproportionate response or acquiescence bias as other variables, such as levels of BI use and performance. However, it is not expected to be theoretically or statistically related to other model variables. When the marker variable was included as an additional determinant of the dependent variables, the significance of path coefficients did not change, providing further assurance that common method bias was not substantial [79]. Cronbach's alpha was used to measure internal consistency of the multi-item scales and found to be greater than 0.70 for all constructs. This provides support for convergent validity and reliability of our scales. To confirm discriminant validity, we compared inter-construct correlations with the square root of AVE of each construct. The square roots of AVE of each construct are presented along the diagonal of Table 5. These are shown as larger than the inter-construct correlations (i.e., constructs share more variance with their own items than with other constructs in the model). In addition, the HTMT ratios were calculated, and results indicate that the ratios are less than 0.85 [52]. Therefore, discriminant validity was confirmed.",42.20%,92.84%,82.83%,33.40%,83.86%
209,281.jpg," Table 3 shows the results of the four-step hierarchical regression analysis. In the first step, we entered the control variables: gender, age, income, education, and nationality. The results show that none of these variables were significantly related to the dependent variable, crowdfunding amount. In the second step, we added the main effects of the three independent variables: crowdfunding platform type, project type, and prosocial motivation. The results show that crowdfunding platform type and prosocial motivation were both significantly related to crowdfunding amount. Specifically, projects on reward-based crowdfunding platforms and projects with higher levels of prosocial motivation were associated with higher levels of crowdfunding. In the third step, we added the two-way interaction terms between the independent variables. The results show that the interaction between crowdfunding platform type and project type was significant. This interaction indicates that the relationship between crowdfunding platform type and crowdfunding amount is different for different types of projects. Specifically, the positive relationship between reward-based crowdfunding and crowdfunding amount is stronger for creative projects than for non-creative projects. In the fourth step, we added the three-way interaction term between the independent variables. The results show that the three-way interaction was not significant. This indicates that the relationship between crowdfunding platform type, project type, and prosocial motivation is not moderated by the other two variables.","A Mann-Whitney comparison of users in the high advanced use subgroup with those in the low advanced use subgroup reveals significant differences across all model variables, except self-efficacy (Table 6). The mean rank for users classified as high advanced users was higher on all four system attributes, and in BI contribution to performance. These users also ranked significantly higher in task complexity.",21.88%,90.40%,83.09%,37.33%,76.20%
210,282.jpg," The table shows the development of the Airbnb market in Romania from 2018 to 2020. The number of active listings decreased from 42,028 in 2018 to 36,012 in 2020. The total revenue decreased from 439,323,137.22 lei in 2018 to 121,072,214.67 lei in 2020. The number of nights booked decreased from 3,650,895 in 2018 to 1,162,574 in 2020. The occupancy rate decreased from 41.35% in 2018 to 16.34% in 2020. The revenue per available night decreased from 49.75 lei in 2018 to 17.02 lei in 2020.

The decrease in the number of active listings, total revenue, number of nights booked, occupancy rate, and revenue per available night from 2018 to 2020 can be attributed to the COVID-19 pandemic. The pandemic has led to a decrease in the number of people traveling and staying in hotels and other accommodations. This has had a negative impact on the Airbnb market in Romania.",The structural model was tested using AMOS with results reported in Table 7 indicating 17 out of 22 hypothesized paths were directly supported by the model being tested.,19.22%,71.62%,79.85%,35.12%,63.81%
211,283.jpg," The table shows the descriptive statistics of Airbnb listings in the United States from 2018 to 2020. The occupation rate is the percentage of nights that an Airbnb listing is booked. The revenue per active night is the average amount of money that an Airbnb listing earns per night that it is booked.

The occupation rate and revenue per active night both decreased from 2018 to 2020. This is likely due to the COVID-19 pandemic, which caused a sharp decline in travel and tourism. The occupation rate was hit particularly hard in 2020, when it fell to just 17.77%. This is because many people were unable or unwilling to travel during the pandemic. The revenue per active night also fell in 2020, to just $14.47. This is likely due to the fact that Airbnb hosts were forced to lower their prices in order to attract guests.

The COVID-19 pandemic had a significant impact on the Airbnb industry in the United States. The occupation rate and revenue per active night both fell sharply in 2020. This is likely due to the fact that many people were unable or unwilling to travel during the pandemic.","System quality (p < 0.001), data quality (p < 0.01), information quality (p < 0.001), and service quality (p < 0.05) were all found to have positive significance effects on routine use. Furthermore, system quality (p < 0.001), data quality (p < 0.05), information quality (p < 0.001), and service quality (p < 0.01) were all found to have positive significance influence on advanced use, with effect sizes larger for advanced use than routine use, thus supporting H1a, H1b, H3a, H3b, H5b, H5c, H7a, and H7b. To further confirm the effects, we ran additional multiple regressions with bootstrap resampling to determine the overlap, if any, among confidence intervals. As shown in Table 8, we confirm no overlapping confidence intervals for effects of DQ and IQ providing added support to H3b and H5c. Although there is some overlap in confidence intervals for the effects of SQ and SQa, the overlap is not more than 50 % of the confidence interval range, thus giving us confidence to support H1b and H7b that their effects are greater on advanced use than on routine use. We also considered data quality to have additional indirect effects on BI use through effects on information quality. The effect of data quality on information quality is significant, supporting H5a.",22.23%,64.57%,78.86%,39.49%,80.00%
212,284.jpg," The table shows the results of the stress test for the period 2018-2019 and 2019-2020. The stress test was conducted to assess the impact of a severe economic downturn on the capital adequacy of banks. The results show that the banks would be able to withstand a severe economic downturn without breaching their minimum capital requirements.

In the first scenario, the banks were subjected to a 25% decline in GDP and a 50% increase in unemployment. In the second scenario, the banks were subjected to a 50% decline in GDP and a 100% increase in unemployment.

The results show that the banks would be able to withstand a severe economic downturn without breaching their minimum capital requirements. This is because the banks have a strong capital base and they are well-diversified.","We considered the model's goodness of fit, the significance of the path coefficients, and the sign of the path to reach conclusions about moderation [48]. The results (Table 9) indicate that the complexity of the management accountant's tasks increases their opportunity to use the BI system innovatively to support their management accounting function (p < 0.001). Task complexity also has a moderating effect on routine use and performance (p < 0.05). The relationship between use and performance is moderated by task complexity, but the moderating effect is weaker for routine use, supporting both H13a and H13b. This confirms that the more complex the tasks of management accountants, the stronger will be the effect of advanced use on performance. This supports the importance of using the advanced features of the BI system to improve performance under conditions of greater task complexity.",48.64%,73.55%,81.74%,48.22%,79.83%
213,285.jpg," Note: Descriptive statistics for the absolute value of PV2020 and PV2019 are displayed. The absolute value was chosen since the purpose of the table is to show the mean and observed PV2020 (chosen according to [18], to analyze the effects of Covid-19); Column on the right: Airbnb listings employing strict policies in 2019 and observed in March-December 2020 (chosen according to [18], to act as a benchmark situation); Last two columns: Column on the left: universe of 10,498 Airbnb listings observed in March-December 2020 (chosen according to [18], to act as a benchmark situation); d) The T-Stat (Mean Comparison) compares the mean value in 2020 with that of 2019, and tests whether the former is significantly larger than the latter. *** p < 0.001; ** p < 0.01; * p < 0.05; + p < 0.10.",Fig. 1 illustrates the conceptual integration of this new phase within the existing body of theory.,24.15%,58.22%,81.41%,35.29%,67.85%
214,286.jpg, Table 4: Regression Results - Airbnb Price per night,"Our study further reveals what concrete contextual manifestations make up these seven bias-inducing factors within the underlying particular context of AI-supported car manufacturing. Throughout the study's course, we captured these contextual manifestations in the form of second-order research elements. Fig. 2 presents a holistic overview over both the first-order psychological factors from the original status quo bias theory and their occurrence in our research as well as their second-order contextual manifestations.",0.00%,52.92%,82.64%,0.00%,66.01%
215,287.jpg, Table 4: Regression Results - Robustness Checks,"In the third step, which refers to Level 2c in the approach of [19] , the interviews were then coded in two different rounds within a parallel deductive and inductive approach. Within the former, we used our first-order research elements (i.e., the psychological factors from the status quo bias theory) for confirmatory coding of the raw case protocol. After that, in the second coding round, inductive coding was used on the statements extracted within the deductive first round to identify the concrete contextual manifestations of those bias-inducing factors. For instance, the sentence ?I think most people have seen the movie ?The Terminator?, right? I mean, the underlying idea as such was well-intentioned, but if it gets out of hand, if you lose control at some point, you're going to have a huge problem? (Interviewee 24, translated) was labeled with the code ?anchoring effects? in the first round and then with the contextual element ?Portrayal of AI in Hollywood movie (?The Terminator?)? in the second round. These contextual manifestations function as the second-order research elements in this study and the presentation of its results (cf. Fig. 2 and Table 3 in the results section).",2.53%,57.78%,79.69%,0.00%,78.73%
216,288.jpg," The figure shows the relationships between technological structure, ecosystem structure, triggering conditions, interpretations and activities, and social media in use. The technological structure of social media includes features such as range of influence, similarity to face-to-face, convenience, transparency, work and time intensity, informational content, risk potential, community features, up-to-dateness, variety of formats, cost, customizability, measurability, professional appearance, and personal availability. These features generate triggering conditions such as discrepancy, obligation, and novelty. The ecosystem structure of social media includes market, industry, customer demands, business relations, competitive pressure, protection against competitors, other environmental entities, and international market. The market is driven by social media in use, which in turn is influenced by the technological structure and ecosystem structure. The interpretations and activities of social media users are influenced by triggering conditions and the technological structure. The activities of social media users include individual meaning-making, collective sensemaking, and use in various business functions and processes.","During this step, insiders were grouped into categories, as shown in Fig. 2. ",3.88%,77.64%,82.01%,34.21%,56.78%
217,289.jpg," The table shows information about 20 SME owner-managers. For each owner-manager, the following information is provided:

- Position
- Age
- Education
- Gender
- Products/Services
- Founding year
- Employees

The owner-managers are mostly male (65%) and have a college degree (45%). They are typically in their 40s or 50s. The most common industries are services (25%), manufacturing (20%), and retail (15%). The majority of the businesses were founded in the 1990s (40%). The average number of employees is 29.","Moreover, we introduce a new kind of insider threat, Category C3 (Disempowered). Before the advent of generative AI, this was not an insider threat category, but now that hackers have these tools, the traditional rule-based approach is no longer sufficient (Fig. 5).",15.23%,66.12%,80.66%,39.09%,66.24%
218,29.jpg," This is an illustration of the process of creating presentations using AI tools. The process starts with the user creating prompts for multiple AI tools based on the content of the presentation. The AI tools then generate the presentation slides, which are then integrated into a final presentation. The final presentation can be exported to a variety of formats, including PowerPoint, Google Slides, and PDF.","The analysis showed that the BU and vendor teams bypassed formal communication channels and exchanged information directly, hiding information from the SSU and the mediated system (see Fig. 1). Under these circumstances, rather than applying dogmatic rules as would be expected from a highly structured mediated system, the broker was astute in recognizing the often conflicting needs of the distributed system and adapted their own brokering actions accordingly. Our analysis shows that brokering actions tended to be based on four main conditions: exchange risk, information load, brokering capacity, and political influence. These are explored in more detail below.",26.42%,86.99%,82.96%,28.71%,80.92%
219,290.jpg," The table shows the perceived usefulness of social media features for different levels of use. The features are: range of influence, similarity to face-to-face, convenience, transparency, work and time intensity, informational content, risk potential, communication, up-to-dateness, variety of formats, customizability, measurability, professional appearance, and personal availability.

For each feature, the table shows whether it is perceived as useful (++, +, -, or --), or whether its usefulness is ambiguous or not established (o).

For example, the range of influence is perceived as very positive for regular use, but very negative for no use. Similarly, similarity to face-to-face is perceived as positive for regular use, but negative for sporadic use.

Overall, the table shows that social media features are perceived as more useful for regular use than for sporadic use or no use. This is likely because regular use allows users to experience the benefits of social media more fully. For example, regular users may be able to build stronger relationships with others, share more information, and be more productive.","Departing from these assumptions, our findings show that because of workarounds, the formal broker needed to adapt its brokering practice and consider alternative practices that would address the exchange of hidden information within the information exchange network. As another level of abstraction, we next synthesize our findings from this case into three formal brokering practices, as depicted in Fig. 2.",16.22%,91.59%,81.67%,38.07%,76.86%
220,291.jpg," Table 1 reports the descriptive statistics and correlation matrix for the variables used in the analysis. All variables are expressed in natural logarithms. The average value of labour productivity is 11.04, with a standard deviation of 0.67. The average value of value added is 17.54, with a standard deviation of 0.96. The average value of employment is 6.50, with a standard deviation of 0.26. The average value of fixed assets per employee is 10.14, with a standard deviation of 1.79. The average value of personnel cost per capita is 10.69, with a standard deviation of 0.42. The average value of capital expenditure is 15.29, with a standard deviation of 1.76. The average value of differentiation strategy is 0.041, with a standard deviation of 0.074. The average value of cost leadership strategy is 1.18, with a standard deviation of 0.68. The average value of dual strategy is 0.045, with a standard deviation of 0.17.

The correlation matrix shows that labour productivity is positively correlated with value added (0.59), employment (0.68), fixed assets per employee (0.53), and capital expenditure (0.51). Value added is positively correlated with employment (0.91), fixed assets per employee (0.87), and capital expenditure (0.86). Employment is positively correlated with fixed assets per employee (0.86) and capital expenditure (0.85). Fixed assets per employee is positively correlated with capital expenditure (0.89). Personnel cost per capita is positively correlated with differentiation strategy (0.16) and cost leadership strategy (0.15). Capital expenditure is positively correlated with differentiation strategy (0.17) and cost leadership strategy (0.16). Differentiation strategy is positively correlated with cost leadership strategy (0.19).","Our proposed PCGT model (see Fig. 3) depicts its three assumptions: the bidirectional reduction of privacy concerns, two types of privacy trade-offs, and anchoring effects. The model also includes a combination of rational and behavioral privacy decisions based on the three types of sustainability and two SSCC applications of monetary incentives. Thus, to demonstrate how consumers disclose their personal information, specifically that for SSCCs, the PCGT model describes not just the effects of the three types of sustainability on disclosure intentions but also a mediation effect and six moderated?mediation effects. Next, we discuss the effect of each type of sustainability on SSCC consumers? disclosure intentions and purchase intentions. Information disclosure acts as a mediator between privacy concerns and purchase decisions concerning SSCCs. Accordingly, we assume that privacy concerns will negatively affect information disclosure [4,107]. In parallel, we predict that SSCC consumers? purchase intentions will be affected more by disclosure intentions than privacy concerns because the willingness to use personal information for location-aware marketing positively influences purchase intentions [113]. Due to the unique privacy issues raised by SSCCs?that is, the large amount of personal information disclosed and its accessibility to multiple stakeholders?we posit that privacy concerns similarly affect consumers? disclosure intentions and purchase intentions regarding SSCCs (see Fig. 3). Thus, we propose the following hypotheses: H1. SSCC consumers? privacy concerns will negatively influence their disclosure intentions. H2. SSCC consumers? privacy concerns will negatively influence their purchase intentions. H3. SSCC consumers? disclosure intentions will positively influence their purchase intentions. H4. SSCC consumers? disclosure intentions will partially mediate the relationship between privacy concerns and purchase intentions.",16.01%,76.53%,78.57%,39.79%,78.49%
221,292.jpg," The frequency is the number of times a value occurs in a dataset. The percentage is the frequency divided by the total number of values in the dataset. The cumulative percentage is the sum of the percentages up to and including that value.

In this case, the frequency of the ""Automate industry"" is 770, the percentage is 4.35%, and the cumulative percentage is 4.35%. The frequency of the ""Informate industry"" is 14,960, the percentage is 84.57%, and the cumulative percentage is 88.92%. The frequency of the ""Physical-transform industry"" is 1250, the percentage is 7.07%, and the cumulative percentage is 95.99%. The frequency of the ""Digital-transform industry"" is 710, the percentage is 4.01%, and the cumulative percentage is 100.00%.","To understand why consumers share their information in response to vendors? sustainability-related advertisements and government policies concerning SSCCs, it is important to consider their rational and nonrational behaviors. We explicate consumers? privacy decisions from two perspectives, as summarized in Table 2 and Fig. 4: (1) from a PCM perspective, rational decisions, which involve personal benefits and monetary incentives, and (2) from a TBR perspective, behavioral decisions, which involve anchoring effects on sustainability.",7.65%,65.69%,78.86%,39.55%,76.40%
222,293.jpg, Table 4. Regression results,"The instructions, manipulations, and surveys were delivered online through the Qualtrics XM? survey and marketing-research platform. Eligible participants were required to read and sign the institutional review board's consent form before deciding whether to participate in the survey. Those who consented were asked to provide demographic information and answer presurvey questions about relevant aspects of their backgrounds, including data breach experiences, general privacy concerns, technical knowledge, and sustainability preferences. Next, the participants read the experiment instructions (see Fig. 5). ",0.00%,62.47%,83.34%,0.00%,52.86%
223,294.jpg," Table 4 reports the estimation results of the system GMM estimations for the five industries. All the models are estimated with a two-step system GMM estimator with robust standard errors clustered at the firm level. We report the results for the first-differenced models as well as the system models. The first column reports the results for the automatic industry. The results show that the effect of digital transformation on labor productivity is positive and significant at the 1% level. A one standard deviation increase in digital transformation increases labor productivity by 0.154%. The effect of digital transformation on employment growth is negative and significant at the 1% level. A one standard deviation increase in digital transformation decreases employment growth by 0.099%. The effect of digital transformation on value-added growth is positive and significant at the 1% level. A one standard deviation increase in digital transformation increases value-added growth by 0.197%. The results for the other four industries are similar. In the information and communication industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 5% level. In the physical-transform industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 10% level. In the digital-transform industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 5% level. In the other services industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 1% level.

The results of the system models are similar to the results of the first-differenced models. In the automatic industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level. A one standard deviation increase in digital transformation increases labor productivity by 0.159%. The effect of digital transformation on employment growth is negative and significant at the 1% level. A one standard deviation increase in digital transformation decreases employment growth by 0.103%. The effect of digital transformation on value-added growth is positive and significant at the 1% level. A one standard deviation increase in digital transformation increases value-added growth by 0.206%. The results for the other four industries are similar. In the information and communication industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 10% level. In the physical-transform industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 10% level. In the digital-transform industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 10% level. In the other services industry, the effect of digital transformation on labor productivity is positive and significant at the 1% level, while the effect of digital transformation on employment growth is negative and significant at the 5% level.","We analyzed the relationship between privacy concerns, government subsidies for SSCCs, consumers? disclosure intentions, and purchase intentions when adopting SSCCs. Government subsidies significantly affected consumers? disclosure intentions and purchase intentions when adopting SSCCs (H7 and H8 supported). Table 10 and Fig. 7 show the results of the effect of government subsidies. The mediation effects of disclosure intentions between privacy concerns and purchase intentions when adopting SSCCs had the same results as the previous output because privacy concerns negatively influenced disclosure intentions (H1 supported) that positively affected purchase intentions (H3 supported). Privacy concerns did not directly affect consumers? purchase intentions (H2 not supported).",20.30%,78.70%,78.58%,36.00%,79.28%
224,295.jpg," The figure shows the research model. The model proposes that utilitarian gratifications (H1), hedonic gratifications (H2), and social gratifications (H3) have a positive effect on consumers’ state anxiety (H4), which in turn has a positive effect on in-store purchase intention."," Fig. B.1 shows the moderated?mediation model and its index, -0.09, which was calculated by SPSS Process 3.5 with 5000 bootstrap samples. Because the moderated?mediation effects of the fatal-accident rate had a negative relationship with privacy concerns, the index value was negative. The zero did not fall in the interval between BootLLCI and BootULCI, which indicates that the fatal-accident rate of SSCCs played a significant role as a moderated mediator. We found that the conditional direct effects of the fatal-accident rate of SSCCs were one of two moderated mediations in the relationship between consumers? privacy concerns and purchase intentions. Because the effect of SSCC privacy concerns on purchase intentions occurred through consumers? information-disclosure intentions and interacted with the fatal-accident rate, we called the moderated?mediation effect ?conditional indirect effects.?",25.79%,69.57%,82.29%,6.44%,81.31%
225,296.jpg," Table 1. CFA Results

Latent Variables/Indicators                                  |   Loadings   |   CR   |   AVE   |
-------------------------------------------------------|-------------|-------|--------|
**Consumer State Anxiety**
•	Uncertainty avoidance (CON1)                     |   0.755   |   0.891   |   0.634   |
•	Consumer second-order (CON2)                     |   0.823   |   0.868   |   0.711   |
•	Buying journey difficulty (CON3)                 |   0.691   |   0.871   |   0.609   |
•	Opinion seeking via social media (OS1)             |   0.623   |   0.786   |   0.531   |
•	Opinion sharing via social media (OS2)             |   0.694   |   0.780   |   0.574   |
•	Information seeking related to a P                 |   0.727   |   0.855   |   0.625   |
•	Branded vs. generic product (OS4)                 |   0.634   |   0.733   |   0.464   |
•	Mobile payment (OS5)                               |   0.802   |   0.852   |   0.724   |
•	Self-service (OS6)                                 |   0.627   |   0.736   |   0.480   |
•	Quick payment (OS7)                               |   0.785   |   0.863   |   0.686   |
•	M-retailing (OS8)                                 |   0.564   |   0.706   |   0.400   |
•	M-banking functional value (OS9)                   |   0.850   |   0.924   |   0.806   |
•	M-banking perceived risk (OS10)                    |   0.736   |   0.839   |   0.614   |
**Utilitarian Gratifications**
•	Hedonic gratifications (HG1)                        |   0.774   |   0.895   |   0.790   |
•	Social gratifications (SG1)                        |   0.861   |   0.927   |   0.839   |
**Consumer Second-order (CSO)**
•	Resale1 (RS1)                                      |   0.757   |   0.865   |   0.716   |
•	Resale2 (RS2)                                      |   0.859   |   0.909   |   0.826   |
•	Passing time (PT)                                  |   0.528   |   0.709   |   0.370   |
•	Socializing (SO)                                   |   0.856   |   0.924   |   0.836   |
•	Convenience (CN)                                  |   0.700   |   0.857   |   0.647   |
•	Control (CO)                                       |   0.770   |   0.879   |   0.705   |
•	Instant gratification (IG)                         |   0.741   |   0.867   |   0.673   |
•	Jobs-to-be-done (JTBD)                             |   0.735   |   0.871   |   0.667   |
**Note.** CFA = confirmatory factor analysis; CR = composite reliability; AVE = average variance extracted; HG = hedonic gratifications; SG = social gratifications; RS = resale; PT = passing time; SO = socializing; CN = convenience; CO = control; IG = instant gratification; JTBD = jobs-to-be-done. All factor loadings are significant at p < 0.001.","The environmental performance of SSCCs, the second sustainability type, conditionally influenced SSCC privacy concerns on disclosure intentions and privacy concerns on purchase intentions, respectively (H6a and H6b supported). The moderated?mediation index of the environmental performance of SSCCs was 0.08, calculation of SPSS Process 3.5 with 5000 bootstrap samples (see Fig. B.2). Because the interval between BootLLCI and BootULCI did not include the zero point, we concluded that the moderated?mediation effects were significant. ",0.69%,24.99%,72.35%,0.00%,58.19%
226,297.jpg," The table shows the composite reliability (CR), average variance extracted (AVE), mean, and correlations among the constructs. All the CR values are greater than 0.7, indicating good reliability. All the AVE values are greater than 0.5, indicating good convergent validity. The mean values show that consumers have a slightly higher level of social anxiety (M = 3.38) than hedonic anxiety (M = 2.66), utilitarian anxiety (M = 3.07), and CSA (M = 2.98). The correlations among the constructs are all significant (p < 0.01), indicating good discriminant validity.","The third sustainability type, SSCCs? fuel and system efficiency, also conditionally affected SSCC privacy concerns on disclosure intentions and purchase intentions, respectively (H6a and H6b supported). The moderated?mediation index of the fuel and system efficiency of SSCCs was -0.07, and both BootLLCI and BootULCI were negative values, which means the moderated?mediation effects were significant based on SPSS Process 3.5 with 5000 bootstrap samples (Fig. B.3). ",21.06%,85.03%,81.78%,39.39%,74.93%
227,298.jpg," Table 4.

Structural model results.","After the participants were given their specific manipulations, they answered manipulation-check questions. These questions enabled us to determine whether participants remembered and understood the manipulations they were given. Table 4 shows the number of samples, means, and standard deviations for each variable. We provided manipulation checks in the following two ways.",3.05%,76.93%,84.81%,0.00%,80.29%
228,299.jpg," The table shows the results of a mediation analysis. The first column shows the mediation paths. The second column shows the direct effect of the independent variable on the dependent variable without the mediator. The third column shows the indirect effect of the independent variable on the dependent variable through the mediator. The fourth column shows the direct effect of the independent variable on the dependent variable with the mediator. The fifth column shows the t-value for the indirect effect.

The results show that the indirect effect of UG on PI through CSA is significant (t = 3.303, p < 0.05), and the direct effect of UG on PI with CSA is also significant (t = 3.303, p < 0.05). This indicates that CSA partially mediates the relationship between UG and PI.

The results also show that the indirect effect of HG on PI through CSA is significant (t = 2.649, p < 0.05), and the direct effect of HG on PI with CSA is also significant (t = 2.649, p < 0.05). This indicates that CSA partially mediates the relationship between HG and PI.

The results also show that the indirect effect of SG on PI through CSA is significant (t = 2.799, p < 0.05), and the direct effect of SG on PI with CSA is also significant (t = 2.799, p < 0.05). This indicates that CSA partially mediates the relationship between SG and PI.","We conducted preliminary tests to assess the reliability and validity of the responses. The measurement model analyses involved the reliabilities for each correlation alpha (CRA), called Cronbach's alpha (a). Table 5 shows that all the scores were over the threshold of 0.7 [20,24]. Based on correlation matrix analyses, there were no critical issues regarding convergent validity and discriminant validity (see Fig. B.1). Because all average variances extracted (AVEs) were greater than 0.5, there were no convergent validity issues. The AVE square roots were greater than interconstruct correlations, which also indicated discriminant validity. The total number of violations was less than one-half of the potential comparisons [17]. We also tested the variance inflation factor to examine for potential multicollinearity. Based on the recommended value of 5, it was good in all cases except FE (FA = 2.2, EV = 2.1, FE = 5.2, GS. = 4.8, PC = 1.1) [40]. We tested for and ruled out common method bias using a marker variable and common latent factors in AMOS [72]. Finally, the results of confirmatory factor analysis (CFA) for sustainability demonstrated strong model fit statistics; the root mean square error of approximation (RMSEA) value was 0.043, which is lower than 0.07 [93]; the comparative fit index (CFI) value was 0.985, which is greater than 0.90; the Tucker?Lewis index value (TLI) was 0.982, also greater than 0.90.",41.60%,88.92%,80.27%,45.66%,84.24%
229,30.jpg, The diagram shows how to create different presentations for different social media platforms. The first step is to use a persona generator to create different personas for each social media platform. The second step is to use a content generator to create different content for each persona. The third step is to use a format generator to create different formats for each piece of content. The final step is to use a social media poster to post the content to each social media platform.,"As Table 6 shows, to analyze the mediation effects of disclosure intentions between privacy concerns and purchase intentions, we used two tools: SPSS Process and AMOS. The effects of SSCC privacy concerns on disclosure intentions (t(454) = -8.82, p < 0) and disclosure intentions on SSCC purchase intentions (t(454) = 15.16, p < 0) were significant (see Table B.2 and Table B.3). We concluded that there were indirect effects of disclosure intentions between privacy concerns and purchase intentions because the interval between the lower level of confidence interval (BootLLCI) and the upper level of confidence interval (BootULCI) did not include zero (see Table B.3). The results of the model fit (GFI = 0.98, AGFI = 0.96, CFI = 0.99, RMSEA = 0.03) indicated that there was good model fit between the proposed model and the data, such that the data does not require re-specification (see Table B.4).",12.14%,51.54%,79.62%,17.62%,90.47%
230,300.jpg," The diagram shows the research methodology that will be used in this study. The research process will be divided into three main stages: 
1. Define and design
2. Prepare, collect and analyse
3. Analyse and conclude. 

The first stage will involve conducting a literature review and developing a theoretical framework. This will be followed by the selection of cases and the development of a data collection plan. 

The second stage will involve conducting exploratory case studies and writing case reports. The third stage will involve analysing the data, drawing conclusions, and writing a final report.","We analyzed the relationship between privacy concerns, government subsidies for SSCCs, consumers? disclosure intentions, and purchase intentions when adopting SSCCs. Government subsidies significantly affected consumers? disclosure intentions and purchase intentions when adopting SSCCs (H7 and H8 supported). Table 10 and Fig. 7 show the results of the effect of government subsidies. The mediation effects of disclosure intentions between privacy concerns and purchase intentions when adopting SSCCs had the same results as the previous output because privacy concerns negatively influenced disclosure intentions (H1 supported) that positively affected purchase intentions (H3 supported). Privacy concerns did not directly affect consumers? purchase intentions (H2 not supported).",19.49%,84.40%,80.48%,39.44%,79.45%
231,301.jpg," The table shows the relationships between the first-order concepts, second-order themes, and aggregate dimensions in the study. The first-order concepts are the specific ideas or statements that emerged from the data, while the second-order themes are the broader categories that these concepts fit into. The aggregate dimensions are the highest level of abstraction and represent the overall themes of the study.

The table shows that there are four aggregate dimensions in the study: Pressures promoting adoption of hyped technology, Segmented perception of hyped technology, Trust-building mechanisms, and WIIFM. These dimensions are based on the relationships between the first-order concepts and second-order themes. For example, the first-order concept of ""Efficiency gain"" is related to the second-order theme of ""Competitive pressure"", which is in turn related to the aggregate dimension of ""Pressures promoting adoption of hyped technology"".

The table also shows that there are a number of relationships between the first-order concepts and second-order themes. For example, the first-order concept of ""A single source of truth"" is related to the second-order theme of ""Optimistic perception"", which is in turn related to the aggregate dimension of ""Trust-building mechanisms"".

The findings of the study suggest that there are a number of factors that influence the adoption of hyped technology in organizations. These factors include competitive pressure, the perceived benefits of the technology, and the trust that organizations have in the technology. The study also suggests that organizations need to be aware of the potential risks associated with hyped technology, such as the risk of being locked into a particular technology or the risk of investing in a technology that does not meet their needs.","Further to the main independent variables, we provide a set of campaign-specific factors to control the model, consistent with Vismara [43] and Nguyen et al. [37]. For instance, we use size of the management team to capture ta project's human capital, while the dummy variable patent indicates the existence of a patent in the project documents and is used as a proxy for projects? intellectual capital. The variable active campaign encompasses parallel projects that raise funds at the same time, which potentially lead to less daily crowdfunding investment in target projects. Some empirical findings indicate that parallel projects diminish support from investors in equity crowdfunding [43] and lenders within lending platforms [18] as well as backers in reward-based crowdfunding [51]. Table 1 provides detailed definitions and descriptive statistics for all variables used. Regarding the number of key statistics in Table 1, on the average, projects attract about 5 investors and raise nearly £10,000 daily. While the daily average number of investors in equity crowdfunding is comparable to that in other types of crowdfunding markets, such as reward-based crowdfunding [45] or lending-based crowdfunding [24,49], their daily fundraising volume is much higher. This indicates the important and potential role of equity crowdfunding in providing capital to young entrepreneurs. Our sample statistics are, to a large extent, consistent with samples of equity crowdfunding projects from other papers [23,42,43].",42.97%,85.62%,80.97%,48.89%,80.55%
232,302.jpg," This figure shows a model of technological hype. The model suggests that technological hype is a process that involves three stages: (1) **Adoption of hyped technology**, (2) **Interpretation of hyped technology**, and (3) **Trust-building mechanisms.**

The first stage, adoption of hyped technology, is driven by a number of factors, including **pressure to adopt hyped technology**, **segmented perception**, and **competitive pressures.**

The second stage, interpretation of hyped technology, is also driven by a number of factors, including **segmented perception** and **trust-building mechanisms.**

The third stage, trust-building mechanisms, is driven by a number of factors, including **trust influencers**, **de-hyping**, and **reality check.**",We provide a correlation matrix among independent variables in Table 2. The correlation coefficients satisfy the condition of no multicollinearity in the model. We have no pair of variables that are highly correlated.,15.56%,54.43%,80.65%,36.67%,70.52%
233,303.jpg," The figure shows the research model. The model proposes that leader firms’ remote work strategy positively affects their survival remote work strategy, which in turn, positively affects their remote work initiatives in t3. The model also proposes that agile firms’ remote work strategy positively affects their survival remote work strategy, which in turn, positively affects their remote work initiatives in t3. Finally, the model proposes that leader and agile firms have a competitive advantage in remote work over their competitors.

The model is based on the following hypotheses:

H1: Leader firms’ remote work strategy positively affects their survival remote work strategy.
H2: Agile firms’ remote work strategy positively affects their survival remote work strategy.
H3: Leader and agile firms have a competitive advantage in remote work over their competitors.","Table 3 reports the results from our different panel regressions on the presence of herding dynamics in equity crowdfunding. Model 1 shows the results from curvilinear regression (specification 2), while models 2 and 3 report the outcomes of the linear regression in the first and last periods of the crowdfunding campaigns (specification 1). As the dependent variable of the daily number of investors is a non-negative integer, we first use random-effect negative binominal panel regressions in models 1?3 to control for overdispersion. Furthermore, following Xiao [45], we replicate models 1?3 in models 4?6 using random-effect OLS regressions with natural log of daily number of investors as the dependent variable. To confirm the robustness of the results, we also run model 4?6 using fixed-effect regressions. The results appear to be consistent.",14.02%,76.90%,81.06%,41.79%,76.55%
234,304.jpg, Table 3. Correlation between remote work and COVID-19,"To illustrate the dynamics of these information sources throughout the funding process, Table 4 presents different statistics (mean, median, max, min) on the number of discussions and of Facebook and Twitter posts at the end of the first stage and the last stage of the funding cycle. Further to Table 4, Graph 1 shows the average growth rate of number of posts from these sources. The growth rate for each project is calculated as where, NDFTPLS is the number of discussions, Facebook, and Twitter posts at the end of the last stage, and NDFTPFS is the number of discussions, Facebook, and Twitter posts at the end of the first stage.",6.66%,79.15%,82.67%,0.00%,62.03%
235,305.jpg," Table 3. Results of the hypotheses testing (n = 238)

Hypothesis	Model	Direct effect	Indirect effect	Total effect	Adj. R2	F	p	95% CI (bias-corrected)
H1	Remote work flexibility → Remote work initiatives	0.278	0.091	0.369	0.586	10.226	0.002	[0.109, 0.411]
H2	Remote work initiatives → Firm’s remote work initiatives	0.294	0.102	0.396	0.622	11.098	0.001	[0.183, 0.489]
H3	Firm’s remote work initiatives → Remote work initiatives	0.104	0.202	0.306	0.550	9.014	0.003	[0.126, 0.418]
H4	Remote work flexibility → Firm’s remote work initiatives	0.056	0.254	0.310	0.565	9.566	0.002	[0.134, 0.422]
H5	Remote work flexibility → Remote work initiatives → Firm’s remote work initiatives	–	0.085	0.085	–	–	–	–	–
H6	Remote work flexibility → Firm’s remote work initiatives → Remote work initiatives	–	0.062	0.062	–	–	–	–	–
H7	Remote work initiatives → Firm’s remote work initiatives → Remote work flexibility	–	–	–	–	–	–	–	–
H8	Remote work flexibility → Remote work initiatives → Firm’s remote work initiatives → Remote work flexibility	–	–	–	–	–	–	–	–","We ran a series of different tests to confirm the robustness of our analysis. First, we constructed an interaction variable between the logarithm forms of lag investors and days available, which measures the number of days remaining in a funding campaign. We replicate the main analysis using this new interaction term in negative binominal and OLS specifications. Models 1 and 2 of Table 6 show that the interaction term is negative and statistically significant, suggesting that herding momentum is more prevalent toward the final days of the funding campaign (i.e., the number of days available is getting smaller). These results are consistent with our previous findings.",1.71%,59.02%,77.07%,34.44%,48.14%
236,306.jpg," The table shows the correlation matrix of the variables used in the analysis. The variables are:

1. Remote work firm's initiatives in t1
2. Remote work firm's initiatives in t2
3. Remote work firm's initiatives in t3
4. Firm size
5. Firm's RSE in remote work in t1
6. Firm's RSE in remote work in t2
7. Firm's RSE in remote work in t3
8. Industry's average RSE in t1
9. Industry's average RSE in t2
10. Industry's average RSE in t3
11. Industry

The correlation matrix shows that there is a strong positive correlation between the firm's remote work initiatives in t1 and t2 (r = 0.411) and between the firm's remote work initiatives in t2 and t3 (r = 0.506). This suggests that firms that are more proactive in implementing remote work initiatives in one year are more likely to continue to do so in subsequent years.

There is also a strong positive correlation between the firm's RSE in remote work in t1 and t2 (r = 0.705) and between the firm's RSE in remote work in t2 and t3 (r = 0.775). This suggests that firms that have a higher level of RSE in remote work in one year are more likely to have a higher level of RSE in remote work in subsequent years.

There is a negative correlation between the firm's remote work initiatives in t1 and the firm's RSE in remote work in t1 (r = -0.233). This suggests that firms that are more proactive in implementing remote work initiatives are less likely to have a high level of RSE in remote work.

There is also a negative correlation between the firm's remote work initiatives in t2 and the firm's RSE in remote work in t2 (r = -0.164) and between the firm's remote work initiatives in t3 and the firm's RSE in remote work in t3 (r = -0.211). This suggests that firms that are more proactive in implementing remote work initiatives are less likely to have a high level of RSE in remote work in subsequent years.

These findings suggest that firms that are more proactive in implementing remote work initiatives are more likely to have a lower level of RSE in remote work. This is because remote work initiatives can help to reduce the costs associated with RSE, such as the costs of recruiting and training new employees and the costs of providing office space.","We also replicate our main analysis using an alternative measure for herding, which is the momentum of daily funding amount and total prior funding amount, as used in the prior literature [24,45]. These measures are considered to be good alternative proxies, as information on funding amounts is publicly available in crowdfunding platforms, so as investors can use it in their funding decision-making. Indeed, as discussed in Zhang and Liu [49], investors may herd to solve two key questions of whether they should invest or not and if so, how much they should contribute. Table 7 replicates our main analysis from Table 3 using funding amount momentum as an alternative measure of herding behavior. The results from the robustness checks are largely consistent with the main finding that herding only occurs in the last stages of those funding campaigns. The results from replicating Table 3 using an alternative measure of herding are also robust.",23.15%,81.01%,79.13%,39.27%,79.08%
237,307.jpg," The Venn diagram shows the relationship between the customer, booking, and destination. The customer is the person who makes the booking. The booking is the reservation for a room at a hotel or other accommodation. The destination is the place where the customer is traveling to.

The initial assumption is that the upgrade acceptance is a combination of the upsell offer, plus factors concerning the customer, booking, and destination. This assumption is upheld through analysis.

It can be quite challenging to balance these features automatically using a rule-based approach. This is because there are many factors to consider, and it can be difficult to determine how much weight to give to each factor.","Finally, we extend our main analysis to another important UK equity platform?Seedrs. We replicate our main tests in a sample of 80 projects, listed in Seedrs during 2017?2018, as a method for assessing the validity of our main findings with out-of-sample data. Using a smaller set of control variables than for the Crowdcube projects, our results, illustrated in Table 8, suggest similar herding dynamics among Seedrs projects, with herding momentum appearing strongly in the final stage of the funding campaigns, and confirm our original findings.",17.78%,90.70%,82.44%,39.82%,80.70%
238,308.jpg," This is a decision tree that models the process of a customer booking an economy trip and the subsequent steps taken by the company.

The process starts with the customer booking an economy trip. The company then determines whether or not the customer is eligible for an upgrade. If the customer is eligible, the company sends an email to the customer with an upgrade offer. The customer can then either accept or ignore the offer. If the customer accepts the offer, the company upgrades the customer to premium class. If the customer ignores the offer, the company takes no action.","Building on the preceding hypotheses, our research model displayed in Fig. 1 explains users? intentions to continue using museum social media official accounts and brand pages, considering the extent to which they perceive certain gratifications (i.e., informational value, entertainment value and social value) as fulfilled through this use. The model also explores certain socio-demographic specificities related to users (i.e., age, gender), as research identifies them as significant in shaping different patterns of social media usage [121,[130], [131], [132], [133], [134], [135]]. To be coherent with the procedure and results of our previous study 2, the population targeted is the digital natives. We respond by doing so to the call of many studies for more research about the ways young people choose to interact with museums and how to attract them [122,136]. H4: Attitude toward digitally supported museum visits is positively related to a museum's social media continuance intention. H5a: Attitude toward digitally supported museum visits moderates the effect of informational value on social media continuance intention. H5b: Attitude toward digitally supported museum visits moderates the effect of entertainment value on social media continuance intention. H5c: Attitude toward digitally supported museum visits moderates the effect of social value on social media continuance intention.H3: Social value is positively related to a museum's social media continuance intention. H2: Entertainment value is positively related to a museum's social media continuance intention. H1: Informational value is positively related to a museum's social media continuance intention.",21.49%,74.15%,79.59%,12.17%,85.57%
239,309.jpg," The diagram shows the architecture of the PREM price elasticity model. 

The model consists of the following components:

* Data Input
* Feature Engineering
* Feature Embedding
* Cost-Sensitive Classification
* Upgrade Offer Maximizer
* Offer Acceptance Prediction Framework

The Data Input component ingests data from various sources, including destination data, customer data, flight data, offer data, third-party data, and generated data.

The Feature Engineering component then extracts features from the raw data. 

The Feature Embedding component converts the categorical features into numerical vectors.

The Cost-Sensitive Classification component uses a cost-sensitive classification model to predict the probability that a customer will accept an upgrade offer.

The Upgrade Offer Maximizer component uses a revenue maximization model to determine the optimal upgrade offer for each customer.

The Offer Acceptance Prediction Framework combines the predictions from the Cost-Sensitive Classification component and the Upgrade Offer Maximizer component to predict the probability that a customer will accept an upgrade offer.","As shown in Fig. 2, our research design allows us to address a variety of views and perceptions that belong to the two groups of actors who co-create the museum experience: museum professionals and visitors. Study 1 confirmed the enabling potential of ICT which are mainly employed from receptive and interactive mediation perspectives [19,20]. Social media technologies are specifically recognized as powerful transformative tools. Characterized by immediacy, interactivity and massive reach, they can improve visibility and accessibility to audiences and foster collaborative mediation patterns [67].",16.95%,85.63%,79.43%,38.92%,77.59%
240,31.jpg, The figure shows the two main stages of our proposed approach. (a) Domain Data Ingestion and (b) Question Answering.,"By applying the retroduction procedure [29,43] and the inductive methodology for the analysis of qualitative data [48], we identified multiple categories, including the self- and others-oriented affordances that potentially explain differences in users? Instagram experiences. The categories presented in the previous subsections were related to one another and formed an affordances-actualizations-outcomes model, as illustrated in Fig. 1. The proposed graphical model (Fig. 1) reflects the layered critical realism ontology. The core categories ?SNS Affordances? and ?Context Characteristics? are represented by ovals with dashed borders. The dashed borders symbolize the real layer of structures that include material objects as well as social and conceptual entities (thoughts, goals, and intentions). ",28.99%,74.23%,82.71%,0.47%,85.61%
241,310.jpg," | Year | Bookings | Offers sent | Offers accepted | Reach rate | Conversion rate |
|---|---|---|---|---|---|
| 2017 | 15.2 M | 2.5 M | 42 K | 16% | 1.67% |
| 2018 | 30.9 M | 7.5 M | 99 K | 24% | 1.32% |
| 2019 | 18.2 M | 4.1 M | 53 K | 23% | 1.29% |
| All | 64.3 M | 14.1 M | 194 K | 22% | 1.43% |","Therefore, as the next step in the candidate mechanism and model validation, we applied quantitative methods and tested the relationships between the actualization of identified affordance groups (self-oriented and others-oriented) and outcomes. Based on the proposed model and, in particular, the nature of user motives behind self- and others-oriented affordances, we put forth and tested the following hypotheses (Fig. 2).",0.00%,9.10%,77.85%,0.00%,36.11%
242,311.jpg," From the table, we can see that the best feature embedding technique is the Denoising AutoEncoder, which achieves an F1 score of 83.9 and a revenue capture of 100%.","The items for the actualization of others- and self-oriented affordances (OAA and SAA, respectively) were developed in an iterative process based on the interview transcripts and often included the exact wording of the participants. Items for measuring the actualizations of affordances were developed for each user's goal-feature combination [57] and assessed on a 5-point Likert scale (1 = strongly disagree; 5 = strongly agree). If participants had trouble understanding an item, they could indicate this by choosing the option ?I don't understand the statement.? In the questionnaire, the items for the actualization of others- and self-oriented affordances were mixed and displayed in random order. Table 4 contains the final survey items with descriptive measures.",33.29%,74.75%,82.10%,2.76%,76.07%
243,312.jpg," The table shows the F1 score and revenue capture for different embedding sizes. The F1 score is a measure of the accuracy of the model, and the revenue capture is the percentage of revenue that the model is able to capture.

As the embedding size increases, the F1 score and revenue capture both increase. This is because a larger embedding size allows the model to learn more complex relationships between the features. However, the revenue capture does not increase as much as the F1 score, which means that the model is not able to capture as much revenue as it could with a smaller embedding size.

The optimal embedding size is 256, as it provides good F1 score and revenue capture.","Given that the measurement models of our predictors?OAA and SAA?are formative, specific criteria for assessing formative measurement models need to be applied. Some researchers propose that item weights that are significant at the 0.05 level and greater than 0.1 demonstrate high relevance for the formative construct [59]. Not all of our indicators met this criterion (Table 5). However, a simulation study by He [67] shows that it might be misleading and overly simplistic to assess indicator validity based on item weights only. He [67] argues that the contribution of individual items to the formative construct might be obscured by shared variance between indicators. This variance affects neither the predictive power nor the reliability of the construct. Despite the diverging views, there is an agreement not to omit items in formative models purely on statistical grounds. Dropping items alters the meaning of the formative construct and therefore is not recommended [58,59,63,67]. Hence, we kept all items, as each one covered a specific aspect of the constructs that emerged in our qualitative study. Another criterion for indicator validity is variance inflation factors (VIF) below 10 [59,68]. All VIFs of our items were below 1.6, which indicates that multicollinearity is not a problem (Table 5).",34.88%,80.36%,81.34%,28.63%,85.91%
244,313.jpg," The table shows the F1 score and revenue capture for different classification models and feature sets. The best model is the first one, which uses a deep learning model with embeddings. It has an F1 score of 83.9 and a revenue capture of 100%. The second-best model is the logistic regression model with embeddings, which has an F1 score of 74.7 and a revenue capture of 97%. The third-best model is the support vector machine model with embeddings, which has an F1 score of 78.3 and a revenue capture of 98%. The fourth-best model is the random forest model with embeddings, which has an F1 score of 79.4 and a revenue capture of 98%. The fifth-best model is the random forest model with the original data, which has an F1 score of 56.3 and a revenue capture of 73%. The sixth-best model is the deep learning model with one-hot encoding of the original data, which has an F1 score of 44.3 and a revenue capture of 62%. The seventh-best model is the random forest model with SMOTE, which has an F1 score of 53.4 and a revenue capture of 46%. The eighth-best model is the random forest model with oversample 1:10, which has an F1 score of 30.2 and a revenue capture of 48%. The ninth-best model is the random forest model with oversample 1:100, which has an F1 score of 42.3 and a revenue capture of 59%. The tenth-best model is the random forest model with GANs, which has an F1 score of 56.4 and a revenue capture of 62%.","Intermittent discontinuance in STSF is a complex psychological and behavioral process, and understanding this process has positive implications for the operation and management of social media. On the one hand, it is related to the operational goals of social commerce and social service [10]. Generally, the relationship between brands and users is characterized by weak ties [13]. However, STSF can provide a carrier of strong social relations to help brand service providers transform weak ties into strong ones, thereby realizing better social marketing and value co-creation. On the other hand, it can help to uncover the black box of intermittent discontinuance, which makes timely identification, monitoring, and improvement in service quality and user experience convenient for a platform. Fig. 1 illustrates the details of this process.",30.80%,69.03%,79.03%,40.74%,78.61%
245,314.jpg," The table shows the F1 score and revenue capture for three different customer segmentation models. The best model is the Binary AutoEncoder, which has an F1 score of 83.9 and a revenue capture of 100%. The next best model is the K-Means model, which has an F1 score of 64.9 and a revenue capture of 79%. The worst model is the Decision Tree model, which has an F1 score of 53.6 and a revenue capture of 73%.","Negative and positive expectancy violations of the current STSF produce push and pull effects, respectively. Positive expectancy violations of the current STSF and negative expectancy violations of the competitive STSF together produce a mooring effect. The push and pull effects act as discontinuance factors for the current STSF, whereas the mooring effect acts as a continuance factor; these act together to produce intermittent discontinuance. Fig. 2 illustrates the mechanism of intermittent discontinuance of STSF under the EVT and PPM models.",29.74%,67.76%,81.68%,47.05%,78.20%
246,315.jpg," The table shows the revenue maximization, F1 score, and revenue capture of two methods: ILP and Greedy.

ILP outperforms Greedy in all three metrics. Specifically, ILP achieves an F1 score of 83.9, while Greedy only achieves 49.2. In terms of revenue capture, ILP captures 100% of the revenue, while Greedy only captures 56%.","H1: Information overload positively affects STSF usage fatigue. H2: Social overload positively affects STSF usage fatigue. H3: System feature overload positively affects STSF usage fatigue. H4: Usage fatigue positively affects intermittent discontinuance in STSF. H5: Privacy concerns positively influence intermittent discontinuance in STSF. H6: Alternative attractiveness positively influences intermittent discontinuance in STSF. H7: Sunk costs positively influence a user's transition fatigue in STSF. H8: Transition costs positively influence a user's transition fatigue in STSF. H9: Transition fatigue negatively influences intermittent discontinuance in STSF. H10: Transition fatigue has a mediating effect on the relationship between sunk costs and intermittent discontinuance in STSF. H11: Transition fatigue has a mediating effect on the relationship between transition costs and intermittent discontinuances in STSF. Furthermore, we control for three demographic variables, including sex, education, and age. Other control variables include duration of use and the number of official accounts followed, contacts, and groups. Based on the above analysis, a conceptual model was developed, as shown in Fig. 3.",9.70%,78.39%,80.34%,5.45%,76.59%
247,316.jpg," The table shows the F1 score and revenue capture of different variants of the PREM model. The F1 score is a measure of the accuracy of the model, while the revenue capture is a measure of how much revenue the model can generate.

The results show that the PREM model with all components achieves the highest F1 score and revenue capture. This suggests that all components of the model are important for its performance.

The PREM model with only the classification and upgrade components achieves a lower F1 score and revenue capture than the model with all components. This suggests that the upgrade and revenue maximizer components are important for the performance of the model.

The PREM model with only the upgrade and revenue maximizer components achieves a lower F1 score and revenue capture than the model with all components. This suggests that the classification and upgrade components are important for the performance of the model.

Overall, the results show that all components of the PREM model are important for its performance.","The results of the structural model and hypothesis tests are presented in Fig. 4 and Table 7. As shown in Fig. 4, the corresponding R2 values for intermittent discontinuance, usage fatigue, and transition fatigue are 0.298, 0.332, and 0.651, respectively, indicating that the structural model has a good fit [75].",40.43%,82.28%,82.74%,39.92%,80.52%
248,317.jpg," The image shows the scenarios used in a study on fake news believability. The scenarios were designed to be relevant to people in the United States and India. The scenarios were presented to participants in a survey, and they were asked to rate how believable they found each scenario to be. The results of the study showed that people in the United States and India were equally likely to believe fake news. This suggests that people in both countries are susceptible to believing fake news, and that it is important to be aware of this issue."," Table 2 provides the sample profile information. There were 47 % male and 53 % female participants. More than 80 % of the participants were aged between 18 and 25 years, which is consistent with the statistics of the China Internet Network Information Center (CNNIC) [64] on social media users in China.",27.13%,48.53%,83.89%,43.06%,76.17%
249,318.jpg," From the table, we can see that the highest correlation between two variables is between COL_2 and CONS_2, with a correlation coefficient of 0.635.","The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem.",43.07%,85.31%,83.14%,0.01%,82.34%
250,319.jpg," The table shows the results of a multiple regression analysis. The dependent variable is not shown, but the independent variables are:

* Conservatism
* Collectivism
* Gender (0 = F, 1 = M)
* Age
* Internet usage
* Country (0 = US, 1 = India)

The results show that conservatism, collectivism, age, internet usage, and country are all significant predictors of the dependent variable. Gender is not a significant predictor.","The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem.",41.28%,86.20%,79.53%,5.50%,84.69%
251,32.jpg,Error: Invalid response from model,"The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem.",3.00%,63.66%,78.66%,0.00%,78.79%
252,320.jpg, **Figure 1.** The research framework.,"VIF values and item weights were examined to assess the reliability and validity of formative variables [69]. The reliability of the variable is considered adequate for VIF values lower than 3.00. If the item weights are significant at the statistical level, that is, if the t-values are greater than 1.960, then the validity test is satisfied. According to the results in Table 6, the formative variable (i.e., intermittent discontinuance) has good reliability and validity.",19.61%,46.24%,82.67%,0.00%,76.67%
253,321.jpg," Correlation is a statistical measure that shows the relationship between two variables. It is expressed as a number between -1 and 1, where -1 indicates a perfect negative correlation, 0 indicates no correlation, and 1 indicates a perfect positive correlation.

In the context of the given data, the correlation matrix shows the relationship between 10 variables: seat utilization, concentration, online ratio, vertical relationship, chain size, price, seat, screen, age, and density.

The highest positive correlation is between seat utilization and chain size (0.88), which means that as the chain size increases, the seat utilization also increases. Other strong positive correlations include concentration and online ratio (0.76), concentration and price (0.74), and seat and screen (0.73).

The strongest negative correlation is between seat utilization and age (-0.35), which means that as age increases, seat utilization decreases. Other strong negative correlations include concentration and vertical relationship (-0.67), concentration and age (-0.55), and online ratio and age (-0.51).

It is important to note that correlation does not imply causation. While the correlation matrix can show relationships between variables, it does not provide any information about the direction of causality.","The results of the structural model and hypothesis tests are presented in Fig. 4 and Table 7. As shown in Fig. 4, the corresponding R2 values for intermittent discontinuance, usage fatigue, and transition fatigue are 0.298, 0.332, and 0.651, respectively, indicating that the structural model has a good fit [75]. Table 7 indicates that all hypotheses are supported except for H3. Information (ß=0.450, t = 8.940, P<0.001) and system feature (ß=0.197, t = 3.671, P<0.001) overload positively and significantly affect usage fatigue, supporting H1 and H2. ",28.14%,93.70%,81.41%,39.07%,81.43%
254,322.jpg," Table 6 reports the results of the regression analyses predicting seat utilization and concentration. Model 1 shows that among the control variables, only seat utilization in the previous time period is a significant predictor of current seat utilization. Among the predictor variables, the ratio of online to offline prices is a significant predictor of seat utilization, and the coefficient is negative. This indicates that as the ratio increases, seat utilization decreases. The interaction between online ratio and vertical relationship is also a significant predictor, and the coefficient is positive. This indicates that the negative relationship between online ratio and seat utilization is stronger when the cinema has a vertical relationship with the online ticketing platform.

Model 2 shows that among the control variables, seat utilization and concentration in the previous time period are both significant predictors of current concentration. Among the predictor variables, the ratio of online to offline prices is a significant predictor of concentration, and the coefficient is positive. This indicates that as the ratio increases, concentration increases. The interaction between online ratio and vertical relationship is also a significant predictor, and the coefficient is negative. This indicates that the positive relationship between online ratio and concentration is weaker when the cinema has a vertical relationship with the online ticketing platform.

Overall, the results suggest that the relationship between online ticketing and seat utilization and concentration is complex. The ratio of online to offline prices has a negative effect on seat utilization but a positive effect on concentration. The interaction between online ratio and vertical relationship has a positive effect on seat utilization but a negative effect on concentration. These findings suggest that cinemas need to consider the potential impact of online ticketing on both seat utilization and concentration when making decisions about whether to offer online ticketing.","We conducted a mediation analysis based on the guidelines of Nitzl and Roldan [77] and Zhao et al. [78]. As shown in Table 8, the direct effect of sunk and transition costs is significantly positive, while the indirect effect of transition fatigue is significantly negative. When the direct and indirect effects are both significant and point in opposite directions, the type of mediation is competitive mediation [78]. Therefore, transition fatigue exerts a competitive mediation effect, which weakens the impact of transition and sunk costs on intermittent discontinuance.",37.24%,86.24%,81.96%,40.89%,78.95%
255,323.jpg," Table 6 reports the results of the regression analyses predicting seat utilization and concentration. In Models 1 and 4, the dependent variable is seat utilization, and in Models 2 and 5, it is concentration. Models 1 and 2 include only the control variables, and Models 3 and 6 include the control variables and the variables of interest.

Across all models, the control variables explained a significant amount of variance in both seat utilization and concentration. In Model 1, the control variables explained 54% of the variance in seat utilization, and in Model 2, they explained 61% of the variance in concentration. When the variables of interest were added in Models 3 and 6, the variance explained increased slightly to 56% and 63%, respectively.

For the variables of interest, the results suggest that vertical integration has a significant positive effect on seat utilization but a negative effect on concentration. Specifically, a one-unit increase in vertical integration is associated with a 0.473 increase in seat utilization and a 0.213 decrease in concentration. These results suggest that vertically integrated firms may be more likely to utilize their seats more efficiently and may also be less likely to engage in practices that would lead to higher levels of concentration.

The results for the other variables of interest are mixed. The coefficient for the online ratio is positive and significant in Model 3 but negative and significant in Model 6. This suggests that the relationship between the online ratio and seat utilization/concentration is not linear. The coefficient for the chain size variable is negative and significant in Model 3 but positive and significant in Model 6. This suggests that the relationship between chain size and seat utilization/concentration is also not linear. The coefficient for the theater fixed effects is positive and significant in both Models 3 and 6, suggesting that theaters with more fixed effects have higher levels of seat utilization and concentration.","Necessity analysis, which is used to identify whether the existence of a variable can be considered a necessary condition for a result, must be tested before analyzing sufficient conditional combinations [82]. Table 9 shows that both the consistency and coverage levels of each variable are below the recommended threshold of 0.9 for necessity analysis [80], indicating that the condition variables could not completely explain the outcome variable. Thus, no single condition was necessary for intermittent discontinuance. In conclusion, further analysis of the conditional configuration combinations is required.",35.85%,86.73%,81.36%,40.25%,75.45%
256,324.jpg," Table 10 reports the results of four regressions where the dependent variable is either revenues or audience. In all models, we control for the price of the ticket, the type of seat, the age of the movie, and the density of the theater. In Model 7, we regress revenues on these control variables. The results show that the price of the ticket and the type of seat are significant predictors of revenue. In Model 8, we add the utilization of the seat as a predictor. The results show that the utilization of the seat is a significant predictor of revenue. In Model 9, we regress audience on the control variables and the utilization of the seat. The results show that the price of the ticket, the type of seat, and the utilization of the seat are all significant predictors of audience. In Model 10, we add theater fixed effects and week dummies to Model 9. The results show that the price of the ticket, the type of seat, and the utilization of the seat are still significant predictors of audience.","FsQCA3.0 is used to construct a 2K row truth table, where k is the number of antecedents. The recommended consistency measurement threshold is 0.8 or 0.9 [79]. This study chose 0.9 as a cut-off point to ensure a high degree of consistency at this stage of analysis. Fiss [83] recommended that the frequency threshold should be three when the samples exceed 150. This study set the number of acceptable cases to four. The final results for the configurations leading to high and low intermittent discontinuance are listed, respectively, in Tables 10 and 11.",47.63%,87.58%,80.76%,42.96%,87.85%
257,325.jpg," From the table above, we can see that there are a total of 9 managers in the company. 2 of them are senior managers, 5 of them are middle managers and 2 of them are junior managers. The experience of the senior managers is less than 10 years, the experience of the middle managers is 10-20 years and the experience of the junior managers is more than 20 years.","FsQCA3.0 is used to construct a 2K row truth table, where k is the number of antecedents. The recommended consistency measurement threshold is 0.8 or 0.9 [79]. This study chose 0.9 as a cut-off point to ensure a high degree of consistency at this stage of analysis. Fiss [83] recommended that the frequency threshold should be three when the samples exceed 150. This study set the number of acceptable cases to four. The final results for the configurations leading to high and low intermittent discontinuance are listed, respectively, in Tables 10 and 11.",33.91%,74.20%,81.46%,37.19%,85.22%
258,326.jpg," The figure shows the relationships between DT-enabled dynamic capabilities, market capitalization agility, operational adjustment agility, and firm performance.","We examine an NFT transactions database sourced from Nadini et al. [30]. It spans from June 2017 to April 2021, containing over 6 million transactions from Ethereum and WAX networks. Table 1 summarizes the dataset. We aggregate the data on a monthly basis and run a battery of statistical tests. The underlying data have been sourced from five different NFT places: OpenSea, Atomic, Decentraland, Cryptokitties and Godsunchained. All the NFTs under consideration belong to the categories games, collectibles, metaverse, and art. The dataset is comprehensive and provides information about the date and time of transaction, accounts of the transacting parties, and transaction amount in USD as well as in Cryptocurrency units. Thus, the dataset is robust, statistically pliable, and representative of the general NFT market. The dataset is summarized in Table 1. The cryptocurrency price data of transactions is then exploited in this study to quantitatively assess the extent of the fraudulent practices prevalent in the NFT markets. All three statistical tests used in this study, Benford's test, Student's t-test for clustering and Power-law fitting, are performed on this parameter only. Fischer's test is then performed on the p-values obtained from the previous tests to address the concerns of type-1 error and p-hacking in the analysis. The present work performs analysis of monthly data to ascertain that wash trading is being thoroughly practiced in the NFT markets.",21.90%,89.52%,80.54%,0.00%,73.57%
259,327.jpg," The table shows the frequency and percentage of firm information and respondent information.

For firm information, the majority of the firms were established 10-19 years ago (41.21%), followed by those established 20-29 years ago (27.27%). In terms of the number of employees, most of the firms had less than 100 employees (60.6%). The majority of the firms were privately owned (68.48%), followed by state-owned firms (15.76%). In terms of industry type, the majority of the firms were in the electronics industry (27.27%), followed by the food industry (14.89%).

For respondent information, the majority of the respondents were male (56.97%), and most had a bachelor's degree (69.09%). The majority of the respondents were aged 35-44 years old (38.79%), and most had 5-10 years of working experience (58.79%). The majority of the respondents were top executives (56.36%), followed by business managers (29.70%).","Meanwhile, MAD conformity deals with the average divergence from an actual digit distribution. As per Nigrini and Miller [63], . We use the same author's decision criteria where values between 0.0012 and 0.0018 are acceptable, between 0.0018 and 0.0022 are marginally acceptable, and beyond 0.0022 are labeled as nonconformity. Results from Table 2 show that and scores from Chi-squared and Mantissa Arc tests necessitate rejection of the null hypothesis for all months. As a robustness check, we run a Distortion Factor Model, which is capable of revealing overstatement and understatement in data. We report the magnitude of the distortion of actual values against what Benford's Law expects. Conformity tests on mean absolute deviation (MAD) register positive results for only two months: September and October 2020. Otherwise, overall evidence overwhelmingly suggests violation of Benford's Law.",27.46%,56.93%,78.71%,40.95%,77.57%
260,328.jpg," The table shows the constructs, items, factor loadings, and reliability and validity of the measurement model. The constructs are digitalization capabilities, operational adjustment agility, market capitalizing agility, and apparent chaos. The items are related to the company's ability to use digital technologies, adjust its operations, capitalize on market opportunities, and respond to the COVID-19 pandemic. The factor loadings show the strength of the relationship between each item and its underlying construct. The reliability and validity of the measurement model are assessed using Cronbach's alpha and the composite reliability (CR) AVE. The results show that the measurement model has good reliability and validity.","Next, we investigate whether clustering takes place in NFT sales prices. Since human traders use round numbers as a mental heuristic to save time, such a test is well-suited to identify potential automatic trades, which often manifest in clusters. For each period, we divide the data into two groups: one with prices that are exact multiples of 1000 base units and another with prices within a 500 unit radius from multiples of 1000 base units. The base for the analysis is 4?10 units. The results of the clustering test in Table 3 show that, for 33 out of the 42 time periods analyzed, ~40 % of the trades are clustered around rounded values. This suggests that the majority of trades might be computer-generated.",35.57%,91.04%,81.22%,38.30%,79.26%
261,329.jpg," The table shows the mean, standard deviation, and correlation matrix of the four constructs. The mean values of the four constructs are 5.535, 5.566, 5.677, and 4.962, respectively. The standard deviation values of the four constructs are 1.066, 0.859, 0.728, and 0.923, respectively. The correlation matrix shows that the four constructs are positively correlated with each other. The correlation coefficients between the four constructs are 0.823, 0.766, 0.801, and 0.789, respectively.","Our third investigation uses power laws to estimate the fat tails of the NFT prices. If detected, they could indicate the presence of herding behavior [65]. Prior works also attribute fat tails to inadequate information available to economic agents to value an asset [66]. Li et al. [67] have shown that for financial assets the tail exponents lie in the Pareto-Levy range, i.e.,where is the power law exponent. The results of the Power fitting law in Table 4 show that power exponents, on 21 out of 42 occasions, do not lie in the Pareto-Levy range, suggesting abnormal trading practices.",29.56%,83.61%,81.17%,29.43%,64.18%
262,33.jpg," The figure shows the process of LeanContext. First, the top-k sentences are selected based on the query. Then, the context is removed from the beginning and end of the selected sentences. Finally, the position-aware text reduction method is applied to reduce the redundancy of the remaining text.","The upshot of the three tests described above is this: the trading patterns suggest high probability of abnormal and computer-generated trading both symptoms of wash trading and price manipulation. However, since we apply multiple tests on the same dataset, concerns over type-1 error and p-hacking may surface. Hence, we perform a multiple hypothesis test using Fischer's method with the null hypothesis that NFT trades are consistent with universal patterns in traditional financial markets. The combined results in Table 5 show a rejection of the null hypothesis for all months alleviating the aforementioned concerns. Our results contradict traditional asset stylized facts. For instance, Corazza et al. [68] show that prices of S&P 500 stocks generally follow Benford's Law but not during extreme events; e.g. the September 11 attack in 2001 or the crash during the Global Financial Crisis. Within the digital asset sphere, our results are comparable to Cong et al.?s [58] report on cryptocurrencies traded on unregulated exchanges, but similarities do not extend to regulated exchanges. Overall, our results appear consistent with the prevalent reputation of NFTs.",45.66%,90.62%,81.95%,3.79%,83.60%
263,330.jpg," Table 4 shows the regression results. From Model 1, we can see that market-capitalizing agility has a significantly positive effect on firm performance (β = 0.263, p < 0.05). This result supports Hypothesis 1, indicating that firms with higher market-capitalizing agility tend to have better firm performance.

From Model 2, we can see that operational adjustment agility has a significantly positive effect on firm performance (β = 0.289, p < 0.05). This result supports Hypothesis 2, indicating that firms with higher operational adjustment agility tend to have better firm performance.

From Model 3, we can see that the interaction between market-capitalizing agility and operational adjustment agility has a significantly positive effect on firm performance (β = 0.263, p < 0.05). This result supports Hypothesis 3, indicating that the combination of market-capitalizing agility and operational adjustment agility has a synergistic effect on firm performance.

Additionally, the control variables show that firm age, firm size, state-owned, privately owned, and industry have significant effects on firm performance. These results are consistent with previous studies.","The second phase of this investigation studies both the first and second significant digits. As stated before, a comma or zero before the first natural number is ignored [61]. Benford's Law is used to attempt to detect anomalies in the datasets of the non-fungible tokens. Via this approach, an effort is made to inspect whether price manipulation is occurring in the non-fungible tokens markets. Most of the requirements that are stated above are met by the datasets in this research. However, for the volume data, the requirement of a large dataset is somewhat difficult. The benchmark for the required number of transactions for the price data was set at 1000. However, in the literature there is a discrepancy that a number of researchers use as the lower bound for the criteria of a large dataset. For example, Riccioni and Cerqueti [56] used 100 as the lower bound. Meanwhile, Vicic and To?ic [29] stated that the rule is at least 50?100 transactions, while they state that often thousands of observations are used. Some papers argue for 500 as the lower bound for a large dataset (Tables 6 and 7).",13.56%,88.67%,79.81%,37.24%,70.45%
264,331.jpg," **Table 4. Results of the hypotheses testing**

Paths	Effect	BootSE	BootLLCI	BootULCI	Hypotheses	Conclusion
DC → FP	0.052	0.078	-0.102	0.205	Hypothesis 1	Support
DC → MCA → FP	0.088	0.054	0.008	0.220	Hypothesis 2	Support
DC → OAA → FP	0.077	0.045	0.001	0.181	Hypothesis 3	Support
DC → MCA → OAA → FP	0.046	0.023	0.001	0.094	Hypothesis 4	Support

Note: DC, MCA, OAA, and FP are the abbreviations of digitalization capabilities, market realizing agility, operational adjustment agility, and firm performance, respectively.","The second phase of this investigation studies both the first and second significant digits. As stated before, a comma or zero before the first natural number is ignored [61]. Benford's Law is used to attempt to detect anomalies in the datasets of the non-fungible tokens. Via this approach, an effort is made to inspect whether price manipulation is occurring in the non-fungible tokens markets. Most of the requirements that are stated above are met by the datasets in this research. However, for the volume data, the requirement of a large dataset is somewhat difficult. The benchmark for the required number of transactions for the price data was set at 1000. However, in the literature there is a discrepancy that a number of researchers use as the lower bound for the criteria of a large dataset. For example, Riccioni and Cerqueti [56] used 100 as the lower bound. Meanwhile, Vicic and To?ic [29] stated that the rule is at least 50?100 transactions, while they state that often thousands of observations are used. Some papers argue for 500 as the lower bound for a large dataset (Tables 6 and 7).",11.03%,66.06%,77.43%,11.28%,62.10%
265,332.jpg," 1. Zephyr/Medtronic sends data to our database.
2. Our database sends data to the Zephyr/Medtronic cloud database.
3. The Zephyr/Medtronic cloud database sends data to the patient avatar in Wepital.
4. The patient avatar in Wepital sends data to the web.","These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11).",17.77%,67.79%,77.75%,0.00%,81.85%
266,333.jpg," The figure shows the hypothesized relationships between the variables in the study. The independent variables are patient predispositions, affordances, and control variables. The dependent variable is satisfaction with the physician. The hypothesized relationships are shown by the arrows between the variables.","These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11).",33.09%,80.54%,79.00%,0.00%,75.84%
267,334.jpg," Structural equation modeling (SEM) was used to test the hypothesized relationships among the study variables. The model was tested using AMOS 26.0. The hypothesized model had good fit indices: χ2 (df = 142) = 216.23, p < 0.001; CFI = 0.95; TLI = 0.94; SRMR = 0.05. All of the hypothesized paths were significant (all p-values < 0.05). The results of the SEM analysis are shown in Figure 1.

As shown in Figure 1, the exogenous latent variable, control, had a significant positive effect on flexibility about method of care (β = 0.23, p < 0.05) and a significant negative effect on lack of privacy concern (β = -0.29, p < 0.05). Flexibility about method of care had a significant positive effect on telepresence (β = 0.70, p < 0.001) and trust (β = 0.63, p < 0.001). Lack of privacy concern had a significant negative effect on trust (β = -0.41, p < 0.001). Telepresence had a significant positive effect on time convenience (β = 0.41, p < 0.001) and satisfaction with physician (β = 0.33, p < 0.05). Time convenience had a significant positive effect on satisfaction with physician (β = 0.43, p < 0.001). Trust had a significant positive effect on real avatar (β = 0.65, p < 0.001) and satisfaction with physician (β = 0.43, p < 0.001). Real avatar had a significant positive effect on satisfaction with physician (β = 0.11, p < 0.05).","These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11).",19.27%,63.90%,76.78%,16.36%,72.92%
268,335.jpg," The diagram shows the different types of hospitals and how they are connected. There are three main types of hospitals: MyHospitals, Wepitals, and Hospitals.

MyHospitals are home-based hospitals that use sensors to monitor patients' vital signs and other health data. This data is then sent to a doctor's avatar, who can use it to diagnose and treat the patient. Wepitals are web-based hospitals that allow patients to see their doctors and other healthcare providers through a videoconference. This type of hospital is often used for follow-up appointments or for patients who live in remote areas. Hospitals are physical hospitals that provide a variety of medical services, including surgery, emergency care, and rehabilitation.

All three types of hospitals are connected to each other through a network of Doc portals. This network allows doctors and other healthcare providers to share information about patients, which can help to improve the quality of care.","The results in the following tables are split into separate tables for the first and second digits and for the price and volume data. In total there are 8 tables with results. Conformity to Benford's Law is judged according to the Mean Absolute Deviation score while Pearson's chi-squared test can be unreliable for large datasets. In Table 12 the results are shown for the price data of the selection of 50 NFTs. For 16 out of the 50 NFTs the first digit distribution conforms to Benford's Law according to the Mean Absolute Deviation. Two of these conforming NFTs are also confirmed by a significant chi-squared statistic. It is noteworthy that, among others, the NFT collections Axie Infinity, CryptoKitties and Gods Unchained score an abnormally high chi-squared statistic.",31.20%,80.24%,80.28%,46.22%,80.81%
269,336.jpg," The diagram shows the proposed architecture for the U.S. Medical Care Internet. The architecture consists of three tiers: the state/city level, the regional level, and the federal level.

At the state/city level, there will be a number of Wepital Hubs, each of which will be responsible for connecting a number of hospitals within a state or city. The Wepital Hubs will be connected to each other via a secure network. At the regional level, there will be a number of regional Wepital Hubs, each of which will be responsible for connecting a number of state/city Wepital Hubs within a region. The regional Wepital Hubs will be connected to each other via a secure network. At the federal level, there will be a single national Wepital Hub, which will be responsible for connecting all of the regional Wepital Hubs. The national Wepital Hub will be connected to the regional Wepital Hubs via a secure network.

The architecture will be closely supervised at the state, regional, and federal levels to ensure security and compliance.","In Table 13 the results for the second digit distribution of the price data of the NFT selection are presented. Something outstanding here is that 22 out of 30 conforming NFTs score in close conformity. Eight out of the 30 NFTs that conform to Benford's Law also achieve a significant chi-squared statistic. Once again, the collections of Axie Infinity, CryptoKitties and Gods Unchained have high chi-squared statistics. For 0xuniverse and CyberKongz, these high values are observed as well.",26.06%,82.53%,79.83%,39.53%,78.26%
270,337.jpg," **Figure 1: System architecture.** The figure shows the system architecture of the proposed solution. The system consists of three main components: (1) a cloud-based patient database, (2) a mobile application for patients, and (3) a web application for doctors. The patient database stores all the relevant information about the patients, such as their medical history, current medications, and allergies. The mobile application allows patients to track their health data, such as their blood pressure and heart rate, and to communicate with their doctors. The web application allows doctors to view their patients' health data, and to prescribe medications and treatments. The system is designed to be secure and private, and to comply with all relevant regulations.","Table 14 reports the results of the first digit distribution of the volume data of the 50 NFTs. Eight NFTs score acceptable conformity, five score marginally acceptable conformity and the remaining 37 NFTs nonconformity. Eleven of 13 conforming NFTs also show significant chi-squared scores. However, out of the 37 nonconforming NFTs, six nonetheless have a significant chi-squared statistic, which would indicate conformity. Either way, the MAD score is deemed as the leading test statistic; therefore these NFTs remain classified as nonconforming.",23.85%,84.79%,81.30%,39.66%,76.32%
271,338.jpg," Table 1. Reliability and validity of the Telemedicine Acceptance Model Constructs

Cronbach's alpha (α) is a measure of internal consistency, which indicates the extent to which the items in a scale are related to each other. It ranges from 0 to 1, with higher values indicating greater internal consistency. An alpha value of 0.7 or higher is generally considered to be acceptable.

AVE (average variance extracted) is a measure of the amount of variance in the items in a scale that is explained by the construct. It ranges from 0 to 1, with higher values indicating that the construct explains more of the variance in the items. An AVE value of 0.5 or higher is generally considered to be acceptable.

Composite reliability (CR) is a measure of the reliability of a scale, taking into account the number of items in the scale. It ranges from 0 to 1, with higher values indicating greater reliability. A CR value of 0.7 or higher is generally considered to be acceptable.

As shown in Table 1, all of the constructs in the Telemedicine Acceptance Model have acceptable reliability and validity. The Cronbach's alpha values range from 0.924 to 0.974, the AVE values range from 0.752 to 0.951, and the CR values range from 0.902 to 0.976. This indicates that the constructs are internally consistent, reliable, and valid.","Table 16 moves on to the results of the yearly aggregated data, starting with the first digit distribution of the yearly aggregated price data. The years 2017 through 2019 scored acceptable conformity, 2020 scored close conformity and the first 117 days of 2021 scored nonconformity. Nonetheless, with a MAD score of 0.01567, it is still close to marginally acceptable conformity. The transactions that are included in the years that do conform to Benford's Law amount to 44.3 % of the total number of transactions (6062,744) that are examined in the results section. All years have a high chi-squared statistic. Regardless, the year 2021 has an extraordinarily high chi-squared statistic, namely over 140,000.",47.19%,87.31%,80.64%,44.69%,84.22%
272,339.jpg," The table shows the square roots of the average variance extracted (AVE) for each construct. The square root of AVE is a measure of the amount of variance in a construct that is explained by its indicators. A high value of AVE indicates that the construct is well-measured by its indicators.

All of the constructs in the table have a square root of AVE greater than 0.5, which is considered to be acceptable. This indicates that each construct is well-measured by its indicators.","Table 17 demonstrates the second digit distribution of the yearly aggregated price data of the complete Nadini dataset. All of the years 2017 through 2021 obtained close conformity to Benford's Law. Once more, all of the years show a high chi-squared statistic",35.19%,79.34%,82.99%,43.78%,80.26%
273,34.jpg," The architecture of ChatDiet, a conversational AI agent that provides personalized nutrition advice. The system is composed of three main components: a personal model, a population model, and an orchestrator. The personal model is responsible for learning the user's personal preferences and nutritional needs. The population model is responsible for learning the nutritional content of different foods. The orchestrator is responsible for generating personalized nutrition advice for the user.","A pattern can be observed in Tables 18 and 19. In total 16 NFTs conform to the first digit distribution according to Benford's Law. Fifteen out of these 16 NFTs also conform to the second digit distribution according to Benford's Law. Therefore, this might be a consistent pattern that can also be found in other transaction data. Economically this pattern seems to make sense as well. If price manipulation would take place, it would be logical that both digits or at least the first do(es) not conform to Benford's Law. It would not make sense if the first digit distribution does conform to Benford's Law while the second digit distribution does not conform to Benford's Law. However, this pattern is not as strong for the volume data. The findings indicate that merely seven NFTs out of the 13 that conform to Benford's first digit distribution also conform to Benford's second digit distribution. Additionally, it can be observed that the 50 NFTs that were selected conform more often to Benford's Law for the price data than for the volume data. There were four NFTs that conformed to Benford's Law in all four tables, namely Crypto Space Commanders, CryptoKitites, Decentraland and Gods Unchained. They conformed for both the price and the volume data to the first- and second-digit distributions of Benford's Law. No pattern is found regarding the different categories of the NFTs. The categories are dispersed relatively in the same manner in the results as in the NFT selection. It can be observed that there are more NFTs in Art, Collectible and Games that appear in the conforming results; however, these NFTs are also represented way more in the NFT selection. Therefore, no relative difference can be observed.",26.59%,86.69%,79.65%,2.19%,81.33%
274,340.jpg," The figure shows the hypothesized structural equation model. The model proposes that social role induction leads to social role taking, which in turn leads to role-consistent behavior. The model also proposes that community commitment has a positive effect on taking an administrator role. Finally, the model proposes that taking an administrator role has a positive effect on public space contribution and private space contribution.","Table 19 shows the first- and second-digit distribution of the yearly aggregated volume data of the entire market history as recorded by nonfungible.com. For both the first- and second-digit distribution, all years (2017 through 2022) score nonconformity according to the MAD statistic. However, for the first-digit distribution, the chi-squared statistic reports a significant value for the years 2017 and 2022. For the second-digit distribution, the same applies for all of the years. Again, this is probably due to the small N. A pattern can be observed in Tables 18 and 19. In total 16 NFTs conform to the first digit distribution according to Benford's Law. Fifteen out of these 16 NFTs also conform to the second digit distribution according to Benford's Law. Therefore, this might be a consistent pattern that can also be found in other transaction data. Economically this pattern seems to make sense as well. If price manipulation would take place, it would be logical that both digits or at least the first do(es) not conform to Benford's Law. It would not make sense if the first digit distribution does conform to Benford's Law while the second digit distribution does not conform to Benford's Law. However, this pattern is not as strong for the volume data. The findings indicate that merely seven NFTs out of the 13 that conform to Benford's first digit distribution also conform to Benford's second digit distribution. Additionally, it can be observed that the 50 NFTs that were selected conform more often to Benford's Law for the price data than for the volume data. There were four NFTs that conformed to Benford's Law in all four tables, namely Crypto Space Commanders, CryptoKitites, Decentraland and Gods Unchained. They conformed for both the price and the volume data to the first- and second-digit distributions of Benford's Law. No pattern is found regarding the different categories of the NFTs. The categories are dispersed relatively in the same manner in the results as in the NFT selection. It can be observed that there are more NFTs in Art, Collectible and Games that appear in the conforming results; however, these NFTs are also represented way more in the NFT selection. Therefore, no relative difference can be observed.",24.44%,82.53%,80.24%,0.42%,80.32%
275,341.jpg," The figure shows the hypothesized structural equation model (SEM) for the relationships between social role induction, social role taking, and role-consistent behaviors. The model was tested using data from a three-wave longitudinal study. The results showed that social role induction had a significant positive effect on social role taking, which in turn had a significant positive effect on role-consistent behaviors. The model also showed that the effects of social role induction and social role taking on role-consistent behaviors were mediated by community commitment. These findings suggest that social role induction and social role taking are important processes for promoting role-consistent behaviors.","HNGOs in Germany typically have a three-level operational structure based on the German federal structure, with a national office, intermediate offices, and regional units (see Figure 1). The intermediate offices sit between the regional and national levels and may correspond to a German state or administrative region. This organizational set-up reflects Kemp et al.?s [12] differentiation of a (1) strategic level (e.g., developing a vision, formulating long-term goals), (2) tactical level (e.g., networking, agenda building), and (3) operational level (e.g., implementing, experimenting, building).",10.39%,67.71%,80.82%,41.91%,75.78%
276,342.jpg," The table shows the distribution of respondents in terms of gender, age, education, and annual income. The majority of respondents were male (81.6%), aged between 20 and 29 years (34.7%), had a college or university degree (71.2%), and had an annual income of less than NT$300,000 (48.6%).","The results of this study highlight the emergence of a self-producing/constructing information governance system reflected in an ?organizationally closed but structurally open? manner through self-reference to its own organizing principles. We see that social media information governance for accruing social capital has been managed in a self-sustaining manner by the HNGOs. Figure 2 summarizes this idea, as well as the focus areas of social media use and accruing social capital at the strategic, tactical, and operational levels.",21.02%,44.56%,80.56%,25.10%,71.14%
277,343.jpg,Error: Invalid response from model,"We conducted nine semi-structured interviews with experts at the regional level, six at the intermediate level, and three at the national office level; these experts came from five different HNGOs. Most of our interviewees working at regional units were volunteers, as this is where most volunteers engage with HNGOs; our paid staff interviewees were mostly active in intermediate and national units (headquarters), where HNGO permanent operations are situated. Hence, our sample reflects these organizational structures. The interviews were conducted in German via Skype or by telephone in 2019 and 2020. All interviews were between 30 minutes and 2 1/2 hours in length, depending on how detailed responses were; interviews conducted later in the study tended to be longer as we refined our interview guide and protocols. Table 2 is an overview of the interview sample.",1.36%,51.10%,80.06%,0.00%,70.43%
278,344.jpg," Table 5. Correlations, Means, Standard Deviations, and Reliabilities of Study Variables

Note: NA = not applicable. All correlations have a p value < .05. Italic numbers of the diagonal are square roots of AVE values.","Contribution intentions were regressed on crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, and their interaction (see Table 2 for the stepwise regression results). As expected, the results revealed a significant interaction between crowdfunding platform and prosocial motivation (B = 2.24, SE = 0.72, p < 0.01). As prosocial motivation is a continuous measure, the analyses were repeated using a spotlight analysis at one standard deviation below and above the mean (see Fig. 1; [63]). The analysis revealed a significant negative simple effect of crowdfunding platform type for participants low in prosocial motivation (B = -2.31, SE = 0.74, p < 0.01), indicating they were willing to contribute less money in the reward (vs. donation) condition. Conversely, there was no effect of crowdfunding platform type for those with high prosocial motivation (B = 0.93, SE = 0.71, p = 0.20).",9.83%,89.12%,80.98%,2.31%,76.07%
279,345.jpg," The table shows the results of a comparison between three groups: TAR-PUB, TAR-PRV, and TAR-OL. The comparison is based on three measures: mean path length (MPL), mean path rank (MPR), and absolute difference (AD). The p-value is based on a one-tailed test.

The results show that there is a significant difference between TAR-PUB and TAR-PRV in terms of MPL (t-value = 8.5, p-value < 0.001) and MPR (t-value = 6.9, p-value < 0.001). This indicates that TAR-PUB has a higher mean path length and a higher mean path rank than TAR-PRV. This means that TAR-PUB is more efficient in terms of information dissemination.

There is also a significant difference between TAR-PUB and TAR-OL in terms of MPL (t-value = 7.5, p-value < 0.001) and MPR (t-value = 6.9, p-value < 0.001). This indicates that TAR-PUB has a higher mean path length and a higher mean path rank than TAR-OL. This means that TAR-PUB is more efficient in terms of information dissemination.

Overall, the results show that TAR-PUB is more efficient in terms of information dissemination than TAR-PRV and TAR-OL.","Given that this study sampled the general population on MTurk rather than a more homogeneous undergraduate sample as in Study 1, we controlled for participants? gender, age, education level, and income. Contribution intentions were regressed on prosocial nature of the project description (1 = high; 0 = low), crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, their three-way interaction, and all lower-order interactions, while controlling for gender, age, education level, and income level (see Table 3 for the stepwise regression results). The results showed significant effects of the interaction between prosocial nature of the project description and crowdfunding platform type (B = -3.64, SE = 1.45, p = 0.01), the interaction between prosocial nature of the project description and prosocial motivation (B = -0.39, SE = 0.19, p = 0.04), and the three-way interaction between prosocial nature of the project description, crowdfunding platform type, and prosocial motivation (B = 0.56, SE = 0.26, p = 0.03). As prosocial motivation is a continuous measure, the analyses were repeated using a spotlight analysis at one standard deviation below and above the mean to probe the three-way interaction [63]. ",20.30%,87.00%,81.79%,44.20%,81.33%
280,346.jpg," The figure shows the hypothesized relationships among the constructs. Data analytics at the marketing unit is hypothesized to have a positive effect on marketing innovation (H1), which in turn is hypothesized to have a positive effect on market performance (H2).  Data analytics at the marketing unit is also hypothesized to have a positive effect on IT-enabled data analytics sensing capability (H3a), which in turn is hypothesized to have a positive effect on marketing innovation (H3b) and market performance (H4). Finally, IT-enabled data analytics sensing capability is hypothesized to have a positive effect on market performance (H5).","Similar to Study 2, given that this study sampled a more general population on Prolific rather than a more homogeneous undergraduate sample, we controlled for participants? gender, age, education level, and income. We also controlled for nationality given that we collected data from both American and Chinese participants. Contribution intentions were regressed on prosocial nature of the project description (1 = high; 0 = low), crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, their three-way interaction, all lower-order interactions, and control variables (see Table 4 for the stepwise regression results). Results showed significant effects of the interaction between prosocial nature of the project description and prosocial motivation (B = -0.74, SE = 0.34, p = 0.03) and the interaction between crowdfunding platform type and prosocial motivation (B = -0.94, SE = 0.31, p < 0.01). There were also marginally significant effects of the interaction between prosocial nature of the project description and crowdfunding platform type (B = -5.09, SE = 2.62, p = 0.05) and importantly, the three-way interaction between prosocial nature of the project description, crowdfunding platform type, and prosocial motivation (B = 0.78, SE = 0.45, p = 0.08).",10.28%,88.08%,80.39%,15.01%,77.90%
281,347.jpg," The figure shows the structural equation model (SEM) of the relationships between data analytics, marketing innovation, and market performance. The model was tested using data from a survey of 200 marketing managers. The results show that data analytics has a significant positive effect on marketing innovation (β = 0.689, p < 0.001) and market performance (β = 0.425, p < 0.001). Marketing innovation also has a significant positive effect on market performance (β = 0.317, p < 0.001). The model explains 47.4% of the variance in marketing innovation and 34.1% of the variance in market performance.

The results of the mediation analysis show that data analytics has a significant indirect effect on market performance through marketing innovation (β = 0.219, p < 0.05). This means that data analytics leads to improved market performance by enabling marketing innovation. The results also show that marketing innovation has a significant indirect effect on market performance through its impact on utility value (β = 0.135, p < 0.05). This means that marketing innovation leads to improved market performance by creating value for customers.

The findings of this study suggest that data analytics is a valuable tool for marketing managers. Data analytics can be used to improve marketing innovation and, ultimately, market performance. Marketing managers should consider investing in data analytics capabilities to improve their company's performance.","To address these research questions, we leveraged on a longitudinal dataset that allowed us to observe the effects of the marketing choices (price and flexible policies as functional attributes) and economic returns (occupation rates and revenues per active nights, as in line with Airbnb literature; [17]) at a single Airbnb property level in the city of Rome (i.e., the largest touristic submarket in Italy). The first evidence that emerged from these data is that the sharp market contraction due to the pandemic shock affected demand much more than supply: the negative change in revenues has in fact been about five times larger than the exit rates (see Table 1), thus depicting a novel market condition on the Airbnb platform. This circumstance suggests that competition must have increased substantially and, consequently, forced (at least some) entrepreneurs to react to the shock with renewed activism.",34.64%,88.57%,81.34%,44.14%,74.84%
282,348.jpg," This is a table that shows the percentage of total employment in each industry in the United States. The industry with the highest percentage of employment is the accommodation and food services industry, which employs 10.8% of the total workforce. The industry with the lowest percentage of employment is the mining, quarrying, and oil and gas extraction industry, which employs 0.8%.","With the aim of describing the impact of the shock in our sample, Table 2 reports the descriptive statistics of the two dependant variables for the considered three years, that is, 2018, 2019, and 2020. In line with Ghebreyesus [18], the descriptive statistics are computed for the months of March to Decemeber of each year. Consistent with the magnitude of the shock, Table 2 reports that the mean values of occupation rate and revenues per active nights declined significantly in 2020, compared to 2018 and 2019, years that were instead rather stable.",50.56%,79.14%,81.63%,33.20%,78.35%
283,349.jpg," The VIF values in the table are all less than 10, which indicates that there is no multicollinearity issue among the independent variables.","Table 3 provides the number of properties (and the relative shares in brackets) that adopted a given cancellation policy (moderate, flexible, or strict) in 2018, 2019, and 2020, and also shows the year-over-year transitions. Table 3a shows data pertaining to the benchmark situation (i.e., 2018 to 2019), while Table 3b shows data for the Covid-19 shocked situation (i.e., 2019 to 2020).",21.83%,74.74%,81.69%,8.61%,74.62%
284,35.jpg," The taxonomy of reasoning is a way of classifying different types of reasoning. The taxonomy is divided into five main categories: casual reasoning, multi-hop reasoning, logical reasoning, non-textual semantic reasoning, and commonsense reasoning.

Casual reasoning is a type of reasoning that is based on the observation of cause and effect. For example, if you see a window that is broken, you might infer that someone threw a rock at it.

Multi-hop reasoning is a type of reasoning that requires you to make multiple inferences in order to reach a conclusion. For example, you might see a person walking into a store, and then you might see them come out of the store with a bag. You might infer that the person bought something in the store.

Logical reasoning is a type of reasoning that is based on the rules of logic. For example, you might know that all dogs are mammals, and you might also know that all mammals have fur. You could then infer that all dogs have fur.

Non-textual semantic reasoning is a type of reasoning that is based on the meaning of words and phrases. For example, you might know that the word ""dog"" refers to a type of animal, and you might also know that the word ""cat"" refers to a type of animal. You could then infer that dogs and cats are different types of animals.

Commonsense reasoning is a type of reasoning that is based on the knowledge that is common to most people. For example, you might know that it is dangerous to run across a busy street, and you might also know that it is important to be polite to others. You could then infer that it is dangerous to run across a busy street while being impolite to others.

The taxonomy of reasoning is a useful tool for understanding the different types of reasoning that people use. The taxonomy can also be used to help people improve their reasoning skills.","Table 4 provides a comparison of the pricing adjustements between 2018 and 2019 and between 2019 and 2020, and it also shows the main descriptive indicators for the absolute price variation (i.e., the absolute value of PV(i,m)) as well as the results of a mean comparison test (i.e., the T-test). Fig. 2, which is complementary to Table 4, plots the distribution of PVy(i,m) in 2019 and in 2020.",27.39%,55.09%,78.87%,38.60%,77.93%
285,350.jpg, The table shows the results of a study that untersucht the impact of IT-enabled data analytics sensing capability on marketing innovation and market performance. The study was conducted on a sample of 200 firms. The results show that IT-enabled data analytics sensing capability has a significant positive impact on marketing innovation and market performance. The study also found that the impact of IT-enabled data analytics sensing capability on market performance is mediated by marketing innovation. This suggests that IT-enabled data analytics sensing capability can improve market performance by fostering marketing innovation.,"Table 5 shows the estimates of the econometric model that was used to predict the impact of the pricing adjustments, the impact of the flexible cancellation policies, and the effect of joint adoptions on the occupation rate. It is worth noting that the IV diagnostic statistics (i.e., Kleibergen-Paap weak identification tests) fully confirmed the validity of the instrument for our empirical setting.16 Models M1 and M2 include control variables only, Model M3 includes control variables and PV2020(i,m), Model M4 includes control variables and ModFlexPolicy(i), Model M5 includes control variables, PV2020(I,m), and ModFlexPolicy(i), while Model M6 includes all the variables as well as the interaction between PV2020(i,m) and ModFlexPolicy(i). Table A4 in the Online Appendix provides the estimation results when a pooled OLS estimation was employed, without instrumenting PV2020(i,m), and rather similar results can be observed.",30.31%,76.42%,81.20%,30.04%,77.78%
286,351.jpg," The table shows the results of a structural equation model (SEM) analysis. The model has three dependent variables: direct effects on IT, direct effects on marketing, and direct effects on innovation. The independent variables are technology readiness, size, and market orientation. The control variables are industry type, B2B type, and year founded.

The results show that technology readiness has a significant positive effect on direct effects on IT (β = 0.689, p < 0.001), direct effects on marketing (β = 0.308, p < 0.001), and direct effects on innovation (β = 0.423, p < 0.001). Size has a significant positive effect on direct effects on IT (β = 0.108, p < 0.05) and direct effects on marketing (β = 0.093, p < 0.05), but no significant effect on direct effects on innovation (β = 0.037, p > 0.05). Market orientation has a significant positive effect on direct effects on IT (β = 0.460, p < 0.001), direct effects on marketing (β = 0.652, p < 0.001), and direct effects on innovation (β = 0.559, p < 0.001).

The control variables industry type, B2B type, and year founded do not have any significant effects on the dependent variables.

The model has a good fit to the data, with a χ² of 2.598 (df = 2, p = 0.274), a CFI of 0.989, a TLI of 0.984, and a RMSEA of 0.047.","Table 6 shows the estimates of the econometric model when RevPAN in 2020 was considered as the dependant variable and tested on the sample of strict cancellation policy adopters in 2019. Again in this case, it is worth noticing that the Kleibergen-Paap weak identification tests fully confirmed the validity of the instrument for our empirical setting.18 Models M1 to M6 are defined as in Section 5.2.1. As for the analyses on the OccR, Table A9 in the Online Appendix provides the estimation results obtained when employing a pooled OLS estimation without instrumenting PV2020(i,m), and they show rather similar results.",25.27%,78.12%,78.88%,38.55%,74.14%
287,352.jpg," The table shows the results of a regression analysis with IT-enabled data analytics sensing, marketing innovation, and market performance as dependent variables. The independent variables are size, year of foundation, industry (manufacturing vs. service), technology intensity (high-techs vs. low-techs), customer type (B2B vs. B2C), quality data, and IT-enabled data analytics sensing.

The results show that IT-enabled data analytics sensing has a significant positive effect on market performance. This effect is mediated by marketing innovation. IT-enabled data analytics sensing also has a significant positive effect on marketing innovation. These findings suggest that IT-enabled data analytics sensing is an important tool for businesses that want to improve their market performance.

The table also shows that the control variables have the expected effects. Size has a significant positive effect on market performance and marketing innovation. Year of foundation has a significant negative effect on market performance. Industry has a significant positive effect on market performance and marketing innovation. Technology intensity has a significant positive effect on market performance. Customer type has a significant positive effect on market performance. Quality data has a significant positive effect on market performance and marketing innovation.","Guided by the theoretical lenses of adaptive structuration and sensemaking theories, the findings from this study have presented a conceptualization of why and how social media use and application evolve (see Fig. 2). Fig. 2 summarizes this processual framework's building blocks and their relationships that emerged from the analysis of the interview data. Social media features as created by designers and perceived by potential users play a central role in this process, which unfolds on the individual and firm levels. Salient characteristics that are likely to be perceived differently generate the triggering conditions as a stage of pre-attention, which may in turn initiate later stages of meaning-making of social media. This meaning-making takes place not only on the individual cognitive level, but also at the group level as part of a mutual exchange. However, the sensemaking of social media and its related use responses mainly occur via the cognitive interpretations of leader-managers, or responsible executives who commonly play the role of ?influential individuals? [93] within SMEs.",18.82%,93.70%,80.33%,43.62%,76.81%
288,353.jpg," The image shows a framework for understanding the role of blockchain in the innovation process. The framework is divided into four phases: assimilation, implementation, adoption, and comprehension. In the assimilation phase, organizations are trying to understand what blockchain is and how it can be used. In the implementation phase, organizations are experimenting with blockchain and trying to figure out how to use it to solve specific problems. In the adoption phase, organizations are starting to use blockchain more widely and are seeing the benefits of using it. In the comprehension phase, organizations are fully理解ing the benefits of blockchain and are using it to transform their businesses.

The framework also identifies three different types of organizations that are involved in the blockchain innovation process: strategists, experimenters, and implementers. Strategists are organizations that are trying to understand the potential of blockchain and how it can be used to create new opportunities. Experimenters are organizations that are trying out different ways to use blockchain and are learning from their experiences. Implementers are organizations that are using blockchain to solve specific problems and are seeing the benefits of using it.

The framework can be used to help organizations understand where they are in the blockchain innovation process and what steps they need to take to move forward.","Sample companies were purposely selected to obtain information-rich data that were able to contribute to a deeper understanding of the perceptions, concerns, and behavior related to social media use [109]. The interviewees were all key informants at their respective companies, i.e., owner- or business-managers, or executives responsible for their firms? social media management (see Table 1). A snowball sampling procedure [122] was employed, initiated via the first author's acquaintances. The use of natural social ties for accessing the ?elite informants? [119,122] counterbalanced the often prevailing asymmetry in the power relationship between interviewer and interviewee. To represent the heterogeneity of the SME area, the interview participants varied along several dimensions, such as gender, education, social media experience levels, and age. The participants? ages spanned from 23 to 71 years, with a job tenure ranging from 1 year to 30 years and more. This long period of service for a company applied to the owner-managers and founders of a hotel (SME 9), a men's shoe fashion wholesale (SME 15), and a firm consulting entrepreneurial couples (SME 18). Valuable insights into the research topic also resulted from the respondents' various social media worlds and accounts thereof complementing each other. Particularly, young participants aged under 35 years, acting in car and fashion retail or online marketing (SME 7, 8, 14, and 16), seemed to feel superior in knowledge and experience with social media to members of other age groups within and beyond their companies. Apart from the 42-year-old, socially and politically highly engaged owner-manager of a book retail shop (SME 13), all firm leaders were male. Most of the female participants held positions of a business, marketing, or social media manager.",36.18%,84.92%,79.99%,37.43%,85.81%
289,354.jpg," The figure shows a 3D space with three axes: the x-axis represents the assessment of blockchain's business value, the y-axis represents the sentiments toward blockchain, and the z-axis represents the state of the organization's innovation process. The figure shows four different types of organizations based on their position in this 3D space:
- The quadrant with positive sentiments toward blockchain, high assessment of its business value, and advanced state of the innovation process is labeled as ""Strategist"". 
- The quadrant with positive sentiments toward blockchain, limited assessment of its business value, and advanced state of the innovation process is labeled as ""Implementer"". 
- The quadrant with negative sentiments toward blockchain, limited assessment of its business value, and limited state of the innovation process is labeled as ""Observer"". 
- The quadrant with negative sentiments toward blockchain, high assessment of its business value, and limited state of the innovation process is labeled as ""Experimenter"".","Although previous investigations [22] found that regular social media activity remains challenging for firms regardless of their size, this study showed differences in companies? behavioral consistencies. As innovation adoption literature in general, and especially in the SME domain, mainly restricts its focus to mere adoption processes, which fail to account for a comprehensive, sustainable use [152], a better understanding of social media usage in terms of (dis)continuity [45] is needed. Initially, executives are equally exposed to a broad bundle of structural, i.e., material and informational, properties of social media [9]. Nevertheless, these features open up different, context-related journeys on which SME managers may embark in conditions triggering perceptions and behavior in terms of this technology as a whole. From a structuration perspective [40], our findings suggest that these structural properties of social media possibly fail to cause triggers unleashing a sensemaking and engagement process. Here, the encounter with few, negatively connotated features may not produce triggers, such as perceiving obligation or novelty [100] for driving sensemaking and steady use behavior. This tendency effect is reinforced by environmental influences, such as necessary protection against competitors or lacking regulatory and market requirements (for a tendential relation between perception of social media features and steadiness of use behavior derived from interview data, see Table 3).",24.02%,87.46%,79.57%,31.33%,81.37%
290,355.jpg," The table shows the professional background and year of the informants. There are 24 informants in total. 6 of them have a background in business administration, 5 in finance, 5 in information technology, 2 in general management, 2 in economics, 2 in product management, 1 in human resources, and 1 in physical science. The interviews were conducted in 2017 and 2018. There were 11 interviews in 2017 and 13 interviews in 2018.","Table 2 reports the key descriptive statistics and correlation matrix for the variables applied in this study, with Bonferroni-adjusted significance levels below 0.01. Multicollinearity does not represent a problem for any of the variables as the Variance Inflation Factor (VIF) is largely below the suggested threshold of ten [110].",21.72%,72.23%,80.07%,40.74%,74.91%
291,356.jpg, 454,"Table 3 shows the distribution of the strategic role of IT in industry in our sample of 1769 firms (and 17,690 observations) operating in 382 four-digit industries from 2011 to 2020.",0.00%,24.33%,78.02%,0.00%,0.00%
292,357.jpg," The figure shows the structural equation model (SEM) for the research. The model consists of three main constructs: firm inputs, implementation, and market value. Firm inputs include R&D, Capex, and staff. Implementation includes blockchain-enabled SCF and traditional SCF. Market value includes market value and trading volume. The model also includes a number of control variables: volume, CP, size, earning, and volatility.

The SEM shows that there are significant relationships between the constructs. For example, there is a positive relationship between firm inputs and implementation, and a positive relationship between implementation and market value. This suggests that firms that invest in firm inputs are more likely to implement blockchain-enabled SCF, and that firms that implement blockchain-enabled SCF are more likely to have a higher market value.

The SEM also shows that there are significant relationships between the control variables and the constructs. For example, there is a positive relationship between volume and market value, and a negative relationship between size and market value. This suggests that firms with higher volume are more likely to have a higher market value, and that firms with larger size are less likely to have a higher market value.

The SEM provides a comprehensive view of the relationships between the constructs and the control variables. The results of the SEM suggest that firms that invest in firm inputs and implement blockchain-enabled SCF are more likely to have a higher market value. The results also suggest that the control variables have a significant impact on the market value of firms.","Models 1a ? 12a in Table 4 report the comparative analysis among subsamples (i.e. Automate, Informate, Physical-Transform, and Digital-Transform) used to assess the best alignment between business strategy and the strategic role of IT in industry on labour productivity growth (Models 1a, 4a, 7a, 10a) and its value components ? output growth (Models 2a, 5a, 8a, 11a) and input reduction (Models 3a, 6a, 9a, 12a) ? in Automate, Informate, Physical-Transform and Digital-Transform industries, respectively.",22.70%,81.27%,79.23%,37.29%,84.69%
293,358.jpg," Table 4 shows the correlation matrix and descriptive statistics of the variables used in the regression. The correlation matrix shows that R&D is positively correlated with capex and staffex, and negatively correlated with block dummy. Capex is positively correlated with staffex and CP. Staffex is positively correlated with volume. CP is positively correlated with size and earning. Size is positively correlated with earning. Earning is negatively correlated with volatility.

The descriptive statistics show that the mean of CAR is -0.169, the mean of block dummy is 0.523, the mean of R&D is 0.033, the mean of capex is 0.013, the mean of staffex is 0.034, the mean of volume is 14.268, the mean of CP is 2.588, the mean of size is 22.992, the mean of earning is -0.002 and the mean of volatility is 0.024.","As a robustness check, we also used a dynamic panel data estimation to overcome any possible endogeneity issues arising from reverse causality due to the prior performance of a firm. The results are shown in Table 6 and are consistent with the results of the comparative analysis of the subsamples (Table 4) and with the results on the moderating effect of the strategic role of IT in industry (Table 5).",36.51%,77.90%,80.98%,43.14%,79.61%
294,359.jpg," The table shows the mean and standard deviation of the annualized return, the log of the mean trading volume, the log of the mean market capitalization, and the log of the mean shares outstanding in the pre-event and post-event windows.","H1 Smartphone utilitarian gratifications will reduce consumer state anxiety during shopping journeys. H2 Smartphone hedonic gratifications will reduce consumer state anxiety during shopping journeys. H3 Smartphone social gratifications will reduce consumer state anxiety during shopping journeys. H4 Consumer state anxiety mediates the relationships between smartphone U&G and in-store purchase intention. Given the hypothesized relationships above, we propose a visualized conceptual framework as follows (Fig. 1).",9.28%,82.69%,80.62%,20.52%,85.44%
295,36.jpg," 1. Train model
2. Evaluate model
3. Run tests
4. Augment training dataset
5. Train model on augmented set
6. Evaluate model
7. Run tests","Reliability was tested using Cronbach's Alpha value (a) (see Table 4) with all variables exceeding the threshold of 0.7. Moreover, to avoid common method bias, we employed Harman's one-factor test [138], which reported about 25.2% of variances explained by extracting only one factor, meeting the thumb value within 50% of the variance among all variables. This implies that there was no bias when applying respondents? answers in the same questionnaire for both independent and dependent variables. A reflective measurement model was inspected by conducting confirmatory factor analysis (CFA) via AMOS 26. Given the adequate sample size of 349 responses, the model fitness indices were as follows: ?2 = 656.355, degree of freedom (df) = 409, CMIN/DF= 1.605 (<3), p < 0.001, GFI= 0.903 (>0.9), TLI= 0.944 (>0.9), CFI= 0.950 (>0.9), RMSEA= 0.042 (<0.07). Hence, all the GOF indices met acceptable requirements, indicating that the measurement model achieved a good fit. As shown in Table 4, each item (Con1, Con2) and the first-order indicators (CON, OSO) significantly relate to the second-order latent constructs, the so-called utilitarian gratifications. Similarly, satisfying results were achieved with respect to hedonic and social gratifications and consumer state anxiety. In addition, the dependent variable (in-store purchase intention) was measured through a single item by probing the extent to which consumers were willing to purchase products after using their smartphones in-store. Single item has been accepted in existing studies as respondents can easily interpret the question [139,140]. This variable is further examined in the SEM and mediation analysis.",4.33%,64.14%,78.87%,0.01%,88.18%
296,360.jpg,"  Fama-French 4-factor model results. This table reports Fama-French 4-factor model estimation results. Fama-French 4-factor model includes market risk premium (MKT), size risk premium (SMB), value risk premium (HML) and momentum risk premium (MOM). All the Fama-French 4-factor model are estimated with and without control of EGARCH error terms. Besides, the Fama-French 4-factor model is also estimated with control of 4-factor model. The estimation results show that the Fama-French 4-factor model can explain the cross-section of stock returns.","Apart from achieving scale reliability, examining construct validity is suggested to embrace both convergent and discriminant validity tests. Convergent validity is assessed according to three aspects. First, all factor loadings should be statistically significant, with a standardized parameter of 0.5 or higher [141]. According to the output in Table 4, all indicators (CON and OSO) are significantly related to the latent constructs, falling between 0.534 (product information seeking via branded mobile apps) and 0.927 (consumer engagement via online brand communities). Second, the average variance extracted (AVE) is considered as the mean variance extracted for the items loading on a construct and is a conclusive index of convergence [142], with a suggested adequate convergence of over 0.5. The third attribute evaluates composite reliability (CR), an acceptable value of good reliability, suggested as being higher than 0.7. Table 5 demonstrates the convergent and discriminant validity performance.",16.68%,82.06%,80.58%,19.40%,78.51%
297,361.jpg,  Fama-French 3-factor and Carhart 4-factor models do not have Fama-MacBeth regressions with EGARCH model counterparts.,"When performing the structural model, smartphones? utilitarian, hedonic, and social gratifications were independent variables, the mediator being named as consumer state anxiety, and purchase intention was the dependent variable. In a similar vein, the model's fitness indices were exhibited first: ?2 = 860.589, degree of freedom (df) = 532, CMIN/DF= 1.618 (<3), p < 0.001, GFI= 0.910 (>0.9), TLI= 0.928 (>0.9), CFI= 0.936 (>0.9), RMSEA= 0.042 (<0.07). These indicators support a valid and reliable structural model leading to hypotheses testing (Table 6).",7.04%,58.24%,79.21%,0.48%,78.68%
298,362.jpg," This table reports the results of the four-factor model estimation for the Fama-French three-factor model with the fourth factor being momentum.  The sample period is from July 1963 to December 2015. The negative and positive values of the coefficients indicate negative and positive relations, respectively. The t-statistics are in parentheses. ***, **, and * indicate significance at the 1%, 5%, and 10% levels, respectively.","Mediation analysis was performed to test the fourth hypothesis via Hayes? PROCESS Macro [144]. The advantages of conducting mediation analysis beyond SEM are threefold in this study. First, the PROCESS can test moderator and mediator effects in one model and suggest conditional outcomes. Second, SEM inspects the entire model while PROCESS can perform each equation separately [145]. Third, PROCESS incorporates bootstrapping methods that further recommend reliable results by evaluating extra information. Table 7 presents the mediation analysis results, including total, indirect, and direct effects of the models.",35.43%,75.44%,81.96%,36.36%,81.68%
299,363.jpg," The table reports the results of the event study. The event window is the time period around the announcement date of the block trade. The trading volume is the average daily trading volume of the stock in the event window. The block firms are the firms that executed the block trade. The non-block firms are the firms that did not execute the block trade.

The results show that the trading volume of the block firms is significantly higher than the trading volume of the non-block firms in the event window. This suggests that the block trade has a significant impact on the trading volume of the stock.

The results also show that the trading volume of the block firms is significantly higher than the trading volume of the non-block firms in the days leading up to the announcement date of the block trade. This suggests that the block trade is anticipated by the market.

The results also show that the trading volume of the block firms is significantly higher than the trading volume of the non-block firms in the days following the announcement date of the block trade. This suggests that the block trade has a significant impact on the trading volume of the stock in the short term.","Fig. 1 provides an overview of the research methodology as a process consisting of three stages; define and design, prepare, collect and analyze, analyze and conclude, which was used in this study. It was abductive in nature going back and forth between the empirical world, the cases, the framework, and theory by matching, redirecting, and directing between those cases [54]. The definition and design phase started simultaneously with the literature overview and empirical frame of reference by looking out for interesting cases and matching them with theory, in this case, the TOE framework. Cases were identified and selected by how far they have come with their cloud sourcing. The data collection was designed to be qualitative and consisted of interviews, observations, and text analyses. Then the next phase of the research method commenced with preparation, data collection, and analysis of data. This phase took about a year and a half and was iterative, and abductive with redirections and matching to literature conducted in the first phase of the research methodology, framework, and theory development. The analysis and initial findings were documented in case reports before the last phase of the research methodology. In the last phase analyze and conclude, we once again reviewed thoroughly the literature, the empirical world, the theory, the cases, and the framework in order to draw cross-case conclusions. This directed us to modify the theory and suggest a matching framework (TOMPE) that complements the TOE with a newly identified factor ? namely the management process barrier (MP). We have further developed the barriers to continuance use of cloud computing.",42.07%,82.03%,79.51%,37.57%,85.84%
300,364.jpg, This table reports the results of the DID regressions. The DID estimator is constructed using the difference-in-differences between the treatment group (nonbank firms) and the control group (bank firms) in the pre- and post- Dodd-Frank periods. The dependent variable is the natural logarithm of the market value of assets. The results show that the Dodd-Frank Act had a significant negative impact on the market value of assets of nonbank firms.,"The research data amounted to 83.5 h of audio recordings. These were transcribed verbatim due to the specifics of the oil and gas jargon and to avoid obstructing preconceptions [87]. To ?recontextualize? the extensive research data, transcripts were further analyzed using the qualitative data analysis software NVivo 20. Applying the Gioia methodology as an analytical device [88], we structured the research data along with multiple data-anchored first-order concepts (those meaningful to the interviewees) that were further structured into theory-anchored second-order themes (induced by the authors). Our data analysis further abductively unpacked three aggregate dimensions related to the research question?pressures promoting adoption of hyped technologies, segmented perception of hyped technologies, and trust-building mechanisms (Fig. 1). We offered labels of the aggregate dimensions either by encapsulating dimensions at a higher level of abstraction or by referring to established literature describing these themes [85]. To increase the rigor of this qualitative research, we seek to provide transparency to our data structuring process. ",35.81%,87.56%,81.10%,14.96%,83.93%
301,365.jpg," Dependent variable: CAR [-15, +15]

| Explanatory variables | Model 1 | Model 2 | Model 3 | Model 4 |
| --- | --- | --- | --- | --- |
| Intercept | 0.906\*\*\* (2.311) | 0.761\*\* (2.169) | 0.828\*\*\* (2.353) | 0.626 (1.605) |
| Block_Dummy | 0.140\*\* (1.987) |  |  |  |
| Block_Dummy\*R&D | 2.322\*\*\* (2.987) |  |  |  |
| Block_Dummy\*Capex |  | 7.215\*\*\* (3.070) |  |  |
| Block_Dummy\*Staffex |  |  | -0.133 (-0.407) |  |
| Volume | 0.057\*\*\* (3.956) | 0.056\*\*\* (3.732) | 0.060\*\*\* (4.002) | 0.057\*\*\* (3.441) |
| CP | 0.085\*\* (2.435) | 0.066\*\* (2.628) | 0.075\*\*\* (2.435) | 0.061\*\* (3.611) |
| Size | -0.081\*\*\* (-3.454) | -0.070\*\*\* (-3.294) | -0.059\*\*\* (-3.524) | -0.067\*\*\* (-2.822) |
| Earning | -0.589\*\*\* (-3.984) | -0.474\*\*\* (-4.742) | -0.436\*\*\* (-4.336) | -0.415\*\*\* (-4.152) |
| Volatility | -0.806\*\*\* (-12.618) | -0.851\*\*\* (-13.501) | -0.851\*\*\* (-13.534) | -0.784\*\*\* (-11.735) |
| F-statistic | 79.269 | 79.555 | 80.495 | 63.207 |
| Adjusted R\^2 | 0.906 | 0.916 | 0.917 | 0.897 |

Note: \*, \*\*, and \*\*\* indicate significance at 10%, 5%, and 1% levels, respectively. t-statistics are reported in ().","Although several studies have examined hyped technologies [[25], [26], [27],90], the role of the hyped status in the implementation of and trust in hyped technologies such as digital twins is far from consensual [39,40]. Fig. 2 synopsizes our main findings about the application of hyped technologies as a multilevel process. This model illustrates the critical role of hype and trust building in the diffusion of technologies. As argued below, this model shows the two main contributions of this study: (1) how the hyped status of technologies materializes in multilevel perception segmentation in the organization(s), and (2) how local interpreting actors maneuver this segmentation by building trust in hyped technologies.",4.63%,24.85%,74.27%,34.67%,54.82%
302,366.jpg," Table 5. Car (-15, 15) Regressions","Fig. 1 presents our proposed research model. This argument, as illustrated in the theoretical hypothesis 1 (H1), invites us to expect and theorize that: Note: CV: Control variables. The control variables are not included simultaneously in the same model. This graphical representation summarizes the control variables included in the base empirical analysis, the test of robustness, and the post-hoc empirical analyses. H1: There is a positive relationship between remote work firms' initiatives in t1 and remote work firms' initiatives in t2, which will ultimately provide a competitive advantage to leaders over agile companies. H2: There is a positive relationship between remote work firms' initiatives in t2 and remote work firms' initiatives in t3, which will ultimately provide a competitive advantage to leaders and agile companies over survival companies.",0.00%,41.64%,81.17%,0.00%,55.76%
303,367.jpg," The table shows the mean, standard deviation, and top of each of the seven Airbnb dimensions, as well as the correlation between each dimension. The top is the percentage of Airbnb listings that fall into the top 20% of that dimension. The correlation between each dimension is shown in the table below.

|  | (1) | (2) | (3) | (4) | (5) | (6) | (7) |
|---|---|---|---|---|---|---|---|
| (1) | 1 | 0.74 | 0.59 | 0.69 | 0.64 | 0.45 | 0.75 |
| (2) | 0.74 | 1 | 0.58 | 0.66 | 0.61 | 0.43 | 0.66 |
| (3) | 0.59 | 0.58 | 1 | 0.45 | 0.68 | 0.37 | 0.51 |
| (4) | 0.69 | 0.66 | 0.45 | 1 | 0.47 | 0.33 | 0.59 |
| (5) | 0.64 | 0.61 | 0.68 | 0.47 | 1 | 0.38 | 0.55 |
| (6) | 0.45 | 0.43 | 0.37 | 0.33 | 0.38 | 1 | 0.45 |
| (7) | 0.75 | 0.66 | 0.51 | 0.59 | 0.55 | 0.45 | 1 |","The constructs of interest were measured using archival data. We measured remote work firm's initiatives through the natural logarithm of the number of remote work firm's initiatives mentions in news published about these initiatives per firm in t1, t2, and t3, with information collected from MyNews database (https://www.mynews.es/). This database includes news from more than 700 online and print editions of the national, regional, and international press. We performed a structured content analysis following the well-established protocol used in leading prior studies (e.g., [43,44,47]). Based on the review of relevant academic literature, managerial reports, and news on IT-enabled remote work, a preliminary list of keywords on remote work firms? initiatives was created. One of the authors discussed this list with three IT executives and four HR (business) executives. According to their recommendations and by carefully using their feedback, 20 keywords were selected and used for the corporate news coding protocol (Table 4). We considered these tools to be crucial to deploy remote work initiatives.4 Two of the authors conducted the news coding protocol. For each period, the two coders carefully read the news where the keyword appeared to check that the news referred to any remote work firm's initiative. We collected and read 2778 news in t1, 7819 news in t2, and 6353 news in t3 where the keywords appeared (Table 4). Each news contained different keywords referring to different initiatives on remote work. In that case, we computed as one each of these initiatives on remote work. After being read and coded, 111, 1137, and 847 remote work firm's initiatives mentions were found in t1, t2, and t3, respectively.",23.06%,7.70%,76.96%,25.68%,72.98%
304,368.jpg," The table shows the results of a mediation analysis. The first column shows the model, the second column shows the R^2 value, the third column shows the path 1 robust F value, and the fourth column shows the path 2 robust F value.

The baseline model has an R^2 value of 0.173, and the path 1 robust F value is not significant. The premises only model has an R^2 value of 0.252, and the path 1 robust F value is significant (p < 0.001). The host only model has an R^2 value of 0.190, and the path 1 robust F value is significant (p < 0.001). The full model has an R^2 value of 0.255, and the path 1 robust F value is significant (p < 0.001).

The results show that the premises and host variables both have a significant effect on the mediator, and that the mediator has a significant effect on the outcome. This suggests that the premises and host variables are both important in explaining the relationship between the mediator and the outcome.","We empirically tested the proposed research model running a partial least squares path modeling (PLS-PM), a variance-based structural equation modeling (SEM) technique [46,50] that is suitable to test the proposed model for two reasons. First, PLS is a full-fledged estimator that enables the empirical test of conceptual models in both confirmatory and explanatory IS research (as in this study) by using an overall evaluation of the fit of the saturated and estimated models [46]. Second, all the constructs of the proposed research model were conceptualized and operationalized as composite constructs, and this estimator is suitable to test composite models [48,51]. Moreover, PLS has been used extensively in the IS research area (e.g., [[52], [53], [54], [55], [56], [57]]). We used the statistical software package Advanced Analysis for Composites (ADANCO) 2.1. Professional (e.g., [58,59]) because it provides consistent estimates, and it has been designed for confirmatory research, as our study [60]. We ran a bootstrapping of 4999 subsamples. The proposed model was specified as a composite model, and all the constructs were estimated using mode B [46]. Table 5 presents the results of the empirical analysis. ",45.95%,73.57%,79.18%,47.87%,86.77%
305,369.jpg," Table 4 shows the results of the spatial regression models predicting property value. We ran three models for each property type, with the first including only the baseline variables, the second including only the premises variables, and the third including all variables. We ran two versions of each model, one with a spatial error term and one with a spatial lag term. We report the robust F statistic for each model, which is robust to heteroskedasticity and spatial autocorrelation. We find that the premises variables are jointly significant for apartments (p &lt; 0.05) and houses (p &lt; 0.01) but not for condominiums (p &gt; 0.10). The full model, which includes both the baseline and premises variables, is jointly significant for all three property types (p &lt; 0.01). The spatial error model outperforms the spatial lag model for apartments and condominiums, while the spatial lag model outperforms the spatial error model for houses.","Table A1 (in the appendix) presents the correlation matrix. We find support for H1 (beta = 0.278, p < 0.01) and H2 (beta = 0.203, p < 0.01) which indicates the following. 1. Companies that designed and executed a leader remote work strategy have continued executing these initiatives, which has provided leaders with a competitive advantage in remote work over their competitors. 2. Leaders and agile companies which have previously worked deliberately (leaders) or emergently (agile) on these remote work initiatives can yield leaders and agile a competitive advantage over survival companies that work on remote work initiatives through improvisation. The R2 values for remote work firm's initiatives in t1, t2, and t3 were 0.587, 0.570, and 0.650, respectively. The adjusted R2 values for the remote work firm's initiatives in t1, t2, and t3 constructs were 0.579, 0.556, and 0.639, respectively. The effect size (f2) values for the supported hypotheses ranged from 0.096 to 0.168, which indicates the medium size of these effects [61]. In addition, we compared the empirical correlation matrix with the model-implied correlation matrix of the estimated model to estimate three discrepancies between these two matrixes [62]: standardized root-mean-squared residual (SRMR), unweighted least squares (ULS) discrepancy (dULS), and the geodesic discrepancy (dG) [46]. SRMR should be lower than 0.080, and all the HI95 values should be greater than the values of the three discrepancies [46]. This analysis suggests that neither model should be rejected based on an alpha level of 0.05 since all discrepancies are below the 95%-quantile of the bootstrap discrepancies, which indicates that with a 5% probability, our theoretical hypotheses and the research proposed model are correct. Related to the control variables, the effects of firm size were significant on remote work firm's initiatives in t1 (beta = 0.112??) and t3 (beta = 0.112*). The size of these effects was similar (0.030 and 0.034, respectively). The influence of the firm's RSE in remote work was significant and with a large effect size in the three periods. The beta coefficients ranged from 0.649??? (t2) to 0.748??? (t1). The effect sizes were large, ranging from 0.994 (t3) to 1.342 (t1). The results show that the largest influence of the firm's RSE in remote work on the deployment of pioneer remote work initiatives was in t1, which is consistent with the leader remote work strategy.",42.56%,93.55%,81.73%,12.15%,86.22%
306,37.jpg,Error: Invalid response from model,"This task is challenging as the acceptance or rejection of upgrade offers relies on complex interactions among the offer, the customer, the booking, and the destination (as shown in Fig. 1). Our premise was based on the constructs as shown in Fig. 1, taken from discussions with experts on revenue, customer experience, and e-commerce within the company, using semi-structured interviews [12]. These dimensions provide a theoretical and practical paradigm for PREM [13,14] concerning the customer and the product. Prior work has also shown the influence of these three constructs in the airline industry [15], with each construct having subattributes such as departure date and price of the booking. Prior work has also shown that whether the customer is, for example, a leisure or business traveler affects the offer acceptance [16] and also the destination [16,17]. Customer perceptions of the airline service have also been noted as important for continued bookings [17]. While booking is often the only primary preference known about the customer [18], it can also involve subattributes such as the quality of the airline website [19]. As discussed later, our premise of the constructs of customer, booking, and destination can be seen to affect the decision to accept or reject the upgrade offer.",0.75%,53.23%,79.44%,0.00%,74.43%
307,370.jpg," The table shows the number of millionaires in different countries. The country with the most millionaires is the United States, with 114,353 millionaires. The country with the least millionaires is Belize, with only 1,268. The index column shows the number of millionaires per 1,000 people. The country with the highest index is Switzerland, with 20.8 per 1,000 people. The country with the lowest index is India, with 0.04 per 1,000 people.","Our collaboration airline company has more than 100 flights to more than 100 destinations on all major continents. The company serves more than 25 million customers annually, and our data covers a significant proportion of these customers over multiple years. The company uses a hub-and-spoke model (where flight routes are organized as a series of routes connected by a single hub airport), has international flights, competes on service and revenue, faces governance issues common in the industry, has a frequent flyer program, and is a member of a major airline alliance. As such, it represents a typical major airline company that offers bookings to multiple destinations to an international customer base. Fig. 2 illustrates the company's rule-based process for upselling offers via email marketing channels. In Fig. 2, the customer books a ticket in economy class. The company determines if the customer is eligible to receive an upgrade offer, which is sent via an email message containing the booking details and the upgrade offer price. Upon receiving the offer, the customer either accepts or ignores the proposed offer.",27.93%,85.57%,80.60%,10.71%,75.03%
308,371.jpg," The table shows the distribution of Airbnb listings in New York City by property type. The most common property type is an entire apartment, which accounts for 43% of all listings. This is followed by private rooms in apartments, which account for 12% of listings. The third most common property type is entire houses, which account for 11% of listings. The fourth most common property type is private rooms in houses, which account for 6% of listings. The fifth most common property type is entire condominiums, which account for 5% of listings."," As we empirically show in the experiments, the multistage approach provides better results than a single-stage end-to-end approach using multiobjective optimization with a constraints-based approach. The overall PREM process and components are presented in Fig. 4. As an overview of Fig. 4, the three most critical problems are noise, sparsity, and imbalance. We tackle each of these problems through two components ? feature embeddings and cost-sensitive classification.",14.31%,70.41%,81.20%,39.13%,71.30%
309,372.jpg," The table shows the distribution of Airbnb listings in New York City by host type, response time, and whether the host is a superhost.

The most common host type is individual, which accounts for 93.72% of all listings. The least common host type is commercial, which accounts for only 0.56% of listings.

The most common response time is within an hour, which accounts for 67.47% of all listings. The least common response time is a few days, which accounts for only 1.83% of listings.

The majority of hosts (63.47%) are Superhosts.","The dataset used for PREM development consists of more than 64 million trip booking records of customers traveling between 2017 and 2019. The dataset contains information concerning the price of the upgrade offer, which customers were sent upgrade offers, and which customers did or did not accept the update offers. This data is valuable for analyzing customer upselling and price elasticity dynamics in a major and competitive industry, and findings have implications for other travel-related domains. More than 14 million customers received upgrade offers (with a reach rate of 22%), and over 194,000 customers accepted the upgrade (with a conversion rate of 1.43%), as shown in Table 1.",27.11%,87.02%,82.61%,39.20%,79.88%
310,373.jpg," | | Premises | Host | Both |
| :--- | :--- | :--- | :--- |
| Max. guests | ***3.200 |  | ***3.199 |
| Log reviews | ***-1.872 |  | ***-1.673 |
| Accuracy | ***3.307 |  | ***3.310 |
| Communication | ***-0.670 |  | ***-0.557 |
| Cleanliness | ***6.025 |  | ***6.064 |
| Location | ***10.941 |  | ***10.861 |
| Value | ***-2.761 |  | ***-2.734 |
| Check-in | ***8.521 |  | ***8.340 |
| No. amenities | 0.005 |  | 0.008 |
| No. bathrooms | ***5.719 |  | ***5.389 |
| No. bedrooms | ***13.276 |  | ***13.251 |
| Host type generic |  | ***-18.080 | ***-21.677 |
| Host type individual |  | ***-8.325 | ***-7.494 |
| Host type multi-person |  | ***-6.929 | ***-6.041 |
| Identity verification | ***0.367 |  | ***0.544 |
| Host log reviews | ***2.273 |  | ***1.976 |
| All respond | ***0.970 |  | ***1.877 |
| Response: a day | ***-2.005 |  | ***-0.988 |
| Response: a few hours |  | ***-4.903 | ***-4.902 |
| Response: an hour | ***-6.574 |  | ***-1.202 |
| Picture | ***1.355 |  | ***1.355 |
| R² | 0.252 | 0.190 | 0.255 |","We did evaluate other encoding approaches, including dimensionality reduction techniques such as PCA. As discussed below in Table 2 (Results of feature embedding analysis), our embedding algorithm outperforms other approaches. While we did not invent this embedding algorithm, we are the first, to our knowledge, to apply it to airline data to tackle noise and sparsity issues. This applied study draws from computer science to solve a problem with real-world impact.",0.39%,-4.27%,75.09%,33.64%,37.50%
311,374.jpg," The table shows the results of a regression analysis with the dependent variable of price per night. The independent variables are all of the variables that were used in the hedonic price model, and the models are estimated separately for apartments, houses, and condominiums. The results show that the coefficients on the independent variables are generally consistent across the three types of properties. 

For example, the coefficient on the variable ""max guests"" is positive and significant for all three types of properties. This means that, all else equal, a property that allows more guests will have a higher price per night. Similarly, the coefficient on the variable ""accuracy"" is negative and significant for all three types of properties. This means that, all else equal, a property with a lower accuracy rating will have a lower price per night.

There are also some differences in the coefficients across the three types of properties. For example, the coefficient on the variable ""cleaning fee"" is positive and significant for apartments and condominiums, but it is not significant for houses. This means that, all else equal, a property with a cleaning fee will have a higher price per night for apartments and condominiums, but not for houses.

Overall, the results of this regression analysis suggest that there are some general similarities in the factors that affect the price per night of different types of properties. However, there are also some differences in the coefficients across the three types of properties, which suggests that there are also some unique factors that affect the price per night of each type of property.","A key hyperparameter is embedding in size, which determines the vector outputted size by the autoencoder, representing the trip information as a fixed-length vector. We evaluate its impact in the next experiment. When using the autoencoder, one could set the size of the latent representation to an arbitrary number. Traditionally, this size is often set as a power of two. Table 3 shows the results of PREM for different embedding sizes. We can see that an embedding size of 256 represents a sweet spot. Reducing the size results in lesser accuracy and revenue capture, while increasing it could potentially result in overfitting the model.",39.50%,91.21%,82.04%,41.85%,80.46%
312,38.jpg," In this figure, we present the overall framework of our proposed approach. It consists of five main steps: data collection, data preprocessing, model training, hyper-parameter tuning, and model evaluation. 

In the data collection step, we collect the data from different sources. The data can be in different formats, such as structured data, semi-structured data, and unstructured data. 

In the data preprocessing step, we clean the data, remove the noise, and tokenize the data. 

In the model training step, we train the joint Bi-LSTM-GNN model. The model is trained to encode the query and the code, and to generate the attention weights. 

In the hyper-parameter tuning step, we tune the hyper-parameters of the model. The hyper-parameters include the learning rate, the batch size, and the number of hidden units. 

In the model evaluation step, we evaluate the performance of the model. The performance is evaluated using different metrics, such as precision, recall, F1-score, MAP, MRR, and NDCG.","From Table 4, we can see that a DL model using embeddings provides the best results. However, one could also use other traditional classifiers, such as logistic regression, support vector machine, or random forest, and pay only a minor penalty in the F1 score and almost none in revenue capture. Using the original data without the embeddings shows a steep drop in the F1 score for both DL-based and non-DL-based methods. As shown in Table 4, we evaluated alternate approaches for handling imbalanced data [69], such as the Synthetic Minority Oversampling Technique (SMOTE), oversampling, and the use of generative models such as Generative Adversarial Networks (GANs). We used random forests as the downstream classifier. From Table 4, we can see that traditional approaches such as oversampling are outperformed by the GAN, where we generate synthetic data for the rare classes so that the training data is balanced. Our embedding-based approach outperforms each of these approaches.",54.08%,94.35%,82.62%,49.15%,84.84%
313,39.jpg," The figure shows the taxonomy of biomedical tasks. The tasks are divided into four main categories: relation extraction, named entity recognition, entity linking, and text classification. Each category is further divided into subcategories. For example, the relation extraction category is divided into three subcategories: chemical-disease relation extraction, drug-target interaction extraction, and disease-disease relation extraction. The named entity recognition category is divided into two subcategories: chemical named entity recognition and disease named entity recognition. The entity linking category is divided into two subcategories: chemical entity linking and disease entity linking. The text classification category is divided into two subcategories: disease classification and chemical classification.","The personalized upgrade model uses a binary autoencoder to segment customer bookings into K segments so that similar customers have similar embeddings. Once the segmentation is obtained, PREM estimates the response rate for each upgrade offer bucket. We consider two other segmentation approaches that also result in K segments. The first is based on K-Means that cluster all bookings into K distinct clusters, with K = 7. Our other baseline is a decision tree that tries to partition bookings using the Gini criterion. As shown in Table 5, our approach gives the best results.",16.02%,83.52%,80.66%,39.81%,72.19%
314,40.jpg," This diagram shows the process of using a language model. The user defines a task and describes it to the model. The model sends a prompt to the LLM, which responds with an output. The user then compares the output to the expected result and iterates on the prompt if necessary.","Next, we evaluate the revenue maximizer component. Recalling that we use an integer programming approach to select the best customers and the corresponding offers, a natural alternative is to use a greedy baseline that works as follows. First, the greedy algorithm computes each customer's expected revenue for each offer. Then, the Revenue Maximizer picks the best among them. For example, if Customer A accepts an offer of $500 with a 0.5 probability and $1000 with a 0.2 probability, then the expected revenues are $250 and $200, respectively. So, the Revenue Maximizer identifies that the best offer for Customer A is $500, with expected revenue of $250. Suppose there is another customer, Customer B, whose expected revenue is $300, and the model needs to select one customer. Here, the greedy algorithm will pick Customer B. As shown in Table 6, the revenue maximization of PREM (ILP) outperforms the greedy algorithm baseline.",42.22%,75.96%,83.82%,8.72%,88.38%
315,41.jpg,Error: Invalid response from model,"We propose a novel multistage approach based on feature development, the offer acceptance model, a personalized upgrade model, and a revenue maximizer. It is worth investigating if one needs such a multistage model in the first place. Specifically, two questions are of interest: (a) What would have happened if PREM used a single-stage classifier? and (b) What would have happened if PREM skipped the offer cost classification component or the revenue maximizer? The results presented in Table 7 address these questions. As shown in Table 7, the proposed PREM approach containing separate and sequential stages provides the best results. If one squeezes all of these stages into a single stage (i.e., if one trains a single classifier that takes all the bookings of a flight as input and returns the list of users and upgrade offers as output), then that gives the worst result. Splitting tasks into the different stages of an ML system clearly improves the performance in this context. Instead of the classifier trying to handle multiple objectives, each classifier in the PREM approach is targeted and focused on a single task, resulting in superior overall performance. Table 7 also indicates that all three components contribute positively. If one skips the revenue maximizer, then the accuracy drops marginally, but there is a steeper drop in revenue capture. Similarly, if one skips the offer classification and runs the upgrade offer determination for every user on the flight, there is a steep drop in accuracy but a smaller drop in revenue capture.",2.31%,59.19%,79.49%,0.00%,81.29%
316,42.jpg," The diagram shows a relationship between two actors. The actors are connected by a link. The link has three characteristics: content, type, and quality. The content of the link is information. The type of the link is communication. The quality of the link is trust.",Fig. 1 depicts the methodological approach followed.,18.11%,67.97%,85.26%,34.81%,71.28%
317,43.jpg," **Phase One:**
Thematic analysis of Facebook posts to reveal the information and link content.

**Phase Two:**
Narrative analysis of interviews with informants to reveal the decision-making process and communication approach (link type).

**Phase Three:**
Thematic analysis of Facebook comments to reveal the effectiveness and impact of the communication approach and public trust (link quality).

**Theoretical and Practical Contributions:**
The research will contribute to theory and practice by providing new insights into the role of social media in crisis communication.","LMM requires data to be set up in the long format such that there were 17 rows per participant. We first examined the psychometric properties of the model by assessing the convergent and discriminant validity of conservatism and collectivism variables. As shown in Table 4, because of low factor loadings, CONS1, CONS7, and CONS8 were dropped from the conservative position scale, and COL5 was dropped from the collectivism scale. The items listed in Table 4 lead to distinct constructs that demonstrated excellent Cronbach's a (Hair Jr, Hult, Ringle, & Sarstedt, 2013). The conservatism scale had Cronbach's a = 0.851, and the collectivism scale had Cronbach's a = 0.849. Discriminant validity was further assessed by examining items? cross-loadings that were all smaller than their factor (of interest) loadings (Hair [113]). As such, the measurement model demonstrated sound psychometric properties. In addition, we gauged the degree of multicollinearity between items and constructs in our study by calculating variance inflation factors (VIFs). All VIF values were <3.3, indicating that multicollinearity was not a concern in this study. While the focus of this study is on individuals? conservative political beliefs and their espoused cultural beliefs, we also added the country variable to control for the participant's country of origin, which may play a part (given the data were collected in two different countries). However, the focus remains on individual-level beliefs ? both political and cultural.",35.67%,70.80%,79.63%,7.58%,82.46%
318,44.jpg," To investigate the official utilization of Facebook to combat misinformation during the COVID-19 pandemic (link content) and to reveal the factors that impact public trust (link quality).

Data Collection: A targeted approach was employed to select data for analysis. From the available pool of content, 29 posts and their attached comments were manually selected. Inclusion Criteria: Posts were chosen based on their focus on major thematic areas of the COVID-19 infodemic, as identified by WHO report (WHO, 2020).

Data Analysis: Open Coding: The content of the posts/comments was read multiple times to identify initial themes and categories. In this step data was broken down into discrete parts, closely examined, and compared for similarities and differences.
Development of Coding Frame: The posts were read individually by the authors, and a coding frame was developed after group discussion sessions.
Categorization: Posts were reread and recategorized according to the agreed structure.
Thematic Analysis: The categorized data was then constructed into a higher logical level and, after further discussion, were conceptualized into final themes. This approach identifies, analyzes, and reports patterns (themes) within the data.
Information Archetype Identification: Based on the themes and categories, the information archetype for each post and its comments was identified.

Validation: Inter-coder Reliability: To ensure the reliability of the coding process, both authors were involved in reading and coding the data. Discrepancies were discussed and resolved in group sessions.

Presentation of Findings: Narrative Description: The findings were presented in a descriptive narrative format, detailing the themes, categories, and patterns identified in the data. This narrative approach allowed for a more in-depth exploration and explanation of the findings, providing context and depth to the analysis.","As shown in Table 5, both hypotheses were supported. With increasing degree of conservatism (ß = 0.65; p < 0.0001) and collectivism (ß = 0.21; p < 0.0001), individuals? fake news believability increased significantly, thereby providing support for both H1 and H2 and answering positively both associated research questions. Participants? gender (ß = -0.02) was found not significant. Age (ß = -0.004; P < .01) was significant such that with increasing age, the fake news believability decreased. Internet usage (ß = 0.03; P < 0.05) was also significant such that with increasing daily Internet usage, individuals? fake news believability increased. Country variable was found significant (ß = 0.36; P < 0 .001) such that American participants (mean = 3.12) in general were more likely than Indian participants (mean = 2.77) to lend credence to the fake news scenarios presented.",21.22%,68.50%,78.24%,38.42%,79.24%
319,45.jpg," The diagram shows a process for dealing with abusive content on a website.
The process starts with automated filtering, which is used to identify and remove the majority of abusive content.
The content that is not automatically filtered is then manually reviewed by a human moderator.
If the content is found to be abusive, it is removed and the user who posted it is blocked.
The community can also help to moderate the content by reporting abusive content to the moderators.",Figure 1 summarizes the hypotheses and relations discussed in the following sections.,21.95%,70.46%,84.38%,35.80%,75.53%
320,46.jpg," The diagram shows the relationships between different actors and factors in the context of a crisis. The actors are government agencies and the public. The factors include the type of link (communication approach), the content of the link (information), the quality of the link (trust), and the feedback loop. The risks include posting contradictory information, aged information, insufficient information, nuance language, inconsistent messaging, past failures, and infodemic. The diagram shows that the government agencies and the public are connected through the link, which can be either a communication approach or information. The quality of the link is determined by the trust between the two actors. The feedback loop shows that the public's response to the crisis can affect the government agencies' communication approach and the information they provide. The risks are factors that can affect the quality of the link and the feedback loop.",Table 1 presents the descriptive statistics and Pearson correlation coefficients of all variables.,24.42%,76.64%,82.33%,34.72%,73.94%
321,47.jpg, The Victorian Department of Health has a lower number of posts but a higher number of comments than NSW Health. This suggests that the Victorian Department of Health's posts are more engaging and generate more discussion.,"Table 2 displays tests of the hypotheses. Models 1?3 and 4?6 use resource utilization (seat utilization) and product concentration (concentration) as the dependent variables, respectively. Models 1 and 4 are baseline models. They only include control variables. Models 2 and 5 include the independent variables for testing Hypotheses 1a and 1b. Models 3 and 6 include interaction terms between demand-side usage of O2O digital platforms and the extent of vertical and horizontal inter-firm relationships to test Hypotheses 2a, 2b, 3a, and 3b. All models tested are statistically significant. Wald's chi-square values are reported.",16.65%,65.62%,80.54%,9.12%,79.97%
322,48.jpg," | COVID-19 infodemic major thematic area | Sampler |
|---|---|
| The cause and origin of the virus and disease | NSW Health has been made aware of a social media post that is being widely circulated warning people not to consume certain foods or visit certain locations in Sydney. |
| Covid-19 symptoms and transmission patterns | Coronavirus does not last very long on surfaces, therefore, imported products or mail do not pose a risk of transmission. |
| Available treatments, prophylactics, and cures | PrEP is effective in preventing HIV when taken as prescribed. But it does not protect against COVID-19. Get the facts on PrEP, HIV drugs and COVID-19 [...] |
| Effectiveness and impact of interventions by health authorities or other institutions | Should I wear a facemask to protect myself from the novel Coronavirus? No. Unless you are a health professional. |","Table A1 in the Appendix presents the results of robustness checks when the total number of all movie theaters is used as the measure of chain size and the new measure of concentration. The results indicate that the online ratio remains a strong predictor for operational decisions made by theaters. The study also finds that vertical relationship and chain size remain significant moderators. Hence, the results are robust to new formulations of critical variables. ",25.30%,55.13%,80.30%,42.40%,81.51%
323,49.jpg," The table shows the role of interviewees and the number of participants. There are 18 participants in total. The roles of the participants are state public health COVID-19 communication director, state COVID-19 social media and design manager, multicultural health team leader, crisis management strategist, director of a local health district, public health assistant and community expert, members of the Aboriginal community working in the public health, nurse, general practitioner, and hospital scientist.","Table A2 in the Appendix shows the results. Models 7 and 9 contain control variables, while Models 8 and 10 include independent variables. In Model 8, the results show that both seat utilization (ß = 0.592, p < 0.01) and concentration (ß = 0.191, p < 0.01) have a positive and significant impact on revenues. In Model 10, the results show that both seat utilization (ß = 0.418, p < 0.01) and concentration (ß = 0.131, p < 0.01) have a positive and significant impact on the audience.",23.28%,44.54%,80.12%,35.59%,68.87%
324,50.jpg," The figure shows the structural model of the relationships between perceived content integration capability, perceived omnichannel interaction quality, and shoppers’ decision-making styles. The results indicate that perceived content integration capability has a positive effect on perceived omnichannel interaction quality (β = 0.447, p < 0.001) and on shoppers’ decision-making styles (β = 0.600, p < 0.001). Perceived omnichannel interaction quality has a negative effect on shoppers’ decision-making styles (β = -0.312, p < 0.05). The model explains 36.1% of the variance in shoppers’ decision-making styles.",Table A3 shows the profile of the respondents.,20.51%,71.34%,84.15%,36.47%,50.44%
325,51.jpg," Structural model of the relationships between perceived content integration capability, perceived omnichannel interaction quality, and shoppers’ decision-making styles. Note: *p < 0.05, **p < 0.01, ***p < 0.001.","Based on the above discussion, our theoretical framework is summarized in Fig. 1.",3.01%,60.08%,82.89%,35.71%,43.03%
326,52.jpg," The figure shows the structural model of the relationships between perceived content integration capability, perceived omnichannel interaction quality, and shoppers’ decision-making styles. The results show that perceived content integration capability has a significant positive effect on perceived omnichannel interaction quality (β = 0.25, p < 0.05) and perceived omnichannel interaction quality has a significant positive effect on shoppers’ decision-making styles (β = 0.79, p < 0.05). Additionally, perceived content integration capability has a significant positive effect on shoppers’ decision-making styles (β = 0.18, p < 0.05). The model explains 62% of the variance in perceived omnichannel interaction quality and 78% of the variance in shoppers’ decision-making styles.",Table 1 summarizes the profiles of responding companies and respondents.,13.79%,74.77%,82.99%,35.20%,55.02%
327,53.jpg," The table shows the percentage of people in each category. The categories are gender, age, and marital status.

The majority of the people in the survey were female (85.8%). The majority of the people in the survey were aged between 26 and 35 (36.7%). The majority of the people in the survey were single (35.8%).","All items in this paper were adapted from the tested scale and measured by a seven-point Likert scale, ranging from ?1? to ?7? (?1? = ?strongly disagree? and ?7 = ?strongly agree?)  as shown in Table 2. In particular  four items adapted from Bharadwaj et al. [6] were used to measure digitalization capabilities  and these items reflected the degree to which companies could access customer-related order-related production-related  and market-related data. """,19.77%,57.00%,80.15%,32.17%,76.91%
328,54.jpg," The table shows the results of a mediation analysis. The independent variable is SDMS, the dependent variable is OIQ, and the mediator is CIC.

The results show that SDMS has a significant positive effect on OIQ (β = 0.312, p = .014). SDMS also has a significant positive effect on CIC (β = 0.600, p = .001). Finally, CIC has a significant positive effect on OIQ (β = 0.447, p = .001).

The results of the mediation analysis show that the effect of SDMS on OIQ is mediated by CIC. This means that the effect of SDMS on OIQ is partially explained by the fact that SDMS increases CIC, which in turn increases OIQ.

The table also shows the 95% confidence intervals (CIs) for each of the paths in the mediation model. The CIs show the range of values that the path coefficients could plausibly fall within.",The results in Table 3 thereby confirmed the effectiveness of discriminant validity.,36.99%,72.72%,82.61%,35.60%,67.85%
329,55.jpg," The table shows the data for the variables sector, geographic location, methodology, universe of population, sample size (% response), sampling error, and data collection period.

The sector is the services sector.
The geographic location is Spain.
The methodology is stratified random sampling.
The universe of population is 3210 firms.
The sample size is 950 firms, which is 36.10% of the population.
The sampling error is 5%.
The data collection period is from June to October 2022.","The estimated results based on the logic of stepwise regression are presented in Table 4. It can be seen that first, digitalization capabilities ( ) positively affect market capitalizing agility. Second, both digitalization capabilities ( ) and market capitalizing agility ( ) positively relate to operational adjustment agility. Third, without any mediators, digitalization capabilities ( ) pose a positive relationship with firm performance. Fourth, when considering the combined effect of digitalization capabilities, market capitalizing agility, and operational adjustment agility on firm performance, only market capitalizing agility ( ) and operational adjustment agility ( ) show significant coefficients, whereas the coefficient of digitalization capabilities is nonsignificant ( ). The above results thereby indicate that the full mediating role of market capitalizing agility and operational adjustment agility in influencing the relationship between digitalization capabilities and firm performance.",20.52%,84.38%,80.09%,20.18%,77.65%
330,56.jpg," The table shows the mean, standard deviation, and p-value for each characteristic for early and late respondents. The p-value is used to test the null hypothesis that there is no difference between the means of the two groups. A p-value less than 0.05 is considered to be statistically significant.

For the characteristic ""Size"", the mean for early respondents is 1.18 with a standard deviation of 0.466. The mean for late respondents is 1.15 with a standard deviation of 0.432. The p-value for this characteristic is 0.590, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""Sector"", the mean for early respondents is 1.86 with a standard deviation of 0.910. The mean for late respondents is 1.91 with a standard deviation of 0.935. The p-value for this characteristic is 0.668, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""Annual Turnover"", the mean for early respondents is 1508.95 with a standard deviation of 3693.255. The mean for late respondents is 1150.52 with a standard deviation of 3654.94. The p-value for this characteristic is 0.368, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""Growth of Sales"", the mean for early respondents is 4.85 with a standard deviation of 1.70. The mean for late respondents is 4.94 with a standard deviation of 1.66. The p-value for this characteristic is 0.647, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""Market share"", the mean for early respondents is 4.71 with a standard deviation of 1.63. The mean for late respondents is 4.87 with a standard deviation of 1.54. The p-value for this characteristic is 0.348, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""ROI"", the mean for early respondents is 4.29 with a standard deviation of 1.69. The mean for late respondents is 4.31 with a standard deviation of 1.68. The p-value for this characteristic is 0.922, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""ROA"", the mean for early respondents is 4.21 with a standard deviation of 1.65. The mean for late respondents is 4.26 with a standard deviation of 1.63. The p-value for this characteristic is 0.754, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""ROS"", the mean for early respondents is 4.36 with a standard deviation of 1.60. The mean for late respondents is 4.49 with a standard deviation of 1.61. The p-value for this characteristic is 0.457, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

For the characteristic ""ROE"", the mean for early respondents is 4.89 with a standard deviation of 1.49. The mean for late respondents is 4.96 with a standard deviation of 1.48. The p-value for this characteristic is 0.635, which is greater than 0.05. This means that there is no statistically significant difference between the means of the two groups for this characteristic.

In summary, there are no statistically significant differences between early and late respondents for any of the characteristics.","Table 5 further presents the results based on the bootstrap method with 5000 samples and a 95% confidence interval (CI) [62]. In particular, when the 95% CI's upper and lower bounds do not include zero, such a path is statistically significant; in contrast, when the 95% CI's upper and lower bounds include zero, such a path is statistically nonsignificant. As Table 5 shows, the direct effect of digitalization capabilities on firm performance (direct effect = 0.052, SE = 0.078) is statistically nonsignificant; in contrast, the indirect effects of digitalization capabilities on firm performance through market capitalizing agility (indirect effect = 0.088, SE = 0.054), operational adjustment agility (indirect effect = 0.077, SE = 0.045), and the sequence of market capitalizing agility and operational adjustment agility (indirect effect = 0.046, SE = 0.023) all have significant coefficients. Hence, these results again support all of our hypotheses.",22.94%,77.28%,80.53%,36.13%,79.32%
331,57.jpg," First-order concepts, second-order themes, and aggregate dimensions of AI strategy","Fig. 1 shows the implementation of the design (reported in Fig. C.1, Appendix C). The data flow (1) from the sensors (attached to the patient's body) to an app on a mobile device, (2) which sends the vital signs to the Zephyr/Medtronic cloud database, (3) which in turn are downloaded to our database, and (4) which is then uploaded to the green armband of the patient's avatar in the wepital. This way, the avatar embodies the patient's vital signs in real time.",2.61%,54.58%,81.93%,0.03%,69.95%
332,58.jpg," The figure shows the knowledge-to-action framework. It consists of five stages: knowledge, persuasion, decision, implementation, and confirmation. The knowledge stage is when individuals become aware of a new idea or behavior. The persuasion stage is when individuals are convinced that the new idea or behavior is worthwhile. The decision stage is when individuals decide to adopt the new idea or behavior. The implementation stage is when individuals put the new idea or behavior into practice. The confirmation stage is when individuals receive feedback on the new idea or behavior and decide whether to continue or discontinue it.

The knowledge-to-action framework is a useful tool for understanding how individuals change their behavior. It can be used to design interventions that are more likely to be successful. For example, an intervention that is designed to increase knowledge about a new behavior is more likely to be successful if it is combined with an intervention that is designed to persuade individuals that the behavior is worthwhile.","We conceptualized a theory-based model to assess the salient factors that contribute to the successful use of the wepital design from the perspective of patients, as shown in Fig. 4. The measure of success is patient satisfaction with the physician in the wepital. We continue to rely on the theory of affordances as the kernel theory in hypothesizing how design affordances impact satisfaction?the meta-requirement of the design.",34.40%,93.10%,83.26%,41.92%,81.24%
333,59.jpg," | Designation | Type | No. of employees | Engagement in corporate sustainability |
|---|---|---|---|
| Company A | Industrial technology provider | +250 | Core activities related to the circular economy, anchored in the corporate culture |
| Company B | Industrial software provider | +400 | CO2 compensation, green power and electric vehicles |
| Company C | Glass manufacturer | +150 | Climate-friendly production and focus on recycling |
| Company D | Web design agency | +30 | Awareness of sustainability, but no action so far |","All but one fit indices for the estimated model were desirably above or below the corresponding threshold levels. The only exception was the standardized root mean square residual (SRMR), which was slightly above the threshold. This was due to the relatively small size of our sample. Asparauhov and Muthén [2] point out that for small samples, the SRMR above 0.08 commonly occurs and should not be a cause for concern.15 Fig. 5 reports the estimated model.",11.78%,17.41%,78.06%,38.46%,73.09%
334,60.jpg," This study employed a mixed-methods research design, combining qualitative and quantitative approaches to gain a comprehensive understanding of the challenges faced by employees in adopting and using green IS in the workplace.

The first stage involved a qualitative field study, where four SMEs from different industries and sizes were selected for in-depth exploration. A three-week period of green IS usage was observed, followed by focus group interviews with employees to gather rich insights into their experiences. Thematic analysis was conducted on the interview transcripts to identify common themes and patterns related to the challenges faced by employees.

To ensure the trustworthiness of the findings, a purposive sample of green IS experts from research and practice was selected for semi-structured expert interviews. These experts reflected on the severity and importance of the challenges identified by employees and provided recommendations for companies to address them. The interview data was analyzed using a hybrid qualitative content analysis approach, combining deductive and inductive coding techniques. The deductive approach involved identifying the most severe challenges based on the frequency of their occurrence in the data, while the inductive approach allowed for the emergence of new themes and insights.

The final stage involved a triangulation process, where the findings from the qualitative field study and the expert interviews were synthesized and compared to identify commonalities and discrepancies. This process enhanced the credibility and reliability of the research results.

The mixed-methods approach employed in this study allowed for a comprehensive exploration of the challenges faced by employees in adopting and using green IS in the workplace. The combination of qualitative and quantitative data provided a rich and nuanced understanding of the issues involved, while the triangulation process ensured the trustworthiness of the findings. The findings of this study have important implications for organizations seeking to implement green IS and for researchers interested in understanding the challenges faced by employees in this area.","As shown in Fig. 7, hospitals can extend their physical and staff capacity by adding patients? rooms at home (mypitals), virtual human supervisors in wepitals, and medical staff in docpitals. In times of extreme need, hospitals can immediately increase capacity by adding more rooms to their wepitals and staffing their wepitals with additional intelligent robots, virtual human providers, and medical staff in docpitals. Thus, hospitals will have the capacity and capability to better handle pandemics on short notice. More importantly, they will have the capability to extend their resources to remote parts of the country or even the globe.",35.04%,90.24%,80.31%,38.84%,73.85%
335,61.jpg," The figure shows the relationship between AI and human performance, expectancy, and satisfaction. The AI (match vs. mismatch) vs. human factor directly affects the expectancy of the performance, which in turn affects the satisfaction.","Fifth, in an advanced application, states can create an infrastructure that facilitates such movements. Each state or city (depending on the number of its hospitals and its population) can create a hub, called a wepital hub, on which its hospitals set up their wepitals. Each hub is supervised for security and compliance at the state level. The collection of wepital hubs creates the US Medical Care Internet, which should be regulated at the federal level for security and compliance (Fig. 8). State and federal governments can provide medical services to more people and handle medical crises with more options and resources. They can disseminate health information and educational materials in wepitals when people have the time and motivation to receive such information and education.",25.72%,81.01%,82.70%,3.75%,80.07%
336,62.jpg," The table shows the responses to a survey about Netflix's AI tool. The survey has three items, each of which is rated on a scale of 1 to 9, where 1 means ""Strongly Disagree"" and 9 means ""Strongly Agree"".

The first item is ""I am satisfied with Netflix's AI tool"". The second item is ""I am happy with Netflix's AI tool"". The third item is ""I think Netflix's AI tool did a good job"".

The table shows that the respondents are generally satisfied with Netflix's AI tool. The mean score for the first item is 7.5, the mean score for the second item is 7.8, and the mean score for the third item is 7.9. This suggests that the respondents are satisfied with the tool and think it did a good job.",The Meta Design of Real Avatar in Wepital,16.23%,33.76%,78.20%,34.34%,60.85%
337,63.jpg," The table shows a 9-point Likert scale, with 1 being ""Strongly Disagree"" and 9 being ""Strongly Agree"". The items are all related to the usefulness of the Netflix AI tool in deciding what to watch.

The first item, ""I find this Netflix AI tool useful in deciding what to watch"", has a mean of 5.67 and a standard deviation of 2.13. This suggests that, on average, people find the tool to be somewhat useful in deciding what to watch. However, there is a fair amount of variation in this opinion, with some people finding the tool to be very useful and others finding it to be not very useful.

The second item, ""Using this Netflix AI tool enables me to decide what to watch quickly"", has a mean of 6.12 and a standard deviation of 1.87. This suggests that, on average, people find the tool to be somewhat helpful in enabling them to decide what to watch quickly. However, there is again a fair amount of variation in this opinion, with some people finding the tool to be very helpful and others finding it to be not very helpful.

The third item, ""Using this Netflix AI tool increases my efficiency in deciding what to watch"", has a mean of 6.33 and a standard deviation of 1.78. This suggests that, on average, people find the tool to be somewhat helpful in increasing their efficiency in deciding what to watch. However, there is once again a fair amount of variation in this opinion, with some people finding the tool to be very helpful and others finding it to be not very helpful.

Overall, the results of this survey suggest that people find the Netflix AI tool to be somewhat useful in deciding what to watch. However, there is a fair amount of variation in this opinion, with some people finding the tool to be very useful and others finding it to be not very useful.","We checked the reliability of the constructs in three ways, as reported in Table I.1. We computed Cronbach alpha values, which were all above the 0.70 acceptable threshold [83]. Composite factor reliability (CFR) values were above the 0.70 acceptable threshold [104]. Average variance extracted (AVE) values were above the acceptable threshold 0.50 [104]. As an additional check on discriminant validity, we computed the square root of AVE for each construct and compared it with the construct's correlations with other constructs [37]. ",17.32%,62.80%,79.47%,36.84%,78.45%
338,64.jpg," | Pseudonym | Gender | Age |
|---|---|---|
| 1. Manoel | Male | 22 |
| 2. Maria | Female | 28 |
| 3. Marcus | Male | 22 |
| 4. Paulo | Male | 43 |
| 5. Anne | Female | 41 |
| 6. John | Male | 24 |
| 7. Mary | Female | 41 |
| 8. Marcio | Male | 23 |
| 9. George | Male | 30 |
| 10. André | Male | 40 |
| 11. Bruna | Female | 25 |
| 12. Eduardo | Male | 22 |
| 13. Eduarda | Female | 30 |
| 14. Gabriela | Female | 22 |
| 15. Rafael | Male | 35 |
| 16. Tamara | Female | 22 |
| 17. Diogo | Male | 44 |
| 18. Isabela | Female | 22 |
| 19. João Pedro | Male | 23 |
| 20. Mario | Male | 41 |
| 21. Lucas | Male | 22 |","For each construct, the square root of AVE was desirably greater than the correlation values (Table I.2). These checks provided support for the reliability and validity of the measured constructs.",0.00%,4.31%,77.20%,0.00%,44.60%
339,65.jpg," The figure shows the hypothesized relationships between artificial intelligence (AI) for business-to-business (B2B) marketing, industry dynamism, customer complexity, and shareholder reaction via stock return. The hypothesized relationships are as follows:
- H1: AI for B2B marketing has a positive impact on industry dynamism.
- H2: Industry dynamism has a positive impact on shareholder reaction via stock return.
- H3a: AI for B2B marketing has a positive impact on customer complexity.
- H3b: Customer complexity has a negative impact on shareholder reaction via stock return.","In our context, users need to actively apply and await approval to be administrators, i.e., administrator-users voluntarily take the administrative role. Hence, taking this role could reflect the process of social role taking, and therefore encourage the display of role-consistent behaviors (public space contribution, private space contribution, and OL). Fig. 1 illustrates our contextualized theoretical model.",5.43%,83.33%,81.34%,34.92%,78.20%
340,66.jpg," Panel A: Characteristics of Sample Firms

The table shows the distribution of sample firms across different industries. The industry with the most sample firms is services, which accounts for 56.18% of the total sample. This is followed by manufacturing, which accounts for 21.34% of the total sample. The other industries have a smaller representation in the sample.

Panel B: Distribution of Samples Across Industries

The table shows the distribution of sample firms across different years. The year with the most sample firms is 2019, which accounts for 16.85% of the total sample. This is followed by 2017 and 2018, which account for 14.61% and 10.11% of the total sample, respectively. The other years have a smaller representation in the sample.","We employed LISREL v.8.80 software to implement structural equation modeling (SEM), which is a recommended method [144]. Our structural model acceptably fit the data, i.e., CFI=.95, IFI=.95, NFI=.94, RMSEA=.094, and SRMR=.039. The reasons that justify the RMSEA value were previously stated for the CFA results, and therefore are not repeated here. Notably, we permitted the LISREL software to freely estimate correlations between a few behavioral indicators. For example, the in-community level is partly determined by the number of posted articles in public areas so they should be related. This action is reasonable as they are reflective items of the same concept. The analytical results are shown in Fig. 2.",35.93%,75.50%,80.22%,46.67%,80.27%
341,67.jpg," Table reports the results of the propensity score matching. Model 1 presents the results of the probit regression used to estimate the propensity score. The dependent variable equals one if the firm is acquired and zero otherwise. Independent variables include marketing efficiency, firm debt, firm profitability, firm size, firm liquidity, financial slack, R&D intensity, and industry and year fixed effects. Model 2 presents the results of the OLS regression of the treatment effect on the propensity score matched sample. The dependent variable equals one if the firm is acquired and zero otherwise. Independent variables include an intercept, marketing efficiency, firm debt, firm profitability, firm size, firm liquidity, financial slack, R&D intensity, and industry and year fixed effects. The number of observations is 1334 in Model 1 and 87 in Model 2.",The participants had an average duration in the community of 8.70 years (standard deviation=5.33 years). Table 1 lists the demographic information of our participants.,25.59%,76.32%,81.01%,37.91%,74.11%
342,68.jpg," Table 1 shows the correlation matrix and descriptive statistics of the variables used in this study. All variables are winsorized at 1% and 99% levels to mitigate the effects of outliers. As can be seen, there is no high correlation between the independent variables, which ensures the absence of multicollinearity.","Consistent with the literature [1,117,[132], [133], [134]], we conducted an exploratory factor analysis (EFA) to test data validity. Table 3 lists our EFA results that do not show marked cross-loadings, that is, the EFA results offer preliminary support for our data validity. All factor loadings are larger than .82.",11.11%,71.35%,82.86%,41.33%,74.22%
343,69.jpg," Table 4 reports the results of the hypotheses tests for the hypothesized relationships. Model 1 shows the baseline model with control variables. Models 2–4 add the three AI-enabled B2B marketing capabilities, respectively. In Model 5, all three AI-enabled B2B marketing capabilities are included simultaneously. In addition, we control for the interaction effects between the three AI-enabled B2B marketing capabilities in Model 6. The results show that all three AI-enabled B2B marketing capabilities have significant positive effects on firm performance. Specifically, AI-enabled marketing analytics has a significant positive effect on firm performance (β = 0.226, p < 0.05), AI-enabled marketing automation has a significant positive effect on firm performance (β = 0.203, p < 0.05), and AI-enabled customer experience management has a significant positive effect on firm performance (β = 0.181, p < 0.05). These results support Hypotheses 1, 2, and 3, respectively.","Table 4 reports the correlations among our study concepts. These ranged from .09 and .59. Moreover, the square roots of the AVE values (on the diagonal of Table 4) exceeded the associated correlations, indicating sufficient discriminant validity [118,141].",19.57%,84.21%,82.29%,38.50%,64.68%
344,70.jpg," The table shows the objectives of the study, which are:

* To explore the specific reasons and emotions behind shareholders' positive reactions to AI implementation in B2B marketing.
* To investigate whether there are common themes or differences among participants' responses.
* To probe into shareholders' perceptions of industry dynamics and how these dynamics influence their reactions to AI implementation.
* To identify key industry concerns and their impact on shareholder sentiment.
* To investigate why shareholders of firms with more complex customer bases might react less positively to AI implementation.
* To explore their concerns, expectations, and factors that contribute to this reaction.","All the hypotheses were supported. Community commitment is positively related to taking an administrator role (H1), which is further positively related to public space contribution, private space contribution, and OL (H2, H3, and H4). Taking an administrator role has a stronger impact on public space contributions than on private space contributions (H5) and on OL (H6). Table 5 lists the testing results of H5 and H6. Specifically, we followed Ray et al. [46] and therefore used the t tests to compare the influences of taking an administrator role on three positive community behaviors. More details can be found in Ray et al. [46]. Both H5 and H6 were supported.",23.26%,72.55%,81.72%,42.77%,77.35%
345,71.jpg," The figure shows the research model that investigates the relationships among perceived privacy concern, perceived anthropomorphism, perceived empathy, perceived trustworthiness, and consumer forgiveness in the context of negative word-of-mouth (nWOM). The model suggests that perceived privacy concern, perceived anthropomorphism, and perceived empathy have a positive impact on perceived trustworthiness. Perceived trustworthiness, in turn, has a positive impact on consumer forgiveness. Additionally, the model suggests that perceived benevolence and perceived integrity mediate the relationship between perceived trustworthiness and consumer forgiveness.","From an information value chain perspective, the specific impacts of quality data and IT-enabled sensing capability are expected to differ depending on their position in the chain, losing strength as they move to more distant elements (see Fig. 1). Proposing each data analytic resource?assets (i.e., quality data) and capabilities (i.e., IT-enabled sensing)?as having different effects (i.e., direct, partial, or fully mediated) on outcomes at the marketing unit and at the firm serves to add precision to our overall understanding of the value creation potential of data analytics.",15.15%,84.94%,81.81%,39.27%,70.85%
346,72.jpg," Study 1 and Study 2 had similar participant demographics. In both studies, the majority of participants were between the ages of 25 and 35, had an annual income of less than £25,000, and had a high school diploma or less.","To test the significance and strength of the proposed relations, we used a one-tailed 5000 subsample BCA bootstrap [137]. Fig. 2 and Table 5 show the results obtained. As can be observed, quality data shows a very strong and positive relationship with IT-enabled data analytics sensing (ß = 0.689). Thus, hypothesis H1 is supported. Moreover, both quality data (ß = 0.340) and IT-enabled data analytics sensing (ß = 0.317) are positively and significantly related to marketing innovation. As the indirect effect of quality data on marketing innovation via IT-enabled data analytics sensing is positive and significant (ß1 × ß2 = 0.219), partial mediation applies. Hence, hypotheses H2a and H2b are supported.",17.74%,68.41%,81.37%,8.47%,78.59%
347,73.jpg," Table 1 shows the correlation matrix of the study variables. As can be seen, consumer forgiveness is positively correlated with perceived empathy (r = 0.18, p < 0.05), perceived ability (r = 0.26, p < 0.01), perceived benevolence (r = 0.25, p < 0.01), and perceived integrity (r = 0.31, p < 0.01).","Thus, taking the initial sample size as a reference (i.e., 342 companies), the stratified sampling procedure ensured that different proportions of company types according to size (mid-sized vs. large-sized), industry (manufacturing vs. service), and technology intensity (high-techs vs. low-techs, as established by the OECD and Eurostat) were preserved as they exist in the population, thereby improving the precision and representativeness of the resulting sample. The final sample included 346 companies? 4 over the threshold of 342?that answered the provided questionnaire. Table 1 provides more details about the composition of the sample.",17.31%,72.42%,79.91%,19.96%,69.71%
348,74.jpg," The table shows the results of two studies that examine the relationships between perceived privacy concerns, perceived ability, perceived benevolence, perceived integrity, and consumer forgiveness. Study 1 has 586 participants, and Study 2 has 508 participants.

The results of Study 1 show that perceived privacy concerns have a significant negative relationship with perceived ability (t = -4.72, p < 0.05), perceived benevolence (t = -1.40, p < 0.05), and perceived integrity (t = -2.83, p < 0.05). Perceived ability has a significant positive relationship with perceived benevolence (t = 3.95, p < 0.05) and perceived integrity (t = 4.28, p < 0.05). Perceived benevolence has a significant positive relationship with perceived integrity (t = 3.67, p < 0.05). Finally, perceived integrity has a significant positive relationship with consumer forgiveness (t = 4.18, p < 0.05).

The results of Study 2 show that perceived privacy concerns have a significant negative relationship with perceived ability (t = -4.25, p < 0.05), perceived benevolence (t = -1.24, p < 0.05), and perceived integrity (t = -3.91, p < 0.05). Perceived ability has a significant positive relationship with perceived benevolence (t = 3.74, p < 0.05) and perceived integrity (t = 3.90, p < 0.05). Perceived benevolence has a significant positive relationship with perceived integrity (t = 4.12, p < 0.05). Finally, perceived integrity has a significant positive relationship with consumer forgiveness (t = 3.98, p < 0.05).

Overall, the results of both studies provide support for the hypothesized relationships between perceived privacy concerns, perceived ability, perceived benevolence, perceived integrity, and consumer forgiveness.","As data was collected through a single method (i.e., survey), this presented the possibility of the occurrence of what is known as common-method bias [[115], [116]]. To determine the extent of the method variance in the dataset, we used the marker variable approach [117]. To that end, we included a two-item scale regarding competition intensity,2 based on Jaworski and Kohli [118]. Subsequent correlation analysis revealed that correlations between the marker variable and independent, mediating, and dependent variables were very low, the largest one being 0.191. Thus, it could be concluded that common method variance was not a likely problem in our dataset. Also, a full collinearity test specially conceived for PLS-SEM [119] was carried out. The above test includes both vertical (predictor?predictor) and lateral (predictor?criterion) collinearity analyses. According to Kock [119], if all the variance inflation factors (VIFs) resulting from a full collinearity test are equal to or lower than 3.3, the model can be considered free of common-method bias. The highest VIF in our model (see Table 2) was 2.023, well below the 3.3 threshold. Therefore, this provides further evidence for ruling out the potential for common-method bias.",9.72%,77.82%,78.02%,39.77%,72.52%
349,75.jpg," Table 4. Indirect effects of perceived privacy concerns, perceived ability, perceived benevolence, perceived integrity, and anthropomorphism on consumer forgiveness via now","Once the quality of the measurement model was guaranteed and before evaluating the structural model, a collinearity test was carried out. This collinearity test was performed to rule out any potential bias in path coefficients due to critical levels of collinearity among the predictor constructs [137]. Analogous to the assessment of composite measurement models, VIF values should be lower than 3. Table 4 shows the results obtained. As can be observed, all VIFs are well below the established threshold, the highest one being 2.121. Therefore, collinearity in the structural model is not a problem in this research.",3.14%,87.89%,81.30%,1.15%,73.30%
350,76.jpg," **Table 1.** Correlation matrix among study variables (N = 250)

| Variable | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
| 1. Consumer forgiveness | 1 |
| 2. nWOM | -0.41 | 1 |
| 3. Perceived privacy concerns | -0.15 | 0.24 | 1 |
| 4. Anthropomorphism | -0.25 | -0.17 | -0.21 | 1 |
| 5. Perceived empathy | -0.20 | -0.14 | 0.10 | 0.16 | 1 |
| 6. Perceived ability | 0.02 | -0.23 | -0.22 | 0.32 | 0.07 | 1 |
| 7. Perceived benevolence | -0.21 | -0.05 | -0.09 | 0.25 | 0.26 | 0.39 | 1 |
| 8. Perceived integrity | -0.22 | -0.06 | 0.13 | 0.27 | 0.22 | 0.44 | 0.49 | 1 |
| 9. Ln Age | 0.17 | 0.16 | 0.08 | 0.08 | 0.09 | 0.06 | 0.12 | 0.22 | 1 |
| 10. Education | -0.09 | 0.18 | 0.09 | 0.11 | 0.07 | -0.02 | 0.04 | 0.05 | 0.06 | 1 |
| 11. Dispositional anger | -0.22 | 0.21 | 0.06 | 0.13 | -0.12 | -0.07 | -0.05 | -0.07 | -0.04 | -0.13 | 1 |
| 12. Dispositional compassion | -0.26 | 0.25 | 0.11 | 0.16 | 0.11 | 0.12 | 0.08 | 0.11 | 0.05 | 0.14 | -0.23 | 1 |
| 13. Mean | 5.71 | 5.94 | 5.34 | 5.06 | 5.19 | 5.61 | 5.91 | 4.91 | 3.35 | 5.51 | 4.51 | 4.62 |  |

Note. **r > .15**, p < .05; **r > .12**, p < .01; **r > .088**, p < .05.","To test the significance and strength of the proposed relations, we used a one-tailed 5000 subsample BCA bootstrap [137]. Fig. 2 and Table 5 show the results obtained. As can be observed, quality data shows a very strong and positive relationship with IT-enabled data analytics sensing (ß = 0.689). Thus, hypothesis H1 is supported. Moreover, both quality data (ß = 0.340) and IT-enabled data analytics sensing (ß = 0.317) are positively and significantly related to marketing innovation. As the indirect effect of quality data on marketing innovation via IT-enabled data analytics sensing is positive and significant (ß1 × ß2 = 0.219), partial mediation applies. Hence, hypotheses H2a and H2b are supported.",0.17%,23.09%,78.05%,34.00%,77.48%
351,77.jpg," Table 1. Convergent and Discriminant Validity

Note: *Italics* values are square root of AVE.","The coefficient of determination (R2 value) of the mediating and dependent variables was also examined, which represents a measure of in-sample predictive power that also indicates explanatory power [122, 137]. As can be observed in Table 6, the amount of variance explained for IT-enabled data analytics sensing reached 47.4%, while for marketing innovation, it scored 38.3% and for market performance 34.1%. Moreover, changes in R2 when a specified exogenous construct is omitted from the model were analyzed by means of the so-called ??2 effect size? [137]. According to Hair et al. [137], for a construct to be relevant when explaining another variable, its effect size should reach the minimum threshold of 0.02. This was the case for both the independent and mediating variables, except for quality data vis-à-vis market performance.",9.01%,66.10%,79.44%,0.01%,85.59%
352,78.jpg, Table 3. Indirect effects.,"Fig. 1 provides an overview of the four types of approaches, the sensemaking value ascribed to the technology, and the associated innovation phase reflecting actions (i.e., whether actions are already taken toward adoption or not). The lightning bolts indicate discrepancies between sensemaking results and blockchain adoption activities. For the sake of simplicity, we will speak of types of blockchain approaches in the following and explain them without explicitly mentioning warrants again. The warrants are already textually merged with the description of four types and the concrete approaches of the organizations in the pre-adoption phase.",0.00%,76.30%,84.25%,0.00%,56.16%
353,79.jpg," The flowchart presents the procedure for checking the content validity of a measurement model. After developing the measurement model, the correlation between the constructs and variables is checked. If the correlation is low, then the content validity is low. If the correlation is high, then the SEM analysis is performed with and without the constructs. If there is no significant difference between the two models, then the content validity is high. If there is a significant difference, then a new content validity needs to be developed.","Fig. 2 summarizes the results of our analyses. For the sake of clarity, the characteristics of approaches to blockchain are depicted as a spectrum, e.g., the assessment of blockchain's future value ranges between ?limited value? up to ?future value? of the technology, assuming that assessments of contingent and current value lie in between. Considering the differences between the four types of approaches in terms of how they evaluate the business value of blockchain and their sentiments and progress in the innovation process, more differentiated recommendations are needed that account for discrepancies between actions taken toward the technology and how organizations seek to make sense of blockchain.",31.17%,92.88%,83.12%,37.47%,81.67%
354,80.jpg," The flowchart above describes the steps involved in conducting a structural equation modeling (SEM) analysis. The first step is to develop a measurement model, which specifies the relationships between the observed variables and the latent variables. The next step is to assess the correlation between the latent variables and the control variables. If the correlation is low, then the structural model can be developed. However, if the correlation is high, then the SEM analysis cannot be conducted. In this case, an alternative explanation for the results should be sought.

The structural model specifies the relationships between the latent variables. The next step is to assess the fit of the structural model. If the fit is good, then the results of the SEM analysis can be interpreted. However, if the fit is poor, then the model should be modified and re-tested.

The final step is to interpret the results of the SEM analysis. The results can be used to test hypotheses about the relationships between the latent variables. The results can also be used to identify the most important variables in the model.","This data collection approach generated a diverse pool of key informants with diverse professional backgrounds and at various stages of their career. Descriptive characteristics of the key informants as well as the identification of key informants (abbreviated with the letter ?I? for informant and numbered consecutively) are summarized in Table 1. Each of the target organizations tasked a relatively small team of people with blockchain knowledge to (potentially) develop and test a prototype and, thus, interviewed key informants are assumed to be representative of their organization given the very small team size. ",28.83%,90.31%,81.18%,40.66%,84.63%
355,81.jpg," The table shows the relationships between the themes, sample quotes, conceptual categories, and aggregate dimensions. Thematic labels on the left correspond to the themes that emerged from the data analysis. The middle column presents sample quotes from the participants that exemplify each theme. The right column shows how each theme relates to the three conceptual categories and two aggregate dimensions.", Table 2 below provides an overview of our primary and secondary data.,4.93%,71.60%,84.91%,34.44%,53.56%
356,82.jpg," This image shows the four main contextual factors that influence ISD team practices in organizations. These factors are structure, technology, people, and task.

Structure refers to the way in which the ISD team is organized. This can be either formal or informal. A formal structure is one in which there is a clear hierarchy and division of labor, while an informal structure is one in which there is more flexibility and less bureaucracy.

Technology refers to the tools and resources that the ISD team uses to complete its work. This can include things like computers, software, and other equipment.

People refers to the members of the ISD team. This includes both the team leader and the team members. The skills, experience, and attitudes of the team members can all have a significant impact on the team's success.

Task refers to the work that the ISD team is trying to accomplish. This can be anything from developing a new product or service to improving an existing one. The nature of the task can also have a significant impact on the team's success.

The four contextual factors shown in this image are all interrelated and can have a significant impact on ISD team practices in organizations. By understanding these factors, organizations can better support their ISD teams and improve their chances of success.","H1. The WHO's announcement of COVID-19 as a ?pandemic? generates a significant loss of valuation for SCF firms. H2. The WHO's announcement of COVID-19 as a ?pandemic? generates a lower valuation loss for blockchain-enabled SCF firms compared to other SCF firms. H3. Due to the WHO's announcement of COVID-19 as a ?pandemic,? a lower percentage of blockchain-enabled SCF firms incur valuation loss compared to other SCF firms. H4. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly higher trading volume for SCF firms. H5. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly lower trading volume for blockchain-enabled SCF firms than that for other SCF firms. H6. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly different valuation loss for banking and non-banking SCF firms that are blockchain-enabled and for those that are not. H7a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more R&D intensive than other SCF firms. H7b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more capital intensive than other SCF firms. H7c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more labor intensive than other SCF firms. H8a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more R&D intensive than other blockchain-enabled SCF firms. H8b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more capital intensive than other blockchain-enabled SCF firms. H8c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more labor intensive than other blockchain-enabled SCF firms. H9a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more R&D intensive than other blockchain-enabled SCF firms. H9b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more capital intensive than other blockchain-enabled SCF firms. H9c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more labor intensive than other blockchain-enabled SCF firms. We summarize our hypotheses in the form of a conceptual framework and present it in Fig. 2.",17.87%,76.64%,78.67%,15.59%,81.32%
357,83.jpg," The table summarizes the data collection methods used in the study. A total of 62 interviews were conducted, with a total duration of 3529 minutes. The average duration of an interview was 56.92 minutes, with a minimum duration of 19 minutes and a maximum duration of 100 minutes. In addition to the interviews, 196 days of participant observation were conducted. These observations included passive observations of the workplace, as well as active participation in meetings, workshops, and steering committees. Data analysis was conducted using a denaturalized transcription, coding, and Gioia et al. (2013) approach."," Table 4 lists the pairwise correlation among all variables, variance inflation factor (VIF) for independent variables, and relevant descriptive statistics. A positive and significant correlation exists between the two main explanatory variables: Capex and Staffex. Among the control variables, CP and Size are highly associated with other variables. VIF scores for the independent variables indicate that the regression models used in the study are not affected by multi-collinearity. Additional daily data related to FF3F and C4F are obtained from Kenneth French's data library. To ensure that the impact of the WHO's announcement is free from any bias induced by extreme observations, we also compute the CAAR of the outlier-adjusted sample firms. The outlier-adjusted sample comprises all sample firms except those that fall in the top 10% or bottom 10% in terms of the CAR generated from day -30 to day 50.",31.41%,89.25%,82.05%,27.96%,78.56%
358,84.jpg," The table shows the roles, employment lengths, places of work, and highest qualifications of employees in a company. The company has 37 employees in its headquarters, 22 in its subsidiary, and 3 external employees. The employees in the headquarters have an average employment length of 5.5 years, while the employees in the subsidiary have an average employment length of 6.8 years. The external employees have an average employment length of 3 years. The majority of the employees in the headquarters (41%) and the subsidiary (59%) work in Switzerland. The external employees work in different locations, with one employee working in Europe, one in Southeast Asia, and one in Central Europe. The majority of the employees in the headquarters (57%) and the subsidiary (64%) have a bachelor's degree as their highest qualification. The external employees have a variety of highest qualifications, including a bachelor's degree, a master's degree, and a PhD.","Table 5 presents some descriptive statistics related to the sample firms for the pre-event and the post-event windows (including the event day). The pre-event period denotes the time interval from 30 days to 1 day prior to the event, and the post-event period includes the time period from the event date to 50 days after it. From Table 5, it is evident that the average daily unadjusted dollar-denominated price reduces from the pre-event ($34.46) to the post-event period ($27.53). In the pre-event period, the average daily return percentage (annualized) is recorded as a minor loss (-0.62%). However, the return decreases even further in the post-event period (-2.03%). The volatility of the return increases in the post-event period (6.32%) compared to the pre-event period (3.06%). The logarithm of the average daily volume of trades also increases after the event. Also, the average daily market capitalization reduces slightly from the pre- to the post-event period. However, there is no change recorded for average shares outstanding between the two-time windows.",38.78%,85.18%,81.44%,39.68%,79.09%
359,85.jpg," The figure shows the relationships between text features, task-related features, characteristics of conversational ability, and task completion and perceived conversational ability of a text-based chatbot.","To investigate the effect of the WHO's announcement on SCF firms, we determine the CAAR as per Eq. (2) and present our findings in Table 6. We estimate the CAAR for two sets of firms (i.e., all sample firms and outlier-adjusted sample firms) over different event windows. From Table 6, we observe that our sample firms incur a significant loss in valuation close to the event date. According to the MM, MMEGE, FF3F, and C4F models, the firms on an average earn -2.8%, -2.1%, -3.2%, and -2.8%, respectively, around the event window [-1, ]. These significant valuation losses are consistently observed throughout the entire event window. In the longest window of our study, (i.e., [-30, ]), sample firms experience a negative and significant CAAR of -19.3%, -23.8%, -28.0%, and -22.3% as per the MM, MMEGE, FF3F, and C4F models, respectively. Therefore, it seems that there is a permanent valuation loss for all SCF firms due to the announcement. This negative impact is not immediate as we do not find a significant drop in the [0, ] event window. However, the loss is quite prominent in the [0, ] window. This significant loss can also be observed in the [-15, ] event window, as estimated by the C4F model. Fig. 3 provides a visual depiction of the valuation loss of firms. The valuation loss is persistent even for a long post-event time period (i.e., [, ]). In contrast, none of the models depict any significant valuation loss for the sample firms in the pre-event time period (i.e., [-30, -1]).",19.54%,75.33%,80.07%,0.00%,71.51%
360,86.jpg," The table shows the number of successful and unsuccessful conversations between the user and the system, as well as the percentage of successful conversations for each type of conversation. 

For short conversations, 57.1% of female users and 42.9% of male users obtained a recipe, while for long conversations, 47.2% of female users and 52.8% of male users obtained a recipe. 

Overall, female users were more successful in obtaining a recipe than male users, with 51.5% of female users obtaining a recipe compared to 48.5% of male users.","We perform a sub-sample analysis and explore the impact of the WHO's announcement on BlockFirms and Non-BlockFirms, separately. We compute the CAAR for these two groups of firms using the same MM, MMEGE, FF3F, and C4F models across all event windows and report the results in Table 7. Interestingly, there is not a single event window where BlockFims incur a significant loss of valuation. On the contrary, Non-BlockFirms exhibit a significant valuation loss across all event windows. It seems that valuation loss encountered by all SCF firms in the sample is predominantly driven by abnormal losses for Non-BlockFirms. BlockFirms, on the other hand, show enough investor confidence during the turbulent time period.",21.21%,67.31%,82.55%,35.36%,77.54%
361,87.jpg, Table 1. Sample characteristics,"This negative and significant CAAR for Non-BlockFirms can be potentially caused by either the higher magnitude of losses of a few sample firms or a significantly large number of sample firms that move into the loss-making domain due to the announcement. We explore the same using a binomial sign test. This test measures whether the percentage of firms earning negative returns on a particular day in a specific time window is significantly different from 50% or not. We compute the AAR earned by BlockFirms and Non-BlockFirms and the percentage of firms from each group earning losses on a particular day in the [-15, ] event window. We show the results obtained from the MM and C4F models in Table 8. The columns ?Mean (%)? and ?Negative (%)? represent the AARs and the percentage of loss-earning firms on each day, respectively. It yields two important insights. First, the C4F model shows a lower impact of the announcement than the MM model. The C4F model includes traditional asset pricing factors that play an important role in explaining the AAR. Second, there are few days, i.e., Days 1, 9, and 12, when a higher number of firms among BlockFirms earn negative returns. In contrast, Non-BlockFirms earn losses on 7 out of 15 days in the post-event period. Even Non-BlockFirms start experiencing valuation loss from one day before the event day, probably due to some information leakage or anticipation of panic. These findings support Hypothesis 3.",3.38%,69.74%,82.45%,0.00%,80.00%
362,88.jpg, Table 2. Comparison of female and male participants on the variables of interest,"To determine the impact of the WHO's announcement on the trading volume, we compute the CAAV around the event date using Eq. (5). Table 9 presents the estimated CAAV for all sample SCF firms as well as for the sub-samples BlockFirms and Non-BlockFirms across different event windows. From Table 9, it is evident that all sample SCF firms generate abnormally high trading volume around the event day and in the post-event period. The significant increase in trading volume is evident immediately after the event. In the event window [0, ], sample firms on the whole experience a CAAV of 1.335, and the CAAV increases up to 12.454 surrounding the event window (i.e., [-30, ]). This insight is consistent with the low return and high volume relationship in the bear market reported by Chen [77]. Thus, it statistically supports Hypothesis 4 of this study. However, such a significant increase in abnormal trading volume is guided by Non-BlockFirms. Non-BlockFirms experience a much higher trading volume compared to BlockFirms. While a significant increase in the trading volume for BlockFirms is observed only within the first two days of the event day, it is consistently higher in case of Non-BlockFirms for most of the event windows. This finding supports Hypothesis 5.",23.39%,80.63%,81.96%,0.00%,87.44%
363,89.jpg," | Task-related features | Task completed | Task not completed | P |
|---|---|---|---|
| No. recipe suggestions (mean ± std.) | 5.85 (4.431) | 7.33 (5.743) | 0.525 |
| No. correct recipe suggestions (mean ± std.) | 4.19 (3.511) | 5.33 (5.433) | 0.863 |
| Percentage of correct recipe suggestions (mean ± std.) | 79.87 (32.145) | 71.86 (43.721) | 0.362 |
| Recipe criteria: narrativeness of KIM (mean ± std.) | 4.251 (28.643) | 59.52 (19.298) | 0.041 |
| Recipe criteria: ingredients (mean ± std.) | 0.35 (0.497) | 0.64 (0.497) | 0.034 |
| Recipe criteria: difficulty level (mean ± std.) | 0.55 (0.500) | 0.64 (0.497) | 0.514 |
| Time spent in conversation in minutes (mean ± std.) | 2.69 (1.338) | 3.00 (1.240) | 0.304 |
| Text features |  |  |  |
| No. messages from KIM (mean ± std.) | 14.16 (5.637) | 17.57 (7.272) | 0.051 |
| No. messages from the user (mean ± std.) | 9.21 (3.452) | 10.93 (4.552) | 0.047 |
| No. messages from KIM * user (mean ± std.) | 23.37 (8.917) | 28.50 (10.219) | 0.035 |
| No. words from KIM (mean ± std.) | 148.21 (136.762) | 210.43 (143.872) | 0.038 |
| No. words from the user (mean ± std.) | 362.16 (195.704) | 400.43 (279.203) | 0.988 |
| No. words from KIM * user (mean ± std.) | 22.51 (13.975) | 22.51 (13.975) |  |
| No. words from KIM \ user (mean ± std.) | 384.67 (202.237) | 429.93 (281.747) | 0.789 |
| No. emojis used by KIM (mean ± std.) | 1.47 (1.488) | 0.86 (1.027) | 0.173 |
| No. emojis used by User (mean ± std.) | 960.87 (1385.264) | 908.71 (836.526) | 0.801 |
| Characteristics of conversation ability |  |  |  |
| No. conversation restarts by KIM (mean ± std.) | 0.61 (1.063) | 1.57 (1.828) | 0.006 |
| No. greetings by KIM (mean ± std.) | 1.21 (0.426) | 1.22 (0.516) | 0.983 |
| Correct greeting by KIM (mean ± std.) | 0.79 (0.426) | 0.79 (0.426) |  |
| Evaluation |  |  |  |
| Satisfaction of the user (mean ± std.) | 4.34 (1.477) | 3.31 (2.228) | 0.047","To explore whether the firm characteristics in our sample of SCF firms play an important role in our findings, we perform a sub-sample analysis. More specifically, we divide both BlockFirms and Non-BlockFirms into two groups: banking and non-banking. We compute the ARs earned by these two groups of firms in different event windows using the Carhart 4-factor model (C4F). The outcomes of the analysis are documented in Table 10.",5.33%,24.05%,76.11%,34.89%,49.79%
364,90.jpg, Table 4. Results of the regression analyses.,"To identify the predictive factors explaining the valuation loss for SCF firms, we perform a cross-sectional regression analysis, following the description in Section 4.3. The CAR estimated by the C4F model for the [-15, ] time window is used as the dependent variable in the regression. The results for models 1 to 4 (as specified in Section 4.3) are reported in Table 11. In model 1, Block_Dummy is found to be positive and significant. This indicates that Non-BlockFirms earn higher valuation loss compared to BlockFirms. Models 2 and 3 in Table 11 show that the impact of the interaction between Block_Dummy and R&D on CAR is significant. It is also observed that the interaction of Block_Dummy and Capex has a positive and significant relationship with CAR. However, there is no significant relationship between CAR and the interaction term Block_Dummy*Staffex. Therefore, we infer that BlockFirms that make a higher investment in R&D and capital expenditure suffer significantly less valuation loss due to the event. Thus, we find support for Hypothesis 7a and 7b, but not for Hypothesis 7c. The control variables show a consistent association with CAR across all four regression models.",26.18%,85.88%,84.92%,0.00%,88.32%
365,91.jpg," The table shows the results of two studies that tested eight hypotheses about the relationship between task-related features, text features, and characteristics of conversational ability.

The first study found that:
- H1a: There was a positive correlation between the % of correct responses to requests and perceived conversational ability.
- H1b: There was a positive correlation between % of correct responses to requests and perceived conversational ability.
- H2a: There was a positive correlation between % of correct questions about request criteria and perceived conversational ability.
- H2b: There was a positive correlation between % of correct questions about request criteria and perceived conversational ability.
- H3a: There was no correlation between time spent in conversation and perceived conversational ability.
- H3b: There was no correlation between time spent in conversation and perceived conversational ability.

The second study found that:
- H4a: There was no correlation between number of messages of user/text-based chatbot and perceived conversational ability.
- H4b: There was a positive correlation between number of messages of user/text-based chatbot and perceived conversational ability.
- H5a: There was a positive correlation between number of words of user/text-based chatbot and perceived conversational ability.
- H5b: There was a positive correlation between number of words of user/text-based chatbot and perceived conversational ability.
- H6a: There was a positive correlation between number of emojis of text-based chatbot and perceived conversational ability.
- H6b: There was a positive correlation between number of emojis of text-based chatbot and perceived conversational ability.
- H7a: There was a negative correlation between number of conversation restarts and perceived conversational ability.
- H7b: There was a negative correlation between number of conversation restarts and perceived conversational ability.
- H8a: There was a positive correlation between presence of a correct greeting and perceived conversational ability.
- H8b: There was a positive correlation between presence of a correct greeting and perceived conversational ability.","So far, we find that the adoption of blockchain enables SCF firms (i.e., BlockFirms) to protect against the erosion of firm value during the pandemic. Moreover, R&D and capital expenditure play an important role in this regard. Next, we aim to identify specific predictive factors that guide BlockFirms to protect the market value. We again run a set of cross-sectional regressions where we use CAR of BlockFirms estimated by the C4F model for the [-15, ] time window as the dependent variable. More specifically, we run models 5 to 10 (as specified in Section 4.3) to explain the impact of consortium and implementation dummies on the CAR of BlockFirms and report the results in Table 12. Results of models 5 and 6 reveal that the interaction terms Consortium*R&D and Implementation*R&D are positive and significant. While interacting capital expenditure (Capex) of BlockFirms with consortium and implementation dummies, we find a similar positive and significant association as depicted in the results of models 7 and 8. However, no significant association is observed in case of the interactions between staff expenditure (Staffex) and consortium and implementation dummies in models 9 and 10.",25.87%,81.68%,79.51%,41.43%,76.57%
366,92.jpg," The figure shows the hypothesized relationships among the constructs in the research model. The model proposes that individual-level antecedents (motivations), such as involvement, perceived anonymity, and consumption, have a direct effect on community engagement level. Community engagement level, in turn, has a direct effect on brand relationship quality (BRQ). Furthermore, BRQ has a direct effect on behavioral outcomes, including trip decision-making and cross-buying. Finally, the model proposes that consumption and community engagement level have an indirect effect on behavioral outcomes through BRQ."," Fractions of top ratings in all categories are provided in Table 1 in Section 4. One can only speculate why such good ratings are usually given. AirBnB does not provide monetary incentives for reviews, which have been shown to lead to more positive reviews [114]. Possible explanations could be an ex post rationalization of the decision made by the guest (?I have chosen this accommodation, so it must be good?), or reciprocity between guests and hosts [30], [80]. On AirBnB, guests are also rated by hosts and might hope for a reciprocal favorable evaluation of themselves if they evaluated the host and the accommodation very positively.",16.61%,82.54%,81.25%,33.37%,70.96%
367,93.jpg," The figure shows the structural equation model (SEM) of the relationships between the constructs in the study. The model was tested using AMOS 24. The results show that all of the constructs are significantly related to each other, with the exception of the relationship between perceived anonymity and decision influence. The strongest relationship is between contribution and hot BRQ, followed by the relationship between consumption and hot BRQ. The model explains 67% of the variance in hot BRQ, 56% of the variance in cold BRQ, 49% of the variance in decision influence, and 38% of the variance in cross-buying.","Table 2 presents the results of this analysis. The F values and their significance refer to the comparison to the previous model. Although adding both types of information to the baseline model leads to a significant improvement of fit, the effect of premises information is much stronger. Adding premises-specific variables to the baseline model increases the by almost 0.08, whereas adding the host information increases it by less than 0.02, and the F-test also clearly indicates a much stronger increase in model fit due to the premises information. Consequently, adding the host information to the model already containing the premises information leads only to a marginal improvement. Thus premises information has a considerably stronger influence on price than host information. This result is in accordance with other results from the literature. Although they did not specifically test for differences between premises-specific and host-specific information,Chen and Xie [24] also used nested regression models in which they first added premises-specific and then host-specific variables. In their model, adding host-specific variables increased the from 0.538 to 0.673. ",46.26%,82.00%,82.81%,26.04%,85.06%
368,94.jpg," The figure shows the structural equation model (SEM) results. The model was used to investigate the relationships between the variables: perceived anonymity, involvement, consumption, contribution, cold BRQ, hot BRQ, decision influence, cross-buying, and creation. The results show that perceived anonymity has a positive effect on involvement, which in turn has a positive effect on consumption. Perceived anonymity also has a positive effect on contribution, which in turn has a positive effect on consumption. Cold BRQ has a positive effect on decision influence, which in turn has a positive effect on cross-buying. Hot BRQ has a positive effect on decision influence, which in turn has a positive effect on cross-buying. Perceived anonymity has a positive effect on creation. Involvement has a positive effect on creation. Consumption has a positive effect on creation. Contribution has a positive effect on creation. Decision influence has a positive effect on creation. Cross-buying has a positive effect on creation.","Table 3 shows the results of this analysis. The results confirm those of the analysis of all premises types. Again, the effect of adding the premises-specific characteristics is considerably stronger than the effect of adding host characteristics.",12.75%,91.70%,83.95%,36.82%,78.14%
369,95.jpg," The table shows the frequency and percentage of respondents in the US and China for different variables.

For the variable ""Gender"", there are 352 males and 355 females in the US, and 352 males and 450 females in China. The percentage of males in the US is 49.8%, and the percentage of females in the US is 50.2%. The percentage of males in China is 43.9%, and the percentage of females in China is 56.1%.

For the variable ""Age"", there are 120 respondents under the age of 26 in the US, and 171 respondents under the age of 26 in China. The percentage of respondents under the age of 26 in the US is 16.6%, and the percentage of respondents under the age of 26 in China is 21.3%. There are 313 respondents aged 26-41 in the US, and 296 respondents aged 26-41 in China. The percentage of respondents aged 26-41 in the US is 43.4%, and the percentage of respondents aged 26-41 in China is 36.9%. There are 175 respondents aged 42-56 in the US, and 222 respondents aged 42-56 in China. The percentage of respondents aged 42-56 in the US is 24.2%, and the percentage of respondents aged 42-56 in China is 27.7%. There are 113 respondents aged 57-76 in the US, and 111 respondents aged 57-76 in China. The percentage of respondents aged 57-76 in the US is 15.7%, and the percentage of respondents aged 57-76 in China is 13.8%. There is 1 respondent over the age of 76 in the US, and 2 respondents over the age of 76 in China. The percentage of respondents over the age of 76 in the US is 0.1%, and the percentage of respondents over the age of 76 in China is 0.2%.

For the variable ""Education"", there are 174 respondents with a high school education in the US, and 70 respondents with a high school education in China. The percentage of respondents with a high school education in the US is 24.1%, and the percentage of respondents with a high school education in China is 8.7%. There are 88 respondents with a two-year college education in the US, and 131 respondents with a two-year college education in China. The percentage of respondents with a two-year college education in the US is 12.2%, and the percentage of respondents with a two-year college education in China is 16.3%. There are 295 respondents with a four-year college education in the US, and 526 respondents with a four-year college education in China. The percentage of respondents with a four-year college education in the US is 40.9%, and the percentage of respondents with a four-year college education in China is 65.6%. There are 155 respondents with a graduate degree in the US, and 71 respondents with a graduate degree in China. The percentage of respondents with a graduate degree in the US is 21.5%, and the percentage of respondents with a graduate degree in China is 8.9%. There are 10 respondents with other types of education in the US, and 4 respondents with other types of education in China. The percentage of respondents with other types of education in the US is 1.4%, and the percentage of respondents with other types of education in China is 0.5%.

For the variable ""Type of Trip"", there are 609 respondents who took domestic trips in the US, and 780 respondents who took domestic trips in China. The percentage of respondents who took domestic trips in the US is 84.3%, and the percentage of respondents who took domestic trips in China is 97.3%. There are 113 respondents who took international trips in the US, and 22 respondents who took international trips in China. The percentage of respondents who took international trips in the US is 15.7%, and the percentage of respondents who took international trips in China is 2.7%.",The following tables provide an overview of descriptive statistics of the data set used. Table 4 summarizes data according to country.,20.04%,58.53%,79.13%,33.85%,71.12%
370,96.jpg, All items show adequate to excellent convergent validity with all items loading above 0.70 on their respective constructs. All constructs demonstrate adequate discriminant validity with the square root of the AVE for each construct exceeding the correlations between constructs.,Table 5 according to the type of premises.,13.96%,70.41%,84.13%,38.46%,51.87%
371,97.jpg," Table 1 shows the pooled data results of the hypothesized relationships among the constructs. All of the hypothesized relationships were supported (p < 0.05). The strongest relationships were found between perceived anonymity and HTBRQ (r = 0.896), followed by HTBRQ and DMI (r = 0.848), and perceived anonymity and DMI (r = 0.814). The weakest relationship was found between CB and PA (r = 0.177).",Table 6 gives an overview of host characteristics.,5.53%,50.64%,84.07%,35.38%,51.11%
372,98.jpg," Table 4 shows the results of the hypotheses testing. All seven hypotheses were supported by the data. The path coefficients for the seven hypotheses were all significant at the 0.05 level. The t-values for the seven hypotheses were all significant at the 0.05 level. The p-values for the seven hypotheses were all significant at the 0.05 level. Therefore, all seven hypotheses were supported by the data.",Table 7 provides the regression coefficients for the three models using all types of premises.,35.04%,83.72%,86.00%,40.40%,64.14%
373,99.jpg," The table shows the results of a study that examined the effects of various factors on customer engagement. The factors studied were consumption customer engagement (CCE), contribution customer engagement (CnCE), creation customer engagement (CrCE), cross-buying (CB), trip-planning (TP), hot brand relationship quality (HBRQ), customer involvement (CI), and perceived anonymity (PA).

The results show that all of the factors studied had a significant effect on customer engagement. The strongest effects were found for HBRQ, CI, and CnCE. These factors were all found to have a positive effect on customer engagement, meaning that they are all important factors to consider when trying to increase customer engagement.

The weakest effect was found for TP. This factor was found to have a negative effect on customer engagement, meaning that it is a factor that should be avoided if possible.

The other factors studied, CCE, CrCE, CB, and PA, were all found to have a positive effect on customer engagement. However, their effects were not as strong as those of HBRQ, CI, and CnCE.

Overall, the results of this study suggest that there are a number of factors that can be used to increase customer engagement. These factors include HBRQ, CI, CnCE, CCE, CrCE, CB, and PA. By focusing on these factors, businesses can increase their chances of engaging customers and building long-term relationships.",Table 8 contains the regression coefficients of the full model (including both premises-specific and host-specific information) for six selected types of premises.,14.76%,81.35%,80.67%,35.16%,71.89%
