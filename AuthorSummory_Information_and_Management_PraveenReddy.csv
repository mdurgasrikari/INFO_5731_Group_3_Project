index,Author Summary
0,"Survey results - usage and demand among Swedish extension offices
The respondents were asked what type of weather or climate information they currently use in their profession, and the results show that weather forecasts are the most common type of information used among the respondents (Fig. 2), whereas climate change projections are the least used. On the four-point Likert scale ranging from ?Never? to ?Very often? using the information, 26 (40.0 % of the respondents to that question) replied that they never use climate change projections. While only six respondents use climate change projections ?very often?, 52.5 % (n = 21) of the respondents that replied that they have used projections at some point (n = 40,) regarded the information as very important to their profession (see, Supplementary Material). Conversely, two respondents that use climate projections responded that they do not regard the information as important in their profession."
1,"We developed a flowchart to diagnose tuberculous pleurisy, pleural infection, malignant pleural effusion, and other diseases by using the above markers (Fig. 1). The flowchart includes the following seven markers: pleural ADA =40 IU/L, pleural fluid LDH <825 IU/L, pleural fluid ADA/TP < 14, neutrophil predominance or cell degeneration, peripheral blood WBC =9200/µL or serum CRP =12 mg/dL, pleural amylase =75 U/L, and presence of pneumothorax; pleural LDH/ADA <15 was excluded according to the algorithm of a decision tree. The final decision columns in the flowchart are labeled from left to right as Groups A to I. The diagnostic accuracy rate was 71.7 % for the four diseases, with 79.3 % sensitivity and 75.4 % positive predictive value (PPV) for tuberculosis pleurisy, 75.8 % sensitivity and 83.2 % PPV for pleural infection, 88.6 % sensitivity and 68.8 % PPV for malignant pleural effusion, and 33.0 % sensitivity and 60.0 % PPV for other diseases in the flowchart. Misdiagnosis ratios were 4.6 % (n = 36/775) for tuberculosis pleurisy, 6.8 % (n = 52/762) for pleural infection, 8.3 % (n = 41/494) for malignant pleural effusion, and 16.6 % (n = 140/843) for other diseases.The diagnostic accuracy rate was 71.7 % for the four diseases, with 79.3 % sensitivity and 75.4 % positive predictive value (PPV) for tuberculosis pleurisy, 75.8 % sensitivity and 83.2 % PPV for pleural infection, 88.6 % sensitivity and 68.8 % PPV for malignant pleural effusion, and 33.0 % sensitivity and 60.0 % PPV for other diseases in the flowchart. The misdiagnosis ratios were 4.6 % (n = 36/775) for tuberculosis pleurisy, 6.8 % (n = 52/762) for pleural infection, 8.3 % (n = 41/494) for malignant pleural effusion, and 16.6 % (n = 140/843) for other diseases."
2,"A diagram illustrating the mixed-methods design is shown in Fig. 1. In the QUANT phase, students enrolled in the IPC CVS-II course were invited to participate in a voluntary (with no compensation) pre- and post-survey on the effectiveness of flowcharts using a 5-point Likert-like rating scale (Strongly agree to Strongly disagree). Three students from the 2019 class and 2 faculty members were asked to review the initial survey and provide feedback. Students' performance was measured as scores in the 3 content areas (shock, antiarrhythmic, and acute coronary syndrome) on the summative examinations with (2019?2021) or without flowcharts (2017?2018). The data were analyzed using SPSS (version 28.0.0.0, Chicago, IL). Once the assumptions of normality and homogeneity were met, a one-way analysis of variance was used to determine the effectiveness of student performance, and the descriptive statistics for each cohort were recorded."
3,"As shown in Fig. 2, the coding analysis yielded 21 codes placed into 4 categories. Three major themes emerged from the qual analysis, namely ?used as a medium for retention and recall,? ?used as a study tool,? and ?used as a decision-making framework.?"
4,"A flowchart for selecting an appropriate surgical airway for the long-term management of NIPP with intubation was created by a laryngologist and a pediatrician. Our flowchart includes the following main branching points; possibility of extubation, intubation period, reversibility related to prognosis, history of repeated aspiration pneumonia and/or continuous saliva aspiration, and permission from the parents for elimination of the patients? vocal function (Fig. 1). The included surgeries were as follows: Tracheostomy with Temporary Tracheostoma (TwTT); Tracheostomy with Permanent Tracheostoma (TwPT), which includes sutures from the anterior wall of the trachea to the surrounding skin flaps7, 8; and APSs such as LTS."
5,"TRANSMAT [2], [3] Ontological and Terminological Resource (OTR) represents concepts in the food packaging domain and the relations between them. This OTR is structured in two parts, a core ontology and a domain ontology (see Fig. 1 and [1] for further details). The up-core ontology is the representation of the n-Ary relations structure, defined as a relation concept linked to the arguments composing the relation. The down-core ontology includes concepts specific to the experimental fields, while the concept is categorized as symbolic or quantitative which are associated with measurement units used in experimental fields."
6,"Overall, an acceptable recall i.e. can be obtained with slight candidate filtering. The selection of several candidates has little effect when using frequentist methods. On the other hand, precision is a problem with these methods, i.e. the precision did not exceed with any of the parameters we tested."
7,"BERTeley has three main features: scientific article preprocessing, topic modeling, and metric calculation. BERTeley?s preprocessing suite addresses the aforementioned challenge by removing specific words from the input data. The topic modeling and metric calculation add quality-of-life features such as pre-selected language models trained specifically on scientific articles, and one-line topic modeling metric calculation. The BERTeley workflow can be seen in Fig. 1. BERTeley can be installed by running the command pip install berteley at the command line (Eric Chagnon and Ushizima, 2023)."
8,"Fig. 1 outlines the selection process. The search yielded 9427 potential publications from databases, registries and additional sources. After removing duplicates, 7328 records were screened based on their titles and abstracts. Next, 147 articles from databases and registries, and 17 articles from additional sources were assessed for eligibility in full-text. Ultimately, 59 articles comprising 50 studies met the inclusion criteria. When study findings were reported in multiple publications, or there was a need to evaluate a study?s protocol or a pilot test to comprehensively assess an intervention, they were included in the data extraction form together with the main study. However, if additional publications described findings in a research design different from the parent study (e.g. qualitative findings), they were analyzed separately due to differences in methods. The data extraction form synthesizing the review findings can be found in Supplementary file 2."
9,"Studies that have used environmental SSI were analyzed. Different aspects on which these studies were analyzed are mentioned in the flowchart (see Fig. 1). All these aspects were finalized after deliberations among the researchers, leading to a consensus being achieved."
10,"To better understand the signaling process, we propose the following conceptualization. Our key idea is that universities can signal the availability and value of their scientific knowledge through three types of signals transmitted through distinct channels (see Fig. 1): (1) signals to members of scientific communities transmitted through scientific outlets; (2) signals to economic agents transmitted through patents5 and patent offices; and (3) signals to members of society at large transmitted through media outlets."
11,"The architecture of ArZiGo is composed of 5 main modules (see Fig. 1): User Interface, Knowledge Bases, Search Module, Interaction Processing Module, and Recommendation Module. The User Interface Module allows the users to perform queries and to filter the results, sending the queries to the search module and displaying the results. It also provides the captured interactions to the Interaction Processing Module."
12,"Fig. 2 illustrates the process that generates users and their interactions. As a result, 684 users have been created with a total number of 3,500,000 interactions approximately. Once the users have been created, a pseudo-random selection of the users whose recommendations will be evaluated has been carried out, where the selected number has been limited to the number of experts recruited by each domain. To avoid any kind of bias, users with a profile which is too general have been discarded."
13,"Pre-processing module is used to identify the edges present in the flowchart. Input to this module will be a flowchart which will be uploaded by the user. This module first convert input image into a binary image then apply the Canny edge detection algorithms [10] to identify the edges present in the given figure. Among the edge detection methods developed so far, canny edge detection algorithm is one of the most strictly defined methods that provides good and reliable detection. Owing to its optimality to meet with the necessary criteria for edge detection and the simplicity of process for implementation, it became one of the most popular algorithms for edge detection. "
14,"We identified publications indexed in the Scopus database. A paper on epilepsy in LA was defined as having at least one author with a Latin American country affiliation. The search strategy included Medical Subject Heading (MeSH) terms for ?epilepsy?. Publications were limited to original research articles, review articles, letter, note, conference paper, short survey, editorial and erratum. For the complete and final search strategy, please refer to Supplementary material 1. A flowchart of the bibliometric search is shown in Figure 1. The period of publication dates was restricted from 1989 to 2018 and to English, Spanish, and Portuguese language only."
15,"MOOSE, as shown in Fig. 1, can be divided into core capabilities, located in the framework, and physics-specific capabilities, which have been placed in physics modules. The Libtorch-based ML functionalities have been divided between the framework and the MOOSE stochastic tools module (MOOSE-STM) [11], which incorporates algorithms necessary for efficient stochastic analysis, surrogate generation, and data analysis. Even though the syntax of Libtorch is directly available in MOOSE and MOOSE-based applications, several wrapper classes have been created to simplify the utilization, creation, and training of NN models in MOOSE. Fig. 1 presents the integration of Libtorch-based functionalities in MOOSE:"
16,Fig. 2. Left: Flowchart of the steps performed to develop a Decision Tree in a ?Classical? analysis and with our proposed Interactive analysis. Right: Strengths and Limitations of Decision Trees Algorithms and experts.
17,"This paper proposes a method and an algorithm to design a queuing network in terms of parallel test stations at city entrance links. Since the queuing network optimization is based on traffic inflow prediction, a two-stage model is proposed. The first stage is traffic inflow prediction, and the second stage is queuing network optimization. Fig. 2 shows the model's conceptual framework. The first stage is a feedback procedure between trip distribution and traffic assignment. It is usually known as transportation system equilibrium. The detailed models are elaborated on in the following sections."
18,We also present a practical flowchart for deciding which algorithm to use in each specific use case: We believe that this flowchart is a unique contribution to DL users as it can help identify which techniques to use for different use cases in a simple way (Fig. 3). 
19,"All articles for this study were retrieved from the Web of Science Core Collection (WoSCC) database and published between January 1, 2003, and December 31, 2022. The retrieval strategy included the topics ?tuina?, ?Chinese manipulation?, ?Chinese massage?, ?Chinese? and the topic ?manipulation or massage?. Overall, 2064 papers were retrieved and screened according to the following inclusion criteria: (a) written in English, (b) with an abstract, and (c) original articles or reviews. Overall, 1902 articles were obtained, screened, and excluded independently by two researchers (BZ and TY). Any disagreements were resolved by a third researcher (MWS). Finally, 25 studies published in 2023 were excluded, and 1877 studies were included (Fig. 1)."
20,"As shown in Fig. 2, four scenarios (system design, design calculation, procedural and coordination) are considered in this examination for evaluating the three knowledge capabilities (recall, analysis and application) of HVAC designers. These scenarios comprise of key concepts, knowledge, and skills related to HVAC design. Each scenario contains multiple various tasks. System design involves the fundamental concepts and terms in HVAC design, such as conditions, variables, and operational parameters. Tasks in this scenario include ?size supply, return, and exhaust ducts?, ?create HVAC zoning and sensor locations?, and so on. Design calculation involves the tasks related to calculations for HVAC design, such as calculating HVAC system requirements and supporting project estimates for system selection. Procedural focuses on the processes executed by HVAC designers, including analyzing compliance with codes and standards, reviewing shop drawings, performing field reviews, etc. Coordination includes the tasks that need cooperation among multiple project stakeholders, such as ?assist in basis of design development?, ?coordinate HVAC equipment space requirements? and ?comply with client specs and performance?."
21,"The performance test flowchart of LLM models is illustrated in Fig. 4. The prompt is generated using the prompt template. It consists of two parts: instruction and question. Instruction explains the task that LLMs should conduct, which is ?Answer the following single-choice questions about CERTIFIED HVAC DESIGNER exam and explain the reason:? for this study. "
22," Fig. 3 shows some typical questions as instances. These questions can be applied to test the recall capability to remember key concepts of HVAC design (?What does a system manual typically include??), the analysis capability to solve questions that are focused on deep insights and analysis (?What's the MOST accurate statement about the task of sequencing heating and cooling??), and the application capability to answer calculation questions (??calculate the total pressure of a centrifugal fan?)."
23,"Distinguished from being created by human authors, AIGC refers to the automated creation of large-scale content in a manual-cost saving way based on Generative AI (GAI) techniques (Cao et al., 2023). Developed based on GAI techniques from GAN (Goodfellow et al., 2020) to ChatGPT (C. Zhang et al., 2023), AIGC technically refers to utilizing the GAI algorithms for content generation in the context of human instructions that would guide the model to complete the required task, with minimum human engagement in the production process (Fig. 1)."
24,The historical view of the development of Generative AI is given in Fig. 2.
25,"The autoregressive model can be interpreted as a feedforward network incorporating all the preceding contextual information, which forms the foundation of GPT models. In an early stage, models from GPT-1 to GPT-3 are built based on the decoder-only Transformer architecture below and further fine-tuned on specific tasks (Fig. 3)."
26,"GPT-3.5 and GPT4. On top of the GPT-3 model, the GPT-3.5 is extended with supervised finetuning, and reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022). Capturing a large number of language patterns and representations, GPT-3.5 is then expected to logically generate suitable content in the specific contexts. Therefore, pairs of prompts and the corresponding answers written by human labeler are constructed as the dataset for the supervised fine-tuning (Fig. 4), namely instruction tuning (Wei et al., 2022), and large-scale code data are used for model enhancement as well (Chen et al., 2021). Furthermore, considering a higher-level human-machine alignment and the creativity of the model to avoid boilerplate answers, the RLHF is applied to the model, in which a reward model is trained firstly based on the data given by human labeled that indicate the rank of the generated contents for a single prompt, and is then used to give the rewards for the outputs to update the model policy using Proximal Policy Optimization (PPO) (Schulman et al., 2017) in the large loop. Due to the data label-independent property, RLHF punishes and rewards the model behavior and plays the significant role in the breakthrough of GPT-3.5 (Fig. 4). Beyond unimodality, the latest GPT-4 model can handle multimodal tasks from image and textual input to textual output process multimodal input. Despite of the unreleased technical details, GPT-4 demonstrates higher capacity of understanding and human alignment, indicating the continuation of the previous but enhanced training framework."
27,"Disinformation Creation with LLMs (Borji, 2023): Feed the generated prompt into a LLMs to produce high-quality, persuasive disinformation content in different flavors. For instance, using an LLM like GPT-4, the input prompt could generate false news articles outlining alleged unethical behavior by the targeted candidate, complete with fabricated quotes and supporting evidence. A flow diagram for this process can be seen in Fig. 8."
28,"Disinformation Packaging: Create a suitable presentation for the disinformation content, such as a fabricated news article, social media post, or multimedia message. This may involve developing fake headlines, crafting misleading images or thumbnails, and selecting eye-catching formatting styles using AI tools such as break your own news1 and Midjourney2: a tool for generated life like images (Oppenlaender, 2022). This can also be given to an AI video-generating tool such as Synthesia3 to generate an AI anchor for reading out the script for the article. For example, the fabricated article could be given a sensational headline like ?Shocking Scandal: Candidate X Caught in Corruption Scheme!? and paired with an edited image of the candidate in a compromising situation. This can then be made into a video using Synthesia and reported as a news channel report. The flow of this step can be seen in Fig. 9."
29,"Content Dissemination: Execute the dissemination strategy by posting the disinformation content on the designated social media platforms using the fake or compromised accounts. For example, share the fabricated news article on Twitter, Facebook, and Reddit, using the established accounts and following the planned dissemination strategy. This has been illustrated in Fig. 10."
30,"To implement a domain-aware question-answering (QA) system, LeanContext implements a traditional retrieval augmented generation (RAG) framework (Lewis et al., 2020). The RAG system is constructed through two distinct phases: (a) domain data ingestion and (b) query-response. Fig. 2 illustrates the workflow of a domain-aware QA system based on RAG."
31,"For context-based QA, generally, the answers reside within a couple of sentences. If the smallest amount of context for a certain question can be identified, the same response can be provided by LLMs at a lower cost i.e. less prompt tokens. So, identifying the top-sentences can reduce the context without compromising accuracy. Motivated by this simple idea, we propose LeanContext which is shown in Fig. 3."
32,"Fig. 4 shows how LeanContext constructs the Reduced Context. It retains the most relevant top-sentences, which are critical for preserving the core context, while concurrently applying an open-source text reduction method to condense the sentences that lie between these top-sentences. This step helps in eliminating extraneous information and noise from the text. Moreover, any sentences that appear beyond the last top-sentence are omitted from consideration. This selective approach ensures that only the most contextually relevant information is retained, contributing to a more efficient and focused analysis. Importantly, LeanContext also maintains the original order of both the top-sentences and the other sentences in the text. This preservation of sentence order ensures that the structural integrity of the input text remains intact. This holistic approach ultimately leads to more accurate and informative results when processing text data, while still achieving the goal of minimizing LLM API usage costs."
33,"In this section, we introduce ChatDiet, a framework for personalized nutrition-oriented food recommendation chatbots. This framework utilizes LLMs to effectively incorporate not only population knowledge but also individual-specific data through augmented models. It includes an Orchestrator that interacts with personal and population models to extract relevant information based on users? inquiries. It then sends the aggregated information to an LLM to be integrated with the LLM?s internal knowledge and offer interactions with users. Fig. 1 indicates an overview of the framework. To clarify the functionality and definitions of the framework, we present and exemplify different components of ChatDiet via a case study. In the following, we initially delve into our case study, focusing on mHealth system leveraging wearable and mobile-based data logging."
34,"There are different types of human reasoning, each with its unique characteristics and applications (Fig. 1). In this section, we describe these different types of reasoning, following the classification done by Bang et al. (2023): logical reasoning, non-textual semantic reasoning, commonsense reasoning, causal reasoning, and multi-hop reasoning. Understanding different types of human reasoning is essential to evaluate the reasoning skills of advanced language models like GPT-3.5, GPT-4, and BARD."
35,"LangTest workflow involves a systematic process to improve language models. Initially, the library trains a language model using a diverse real world data set. Subsequently, it rigorously tests the model?s performance in various dimensions, including bias, robustness, accuracy, fairness, and security. If the model fails on specific tests, data augmentation can be used on training data to incorporate extra examples specific to the gaps identified. The model is then retrained using this augmented data set, leading to a more reliable version. The iterative nature of this process involves repeated tests to gauge the model?s progress in terms of robustness, fairness, and overall effectiveness (see Fig. 1)."
36,"As presented in Fig. 1, which illustrates the process of the intervention protocol, the Critical Thinking Scale (CTS), Creative Thinking Scale (MCTS), and Reflective Thinking Scale (RTS) were administered as a pretest to both groups of respondents. After participating in sampling method activities over 3 weeks, the respondents were re-examined utilizing the same scales. A semi-structured opinion guide for students was also implemented to garner qualitative data. This form was employed to acquire qualitative data established on the students' research objectives and, therefore, to achieve triangulation by gathering data from respondents leveraging quantitative and qualitative approaches to enhance the findings and achieve more details. The semi-structured guide for the students was utilized to determine the benefits and hindrances of ChatGPT in the activities and to expose underlying characteristics that could influence the quantitative findings."
37,"The process of software development often involves retrieving relevant source code snippets. However, traditional keyword-based search engines may not always be effective in retrieving the most relevant code snippets. To address this issue, various techniques have been proposed in the literature, including machine learning-based approaches that use NLP techniques to match query statements with source code. In this paper, we propose a novel approach that combines Bi-LSTM and GNNs to improve the accuracy of source code retrieval. This methodology involves using a Bi-LSTM and GNNs model to encode query and source code and then applying attention mechanisms to identify relevant code snippets. The detailed methodology for building LLM for source code retrieval using Bi-LSTM is shown in Fig. 1."
38,"An overview of our methodology to evaluate 6 biomedical tasks across 26 datasets in this paper. At first, we construct the prompt for each dataset. Then, we generate the response for each dataset using respective LLMs. Finally, depending on the task, we apply various evaluation techniques."
39,"The success of these models is not solely determined by the quality of their output and their general-purpose nature (refer to Section 2 for more details), but primarily by the way users can interact with them. Fig. 1 elucidates this interaction. A user starts by defining a task. The user then interacts with the model by describing a task (e.g. writing a research paper, generating novel ideas for a market advertising, or debugging a Python code) via a prompt (i.e., a request made to the generative system), which is then sent to the model. The model analyses the prompt and responds with a textual output. Since the user expects a certain result, the result is compared with the actual output. The user is then able to iterate a new prompt to refine the output, thus engaging in a form of conversation with the system until the output does not match the expected result."
40,"Fig. 2 presents the methodological steps for analysing the tasks performed by ChatGPT. First, we collected tweets related to ChatGPT; second, we used NER techniques to identify the tasks that users assigned to ChatGPT; third, we normalised the identified tasks; and finally, we clustered the tasks using BERTopic"
41,"Seppänen et al. (2013) underscored the importance of SSA in driving effective decision-making during emergencies. Drawing from a rich tapestry of literature on SSA in crisis management (Ödlund, 2010, Virrantaus et al., 2009, Waugh and Streib, 2006), they identified three pivotal factors influencing SSA within crisis response entities outlines in Fig. 1. The first, information needs or link content, bifurcates into critical information (vital for SSA development) and action-triggering information (ensuring proactive information dissemination). The second, link type, pertains to the modality of information communication. Trust, the third factor, is instrumental in gauging the quality of the link. Seppänen et al. (2013) concentrate on SSA within crisis agencies, overlooking public SSA. We identify this as a key gap, emphasizing the necessity of SSA between agencies and the public for effective crisis management."
42,"As Fig. 2 shows, this study employs a comprehensive three-tiered methodology to dissect the dynamics of crisis communication on social media platforms. Starting with a thematic analysis of Facebook posts, the research unveils the content and essence of health agencies' communication. Progressing to a narrative analysis of informant interviews, it demystifies the underlying decision-making processes and the broader communication strategy. The final phase, a thematic analysis of Facebook comments, offers a lens into the real-world effectiveness of these strategies and the consequential development of public trust. Collectively, these phases provide highly relevant insights, capturing the intricate interplay between information, communication, and trust in a digital age."
43,"Aligned with prior studies (Pennycook et al., 2020, Velichety and Shrivastava, 2022), we classified information typologies that have been disseminated by health agencies to address misinformation. The authors analysed the posts to identify themes during the systematic analysis informed by Terry et al. (2017) and outlined in Fig. 3. Braun and Clarke (2006) emphasise that a theme captures an important aspect of the data in a patterned manner ?regardless of whether that theme captures the majority experience? (Scharp & Sanders, 2019, p. 1). While recognising a pattern might involve noting frequency, it's more about interpreting meaning than highlighting quantity. Braun and Clarke (2006) note there's no definitive threshold for how much of the data must evidence a theme for it to be considered valid. Essentially, the themes should address the research question, even if not explicitly mentioned in every instance."
44,"The agencies' communication team actively manages and moderates comments to curb misinformation. The comment moderation and filtering process is multi-tiered illustrated in Fig. 4.1.Automated Filtering: Initially, comments undergo automated filtering to catch and remove any that violate basic guidelines or contain profanities.2.Manual Review by Communication Team: Comments that pass the automated filter are then manually reviewed by the communication team. If misinformation appears unintentional and can be addressed, the team responds with a correction.3.Community Self-Moderation: Often, members of the online community themselves address and correct misinformation in comments. In such cases, the communication team may choose not to intervene, allowing the community's self-regulation to take precedence.4.Blocking Persistent Offenders: If a particular user consistently posts misinformation, especially if it seems intentional, the team takes the step of blocking that user to maintain the integrity of the information on the platform."
45,"The ""SSA Dynamics Model"" in Fig. 5 outlines the complex interplay between information dissemination, communication strategies, and trust, essential for countering misinformation and enhancing SSA during crises. The model emphasize the fluidity of trust-building and SSA through ongoing interactions between the public and response agencies."
46,We used a self-developed Python app and collected a random sample of 765 posts and their linked comments published from 1 January to the end of April 2020 on the NSW1 and VIC2 Department of Health Facebook page. Table 2 presents a summary of our initial Facebook dataset.
47,"To investigate official Facebook use against misinformation, we analysed 29 posts that directly addressed COVID-related uncertainties or misinformation. This targeted approach aimed to understand official strategies to counter misinformation during the pandemic. These posts aligned with the four major thematic areas of the COVID-19 infodemic identified by the ?Coronavirus Disease 2019 (COVID-19) Situation Report?85? (WHO, 2020). Table 3 showcases samples of these posts by Australian health agencies."
48,"We conducted nineteen semi-structured interviews, averaging an hour each, with professionals from the health agencies whose Facebook activities we studied. These interviews, held online via Zoom, aimed to deepen our understanding of the topic, gauge the extent of the problem, comprehend the organisational approach to using Facebook against COVID-19 misinformation, and benefit from the experts' tactical knowledge. All interview participants held roles in the public health sector, as shown in Table 4. They were recognised for their exceptional academic and hands-on knowledge in the field, qualifying them as experts. Although our engagement with these experts significantly influenced our interpretation of the study's results, it's essential to clarify that only data from six informants were incorporated into this study. These six (rows i to vi in Table 4) were directly involved in managing and operating public health social media communication and were recognised as online community administrators."
49,"To examine the proposed mediation pathway (H2), we used SPSS PROCESS macro model 4 (Hayes, 2017) with 5000 iterations. Shoppers? decision-making styles (coded as maximisers = 1 and satisficers = 0) served as the independent variable, while perceived content integration capability was the mediator variable, and perceived omnichannel interaction quality was the dependent variable. Results show that shoppers? decision-making styles had a positive effect on perceived content integration capability (ß = 0.600, t = 3.497, p < .001), which in turn positively influenced perceived omnichannel interaction quality (ß = 0.447, t = 6.945, p < .001). In line with H2, the indirect effect (ab) of shoppers? decision-making styles on perceived omnichannel interaction quality through perceived content integration capability was positive and significant. Thus, perceived content integration capability mediated the proposed relationship as the confidence interval excluded zero (ß = 0.268, 95% CI = [0.115, 0.445]), as presented in Fig. 2 and Table 6."
50,"Manipulation checks were examined using a one-way ANOVA test and the MTS measures. Results show that maximisers showed higher mean MTS scores compared to satisficers (Mmaximisers = 6.04, SD = 0.57, Msatisficers = 4.81, SD = 0.77, F[1,127] = 234.681, p < .001). Therefore, the results confirm the effectiveness of the priming task used in Study 3. To assess the moderating effect of omnichannel configuration quality, we applied SPSS PROCESS macro model 5 (Hayes, 2017). Shoppers? decision-making styles (maximisers = 1 and satisficers = 0) were coded as the independent variable, with omnichannel configuration quality serving as the moderator variable, perceived content integration capability serving as the mediator variable and perceived omnichannel interaction quality serving as the dependent variable. Results confirm that the interaction between shoppers? decision-making styles and omnichannel configuration quality had a positive effect on perceived omnichannel interaction quality (ß = 0.288, t = 3.080, p < 0.001). Interestingly, decision-making styles had no effect on omnichannel interaction quality when configuration quality was low (ß = 0.007, p = .962, 95% CI [?0.297, 0.312] for ?1 SD, Mmaximisers = 4.23 Msatisficers = 4.24). However, when omnichannel configuration quality was high, maximisers perceived higher omnichannel interaction quality compared to satisficers (ß = 0.642, p < .001. 95% CI [0.324, 0.959] for  SD, Mmaximisers = 5.13, Msatisficers = 5.77). Therefore, controlling for the mediation effect, results confirm the conditional indirect pathway from shoppers? decision-making styles to perceived omnichannel interaction quality, as zero was excluded from the confidence intervals (ß = 0.143, CI [0.024, 0.315]). The higher the perceived omnichannel configuration quality was, the better the perceived omnichannel interaction quality was for maximisers compared to satisficers, as shown in Fig. 3 and Table 9."
51,"The moderating effect of omnichannel configuration quality was tested using SPSS PROCESS macro model 5, as in Study 3. The results indicate a significant positive interaction between shoppers? decision-making styles and omnichannel configuration quality, positively impacting perceived omnichannel interaction quality (ß = 0.178, t = 2.168, p < 0.01). Notably, decision-making styles had no effect on omnichannel interaction quality when configuration quality was low (ß = ?0.050, p = .708, 95% CI [?0.314, 0.213] for ?1 SD). However, when omnichannel configuration quality was high, maximisers perceived better omnichannel interaction quality compared to satisficers (ß = 0.335, p < .001. 95% CI [0.102, 0.569] for  SD). Moreover, when controlling for the direct effect, the study?s results confirm full mediation. This outcome suggests that the relationship between shoppers? decision-making styles and perceived omnichannel interaction quality is entirely mediated by omnichannel configuration quality. The exclusion of zero from the confidence intervals (ß = 0.220, CI [0.108, 0.347]) substantiates the conditional indirect pathway from shoppers? decision-making styles to perceived omnichannel interaction quality. Thus, the findings of Study 4 confirm that omnichannel configuration quality showed a higher positive influence on maximisers than on satisficers, who reported higher omnichannel interaction quality (see Table 13 and Fig. 4). These findings support external validity of Studies 1, 2 and 3, as they build upon the broader and more general experiences of shoppers, providing a comprehensive understanding of the dynamics at play in digital omnichannel environments."
52,"A total of 120 participants (Mage = 30.98, SDage = 8.14) were recruited by Qualtrics (see Table 4). Participants were randomly assigned to one of the two priming conditions (maximisers = 60 and satisficers = 60). The priming task and shopping task were the same as in Study 1. In this study, participants were asked to shop and select a new footwear product using a real-world omnichannel retailer. Perceived omnichannel interaction quality and MTS were measured as advised in Study 1. Perceived content integration capability was measured using two items adapted from Sun et al. (2020), such as ?I trust my ability to process the information I gather from different channels?. Participants responded to all items using a seven-point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree)."
53,"A one-way ANOVA was used for the manipulation check using the MTS measures. Results show that maximisers scored higher in the MTS compared to satisficers (Mmaximisers = 5.95, SD = 0.42, Msatisficers = 4.30, SD = 0.52, F[1,118] = 359.280, p < 0.001). Therefore, the significant results of the manipulation checks confirm the success of the priming method used in this study. To examine the proposed mediation pathway (H2), we used SPSS PROCESS macro model 4 (Hayes, 2017) with 5000 iterations. Shoppers? decision-making styles (coded as maximisers = 1 and satisficers = 0) served as the independent variable, while perceived content integration capability was the mediator variable, and perceived omnichannel interaction quality was the dependent variable. Results show that shoppers? decision-making styles had a positive effect on perceived content integration capability (ß = 0.600, t = 3.497, p < .001), which in turn positively influenced perceived omnichannel interaction quality (ß = 0.447, t = 6.945, p < .001). In line with H2, the indirect effect (ab) of shoppers? decision-making styles on perceived omnichannel interaction quality through perceived content integration capability was positive and significant. Thus, perceived content integration capability mediated the proposed relationship as the confidence interval excluded zero (ß = 0.268, 95% CI = [0.115, 0.445]), as presented in Fig. 2 and Table 6."
54,"Our study used stratified random sampling of 950 companies, establishing equal probability that any firm could be selected at any step during sampling. The companies were contacted by phone and e-mail to explain the study?s purpose and offer them the opportunity to receive the results once the study was finished. Analyzing the results in aggregate and promising confidentiality of responses increased the response rate (36.10%, 343 valid responses (see Table 4)) and reduced the possibility of desirability bias."
55,"A comparative analysis of two groups of respondents was performed, the companies that returned the completed survey within three weeks of receiving it and the companies that returned the survey only after follow-up reminders. This test assumes that late respondents are similar to non-respondents (Armstrong & Overton, 1977). Comparing the characteristics of the firms with late vs. early respondents (Table 5) indicated no significant differences between early and late respondents."
56,"To analyse the data, we followed a thematic approach to identifying relevant patterns and themes (Braun & Clarke, 2006). A theoretical coding process was carried out, using an abductive approach based on the framework of reference (Dubois & Gadde, 2002). The coding process was rather iterative, going back and forth between the theory and the collected data (Gioia et al., 2013). This method provides consistent and rigorous analysis across three iterative stages of data analysis, following the approach commonly undertaken in case studies (Yin, 2014). With the support of NVIVO software, data were coded to identify prominent themes that helped us to glean important insights from the phenomenon under observation. This process was conducted through a within-case analysis for each SME, followed by a cross-case analysis with the continuous identification of how SME resource orchestration supports AI implementation as the company makes its digital transformation (Beverland & Lindgreen, 2010). Fig. 1 depicts the data analysis process developed in three stages (Gioia et al., 2013)."
57,"DOI posits that an individual?s decision to adopt an innovation is a process consisting of five sequential stages (Rogers, 1983) (see Fig. 2). The DOI?s five stages have parallels to Prochaska?s transtheoretical model of behavior change (Prochaska et al., 1992) and McGuire?s hierarchy of effects (McGuire, 1989), supporting the assumption that these stages do exist (Rogers, 2003)."
58,"Table 3 shows the participating companies. Company A is an industrial technology provider with approximately 250 employees from various sectors, with a focus on engineering and software development. Their product relates to circular economy and recycling, and according to their own presentation, environmental sustainability is already embedded in their culture. Company B is an industrial software provider with approximately 400 employees who are mainly software developers due to their core B2B software product. Company B is making various sustainability efforts, such as offsetting its emissions, using green power, and switching to electric vehicles, but its employees have not been as involved in its sustainability strategy. Company C is a glass manufacturer with approximately 150 employees. The majority of these employees work in the production halls and often come from abroad or have little education. In administration, about 30 employees work in research and development, procurement, sales, marketing and process optimization. The company attaches great importance to the recycling and reuse of its glass products and environmentally compatible production, but is particularly interested in new approaches to creatively involve its employees and motivate them to adopt sustainable behavior. Finally, Company D is a web design agency with 30 employees, mainly software developers and UX designers. Environmental sustainability has gained importance for them as a key criterion of responsible and sustainable digitalization. However, the company is not yet engaged in efforts for sustainability."
59,Details of the data collection and analysis process and participants of both studies are provided in the respective sections. Fig. 4 illustrates how the results of both studies in an integrated research process lead to an in-depth understanding of the challenges employees encounter when introducing and using green IS in the workplace.
60,"Consumer satisfaction is formed through a cognitive process that links prior expectations to perceived performance and confirms or disconfirms expectations concerning the actual performance (Morgeson, 2012). In digital services, specifically in the adoption of AI, performance expectancy is an important predictor of customer emotions, as higher levels of performance expectancy lead to increased usage intentions (Choi et al., 2011, Gursoy et al., 2019b). Thus, this research proposes that performance expectancy is the underlying mechanism explaining the effect of AI vs. autonomous choices on satisfaction. In particular, when AI makes the decision, there is a conflict between autonomy and technology that decreases consumers? performance expectations and, as a result, decreases satisfaction. Fig 1.Hypothesis 3 Performance expectancy mediates the effect of AI vs. autonomous choices on satisfaction."
61,Survey Items for Satisfaction.
62,Survey Items for Performance Expectancy.
63,"In total, we conducted 21 interviews. The data collection reached saturation when the interview data showed no new information (Jacobson & Harrison, 2022). Table 3 describes the participants? demographics. We elaborated an interview guide with 13 questions inspired by the theoretical model tested in the previous studies. The average time for each interview was 20 min. Interviews were conducted face-to-face and mediated by videoconferencing technology (Zoom) in Portuguese by the last author, who transcribed, anonymized, and translated the scripts into English."
64,"Together, these theoretical foundations guide our research by providing a framework for understanding the dynamics of AI adoption, shareholder perceptions, and the organizational business contexts. They allow us to delve into the complex interplay between actors, roles, and business conditions within the context of B2B marketing, ultimately contributing to a deeper comprehension of the impact of AI adoption on shareholder reactions. This study theorizes that the focal firm's AI implementation for B2B marketing can be seen as an action delivered by the firm (i.e., performer) to present its IT proficiency and future business opportunities to its shareholders (i.e., audience). Following this logic, adopting the social actor perspective to explain how AI implementation for B2B marketing presents their underlying capabilities or competencies to shareholders, affecting shareholders' perception and influencing the firm's market value. In addition, this study further argues that the shareholders' reaction to the implementation of AI for B2B marketing relies on different business conditions, such as the characteristics of firms' industry type (i.e., industry dynamism) and operational environments (i.e., customer complexity). In this way, it offers an insightful lens for situating firms in a broader social landscape and investigating the relationships between their AI-enabled B2B marketing and shareholder reaction in different environments. The conceptual model of this study is illustrated in Fig. 1."
65,"After the data collection, panel A of Table 1 presents the characteristics of the sample firms. For instance, the mean of the sample firms' net income and total assets are $3292.382 million and $ 139,398 million, respectively. At the same time, the mean of sales, total liabilities and employee number of the sample firms are $ 24,361.830 million, $ 11,933.500 million, and 66,487, respectively. Panel B and C of Table 1 show the distribution of the sample firms by industry (via its two-digit SIC codes) and year. It illustrates that AI-enabled B2B marketing activities are most popular in the services industry (i.e., SIC 70?89), which takes 56.18% of the total sample. The distribution panel shows that most of the firms only started using AI for their B2B marketing practices during 2016?2020, with 2017 as the peak year."
66,"After performing the binary logistic regression model, we acquire the propensity score, which indicates the probability of implementing AI-enabled B2B marketing initiatives for all firms included in the model. Then, we use a nearest-neighbor one-on-one matching method to identify the control firms. To improve the matching quality, we set a predetermined caliper of 0.02, which measures the absolute distance between the control and treatment firms' propensity scores (Ye et al., 2020). As shown in Model 1 (pre-match model) of Table 2, the number of firms included in the regression model is 1423, consisting of 89 treatment firms collected from Factiva, 1334 potential control firms with the same 4-digit SIC codes as the treatment firms. 87 out of the 89 treatment firms are matched successfully through the above-mentioned matching procedures and criteria. Therefore, the total sample size for this research reached 174, including 87 treatment firms and 87 matched control firms. Model 1 shows that the coefficients of marketing efficiency are negatively significant, while the coefficients of firm size, liquidity and R&D intensity are positively significant. This result indicates that firms with lower marketing efficiency, larger firm size, higher firm liquidity and greater R&D intensity tend to be more likely to employ AI technology for their B2B marketing practices. Also, we further check the matching quality by comparing the results of pre-match and post-match logistic regressions. As shown in Model 2 (post-match model) in Table 2, there are no statistically significant predictors, thus indicating a satisfying matching quality is achieved."
67,"Table 4 shows the descriptive statistics, including means and standard deviations, and the correlations of all variables in Eq. (1). "
68,"Table 5 presents the results of cross-sectional regression analysis with CAR over the event window (-1, 0) as the dependent variable. More specifically, model 1 is the basic model and only includes all control variables. In model 2, the direct effect of AI-enabled B2B marketing is introduced. The interactions between AI-enabled B2B marketing and industry dynamism and customer complexity are sequentially included in models 3 and 4. The F-tests (p < 0.05) show that these four models are significant, with adjusted R-squared values between 0.071 and 0.141. To test for multicollinearity, we calculate the full model's variance inflation factor (VIF). The maximum and mean values of VIF are 1.93 and 1.40 (much lower than the threshold of 10), thus suggesting that multicollinearity is not a concern in our models (Kennedy, 1998)."
69,"To facilitate our focus group discussion, we meticulously framed our research objectives to provide clear guidance on the purpose of our study, as outlined in Table 6. These predefined objectives served as a framework for our inquiries during the workshop. The discussion revolved around the examination of the three hypotheses central to our research. For Hypothesis 1, participants engaged in conversations addressing questions such as ""Could you share your thoughts on why shareholders generally react positively to AI implementation in B2B marketing?"" and ""What specific benefits or expectations do you associate with AI adoption in B2B marketing?"" Hypothesis 2 prompted discussions with questions such as ""From your perspective, how do industry dynamics and concerns affect your perception of AI adoption in B2B marketing?"" and ""Do you believe that firms in more dynamic industries have unique considerations when it comes to AI implementation? Please elaborate."" Hypothesis 3 guided participants to respond to queries like ""What challenges or complexities do you perceive in firms with more complex customer bases when implementing AI in B2B marketing?"" and ""Could you provide examples of situations where shareholders may have concerns about AI adoption in such firms?""."
70,"However, when firms take actions to recover from a service failure, such as chatbots resolving consumer issues, consumers are likely to focus on efforts made by an able, benevolent, and integral chatbot to resolve the issue. When consumers perceive chatbots to be trustworthy by their benevolence and integrity towards resolving the problem, consumers? intent to penalize the company may decline, resulting in less spread of nWOM. Hence, we hypothesize: H4a Perceived ability of chatbots encourages consumers to forgive firms for service failures. H4b Perceived benevolence of chatbots encourages consumers to forgive firms for service failures. H4c Perceived integrity of chatbots encourages consumers to forgive firms for service failures. H5a: Perceived ability of chatbots encourages consumers to reduce nWOM against firms in case of service failures.H5b: Perceived benevolence of chatbots encourages consumers to reduce nWOM against firms in case of service failures.H5c : Perceived integrity of chatbots encourages consumers to reduce nWOM against firms in case of service failures. In the above five sets of hypotheses, we discussed how chatbots' traits influence their perceived trustworthiness and how this trustworthiness influences consumers' willingness to forgive service providers and spread less nWOM. Corollary, the trustworthiness dimensions mediate chatbot traits and consumer response to service failure relationships. Hence, we hypothesize: H6 : Perceived ability, benevolence, and integrity mediate chatbot traits (i.e., privacy concerns, anthropomorphism, and perceived empathy) and customer outcome (i.e., forgiveness and nWOM) relationship. Fig. 1 presents the conceptual model."
71,"Following a similar strategy as Study 1, in Study 2, we obtained 508 [Females= 260] filled-in questionnaires from UK-based participants [Median Age= 33.78 years; Median Income= £32,820]. In both studies, our final sample was skewed toward younger adults compared to the UK population. Table 1 presents the sample demographics of studies 1 and 2."
72,"In Table 2, we present the descriptive statistics of the variables in the study. There are positive and statistically significant correlations between the antecedents (i.e., perceived privacy concern, anthropomorphic chatbots, and perceived empathy) and the mediators [i.e., perceived ability (rprivacy concerns, perceived ability=-0.21, p < 0.001; ranthropomorphic chatbots, perceived ability =0.25, p < 0.001; rperceived empathy, perceived ability=0.34, p < 0.001), perceived benevolence (rprivacy concerns, perceived benevolence=-0.06, p < 0.10; ranthropomorphic chatbots, perceived benevolence =0.28, p < 0.001; rperceived empathy, perceived beenvolence=0.24, p < 0.001), and perceived integrity (rprivacy concern, perceived intergrity=-0.05, p < 0.10; ranthropomorphic chatbots, perceived integrity =0.33, p < 0.001; rperceived empathy, perceived integrity=0.28, p < 0.001)."
73,We tested hypotheses 1?6 using a structural equation model. We employed MPLUS 8.0 to test the structural model. Table 4 presents the results of the hypothesis tests.
74,"We predicted through the sixth hypothesis that perceived ability, perceived benevolence, and perceived integrity act as mediators between the antecedents (perceived privacy concerns, anthropomorphism, and perceived empathy) and customer outcome (i.e., consumer forgiveness and nWOM). To test the mediation models, we employed Hayes? (2018) procedure and further employed a bootstrapping re-sample value of 5000. In Table 5, we present the results of the mediation analyses."
75,"Next, from Table 6, we can observe that the correlations of the antecedents and the mediators are positive and statistically significant, and the correlations of the mediators and the two outcome variables: consumer forgiveness and nWOM, are in the expected directions. These encouraging findings provide preliminary evidence in support of the study hypotheses."
76,"Next, we tested the measurement model using MPLUS 8.0. The Study 2 measurement model reported a good fit (Chi-square/df= 2.83; RMSEA= 0.042; CFI= 0.955; TLI= 0.963). We also assessed the constructs? convergent and discriminant validities (Fornell & Larcker, 1981) that we report in Table 7."
77,"Finally, to test hypothesis six, we employed a strategy similar to Study 1, i.e., Hayes?s (2018) mediation procedure with a bootstrapping resample value 5000. We present the results of the mediation analyses in Table 8. The estimated path coefficient for the indirect effect of perceived privacy concerns on consumer forgiveness through perceived ability (Column 1 of Table 8) was statistically significant (? = -0.0089; LCI=-0.0122; UCI=-0.0056). Also, the estimated path coefficient for the indirect effect of perceived privacy concerns on nWOM through perceived ability (Column 2 of Table 8) was statistically significant (? = 0.0092; LCI=0.0044; UCI=0.0140). From Column 3 of Table 8, we observe that the estimated path coefficients for the indirect effects of chatbot anthropomorphism through perceived ability (? = 0.0125; LCI=0.0062; UCI=0.0188), perceived benevolence (? = 0.0208; LCI=0.0124; UCI=0.0292), and perceived integrity (? = 0.0298; LCI=0.0204; UCI=0.0392) on consumer forgiveness were statistically significant. Also, in Column 4 of Table 8, we observe that the estimated path coefficients for the indirect effects of anthropomorphism through perceived ability (? = -0.0127; LCI=-0.0178; UCI=-0.0076), perceived benevolence (? = -0.0175; LCI=-0.0246; UCI=-0.0104), and perceived integrity (? = -0.0352; LCI=-0.0481; UCI=-0.0223) on nWOM were statistically significant. Finally, in Column 5 of Table 8, we observe that the estimated path coefficients for the indirect effects of perceived empathy through perceived ability (? = 0.0193; LCI=0.0107; UCI=0.0279), perceived benevolence (? = 0.0261; LCI=0.0169; UCI=0.0353), and perceived integrity (? = 0.0292; LCI=0.0190; UCI=0.0394) on consumer forgiveness were statistically significant. Similarly, in Column 6 of Table 8, we observe that the estimated path coefficients for the indirect effects of perceived empathy through perceived ability (? = -0.0175; LCI=-0.0269; UCI=-0.0081), perceived benevolence (? = -0.0220; LCI=-0.0329; UCI=-0.0111), and perceived integrity (? = -0.0352; LCI=-0.0494; UCI=-0.0210) on nWOM were statistically significant."
78,"To detect a CV?s presence in SEM research, researchers should seriously consider explaining why a new CV should have a relationship with the dependent variables and/or a correlation with the independent variables. Researchers should consider using our proposed procedures under two conditions: with and without theoretical support. Fig. 1 illustrates the proposed process for identifying a new CV without theoretical support. Researchers should consider testing the measurement model. If the correlations between a new CV and the other variables are high, the researchers have not been able to separate out the CV?s effects on the other variables; thus, a CV has not been identified. If the correlations between a new CV and the other variables are low, researchers need to statistically compare structural models with CVs and those without CVs. If the path coefficient and R-squared value show significant differences between such models, then a new CV has not been identified. However, if the path coefficient and R-squared value do not indicate significant differences between such models, a CV has been identified."
79,"Fig. 2 illustrates the proposed process for identifying a new CV when there is theoretical support for it. When there is a theory that suggests including the CVs, they should be included in the initial research model. Theoretical support ensures that CVs will be in the measurement model. If the correlations between a new CV and the other variables are high, researchers should correlate the new CV with the independent variables and conduct an SEM analysis with and without the new CV. Otherwise, it is unnecessary to add this correlation, but the SEM analysis should still be performed. If the path coefficient and R-squared value with the new CV differ significantly from those without it, researchers should present the results without the new CV and provide post hoc alternative explanations (i.e., modify the supporting theoretical reasons). If the path coefficient and R-squared value with the new CV do not differ significantly from those without it, researchers should present the results with the new CV."
80,"Data analysis followed Lee and Baskerville?s (2003) generalization framework, with iterations between four types of generalization between empirical and theoretical statements: generalizing from empirical data to empirical descriptions (EE), generalizing from empirical descriptions to theoretical statements (ET), generalizing from theory to descriptions of other settings (TE), and generalizing from concepts to theory (TT). While data analysis activities were highly iterative and intertwined with data collection activities (Klein and Myers, 1999, Walsham, 2006), our general approach was to progress from rich empirical descriptions to theoretical insights. In the first analysis step (EE), we coded the transcripts in MAXQDA. Codes ? abstract descriptions of intersubjective realities ? can originate from theory and data (Walsham, 2006). Drawing on Gioia et al.?s (2013) framework, we started with data-driven open coding to generate tentative empirical themes from the interview transcripts at the level of phrases or paragraphs. In a second step (ET), we compared the emerging empirical themes with existing theories to integrate them into higher-level conceptual categories via axial coding. Here, we conceptualized relationships between emergent categories by focusing on how categories are related and how they influence each other (Gioia et al., 2013). In a third step (TE), we further refined and linked these conceptual categories by revisiting the data, resulting in many iterations of a conceptual model with four aggregate dimensions and two manifestations each: people (innovator or facilitator), structure (informal or formal), task (idea generation or idea evaluation), and technology (representation or collaboration). Fig. 1 illustrates our coding process and further quotations illustrating the codes are used throughout Section 4."
81,"Our empirical analysis reveals that creativity emerged when WealthTech?s ISD team members empathetically interacted, aligning their distinctive creative styles and contributions within formal and informal structural settings, while engaging in idea generation and evaluation using technologies for representation and collaboration. This synthesis of insights enriches the STS perspective on creativity emergence in ISD teams, as visually illustrated by the conceptual model in Fig. 2 and elaborated in detail below the model. The conceptual model and the detailed findings from WealthTech afford opportunities to advance theory on the phenomenon of interest: the emergence of creativity in ISD practices. In doing that, we follow Whetten's (1989) guidelines to explicate what concepts and relationships we consider to understand and explain the phenomenon of interest, how we articulate the relationships between the concepts as propositions, and why we choose to emphasize specific relationships and articulations."
82,"We studied in detail the creative practices of ISD teams at the WealthTech headquarters in Switzerland and various subsidiaries worldwide. The style of involvement was that of an embedded researcher having in-depth access to the research site (Walsham, 2006), including meeting rooms, in-house workstations, and the intranet. For nearly three years, from February 2013 to December 2015, the lead author spent several days weekly onsite in the Swiss offices, having access to an in-house workstation and intranet. There, the author conducted 32 interviews to get an in-depth understanding of creativity in ISD teams from a participant?s perspective (Walsham, 2006). In addition, the author attended meetings to do participant observations, in which a researcher participates actively in discussions, as opposed to acting as a passive outside observer (Walsham, 2006). In April 2014, the author also visited the UK offices to conduct 30 interviews and observe how ISD teams collaborated virtually. Table 1 provides an overview."
83,"We conducted interviews at WealthTech with 62 different ISD team members, ranging from 19 to 100 min. We took care to select a broad sample of participants to capture diverse perspectives, relying on a combination of theoretical sampling (seeking out different ISD team members), purposive sampling (seeking out diverse views), and snowballing (following referrals from other participants). We audio recorded and transcribed all 62 interviews except two, where the participants did not give permission to record, in which case we took notes. Of the 62 interviews, 39 were in German, in which case we translated the quotations into English. The remaining 23 interviews were in English. Table 2 provides an overview."
84,The hypotheses are visualized in the research model in Fig. 1.
85,"For the scenario-based experiment (Study 2), we selected and adapted four conversations with KIM representing a 2 × 2 factorial design: two conversations were completed by KIM, while this was not the case for the other two. In the unsuccessful conversations, the chatbot responded twice with textual elements to restart the conversation. In addition, two conversations were short in terms of the length of the conversation itself and KIM?s responses presenting 2 correct recipes, while the other two conversations were long with a lengthy introduction by KIM, and a detailed textual presentation displaying 5?6 recipes (see Figure A1 in the appendix). The experiment was supported by a German market research institute and took place with 627 participants in February 2022. We asked the respondents to read one sample conversation with KIM thoroughly and to evaluate KIM in terms of the conversation ability score and the three item scales (perceived naturalness of KIM, performance of the chat, satisfaction with the interaction). To check manipulation, they were asked to indicate whether KIM made good recipe suggestions and answered straight to the point ? that is, short and concise. The participants were randomly assigned to one of the four scenarios (see Table 3)."
86,"Both samples consist of more females than males. In the usability study (Study 1), the percentage of females was higher (60.2%) than males (39.0%, 0.8% diverse). In the experiment (Study 2), the gender distribution was more balanced with 51.0% females and 47.7% males (1.3% diverse). The respondents are young, most often 20?25 years (Study 1: 67.5%, Study 2: 62.3%) or 26?30 (Study 1: 16.3%, Study 2: 17.3%) old. The share of respondents under 20 was more than twice the size in Study 2 (14.7% vs. 5.7%) reflecting the aim to include a younger age group. Over time, the claim of familiarity with the term chatbot increased from about two-thirds to three-quarters of the respondents, more often males. In both samples, less than half of the respondents already had experience of chatbot usage. The percentage was higher compared to an earlier study on the acceptance of chatbots (Rese et al., 2020). About two-thirds of the sample in Study 1 use Facebook Messenger, and about half of the sample Study 2. However, the chatbot, KIM, is largely unfamiliar to respondents, in particular in Study 1. With regard to diet preferences, more than half of the respondents follow no special regimen, whilst about a quarter to a third are vegetarians, among them significantly greater numbers of females. Female respondents search for recipes more frequently on the internet in both samples. In general, the time spent searching for a recipe was short. In Study 1, half of the respondents take up to 5 min and another third up to 10 min, whilst in Study 2 about a quarter need longer with up to 20 min and more (see Table 4). While Study 1 relied on a student sample, participants of Study 2 included students (28.2%), employees (43.5%), self-employed (5.9%), pupils (9.3%), and trainees (8.9%)."
87,"With regard to the dialogues, there were no significant differences between female and male participants. Most participants completed the recipe search, corresponding more or less to their food preferences. Often KIM asked two questions about recipe criteria (48.0%), including the ingredients. In 3.3% of the dialogues, KIM referred to all three criteria, whilst not mentioning them explicitly in 21.1% of the conversations. On average, KIM suggested 6 recipes to the user, of which a small number were inappropriate (18.2%). The search was quick, with KIM posting more messages and using more words. KIM?s messages usually included one emoji per dialogue. KIM had to restart the conversation once in almost every dialogue. This had a significantly greater frequency in dialogues with female users, who also received a lower percentage of correct recipes. In about a quarter of the cases, KIM failed to employ a correct greeting, either repeating the same message or omitting the greeting. In the evaluation by external reviewers, KIM received a mean conversational ability score above 0 = poor, machine-like, but still under 50. This score was termed as ?good conversationalist? and was achieved by 2 of the 6 chatbots evaluated (Shah et al., 2016). The users rated KIM slightly better on satisfaction, naturalness, and performance expectancy with a score above the average value of 4 (see Table 5; for a description of the variables, see Table A1 in the appendix)."
88,"For hypotheses on task fulfilment, we relied on a comparison of users successfully and unsuccessfully completing the search for recipes with KIM in Study 1. We compared the mean values of task and conversation elements as well as evaluation criteria for the two groups. Since the group failing to complete the task was small, a non-parametric test ? the Mann-Whitney U Test ? was used (see Table 6)."
89,"With regard to the perceived conversational ability of KIM, we transformed the independent variables with a box-cox transformation to compensate for the non-normal distribution of the data. We employed stepwise regression analysis to integrate into the model not only the independent variables concerning the hypotheses but also all available independent variables from Table A1. We chose this more exploratory approach because of the scarcity of research results on the human?chatbot conversation. We used the four internal and external dependent evaluation variables. Similar to the results from the mean comparison, the external evaluation with the conversational ability score and, to some extent, user satisfaction were shown to be suitable and provided higher R2. Taking the external evaluation ? the conversational ability score ? into account, H1b and H7b were confirmed, and a positive effect of a high percentage of correct recipe suggestions (0.212, p = 0.003) and a negative effect of the number of conversation restarts (-0.506, p = 0.000) were established. For user satisfaction, the positive effect of a high percentage of correct recipe suggestions (0.174, p = 0.043) was also found. Other hypotheses on the perceived conversational ability of KIM could not be proven. The internal evaluation criteria established negative effects for the number of messages, the number of words from the user, the number of greetings by KIM, and the number of wrong recipe suggestions. While the presence of a correct greeting did not show the hypothesized positive effect (H8b), additional greetings had a negative effect (Table 7)."
90,"This research has tried to identify a mix of instrumental (task-oriented) and social (small-talk) conversational elements that influence users? perception of a chatbot and their conversation with it. To investigate conversational ability, we used the task-based and text-based chatbot, KIM, by MAGGI Kochstudio to perform the task of finding an accurate recipe and to undertake an analysis of 123 unstructured chat records (Study 1). An overview of previous studies analysing text-based chatbot conversations supported the identification of relevant features and evaluation variables from usability research (Frøkjær et al., 2000). We focused on several task-based and text-based variables as well as two characteristics of conversational ability that potentially influence the success of KIM in this area. In addition, we relied on an external evaluation by three reviewers based on the conversational ability score (Shah et al., 2016) and a subjective user evaluation following the conversation with KIM using item scales describing user satisfaction (Hornbæk, 2006, Shawar and Atwel, 2007, Söderlund, 2022, Söderlund and Oikarinen, 2021). A scenario-based experiment (Study 2) was used to gain further insights into user evaluation of selected conversational elements (see Table 8). The usability study (Study 1) provided insights into several task-based and text features as well as characteristics of conversational ability, and it was used as a source for example dialogues. While chatbot usage could be monitored, the implementation period was rather long with data collected in person. The scenario-based experiment (Study 2) was quickly implemented but was restricted to a few features and did not include real experience with KIM."
91,H1a.Involvement relates positively to the consumption level of tourist engagement in SMNs. H1b.Involvement relates positively to the contribution level of tourist engagement in SMNs. H1c.Involvement relates positively to the creation level of tourist engagement in SMNs. H2a Perceived anonymity relates positively to the consumption level of tourist engagement in SMNs.H2b Perceived anonymity relates positively to the contribution level of tourist engagement in SMNs. H2c Perceived anonymity relates positively to the creation level of tourist engagement in SMNs. H3a Consumption engagement in SMNs relates positively to cold BRQ. H3b Consumption engagement in SMNs relates positively to hot BRQ. H6a Cold BRQ on SMNs relates positively to trip decision-making. H6b Hot BRQ on SMNs relates positively to trip decision-making.H6a Cold BRQ on SMNs relates positively to trip decision-making. H6b Hot BRQ on SMNs relates positively to trip decision-making H4a Contribution engagement in SMNs relates positively to cold BRQ. H4b Contribution engagement in SMNs relates positively to hot BRQ. H5a :Creation engagement in SMNs relates positively to cold BRQ. H5b Creation engagement in SMNs relates positively to hot BRQ. H7a Cold BRQ on SMNs relates positively to cross-buying decisions.H7b Hot BRQ on SMNs relates positively to cross-buying decisions. Fig. 1 shows the theoretical model.
92,"The findings further demonstrated that the effect of involvement on creation (|?ß| = 0.146, p < 0.05), the effect of creation on hot BRQ (|?ß| = 0.203, p < 0.05), and the effect of cold BRQ on cross-buying decisions (|?ß| = 0.154, p < 0.05), were higher for group 2. The effect of hot BRQ on trip decision-making (|?ß| = 0.134, p < 0.05) was higher for group 1. Interestingly, no differences were found pertaining to the associations between involvement and consumption, brand involvement and contribution, consumption and cold BRQ, consumption and hot BRQ, contribution and cold BRQ, contribution and hot BRQ, creation and cold BRQ, cold BRQ and trip decision-making, and hot BRQ and cross-buying. Fig. 2, Fig. 3 present the findings of the structural models of groups 1 and 2."
93,"The findings further demonstrated that the effect of involvement on creation (|?ß| = 0.146, p < 0.05), the effect of creation on hot BRQ (|?ß| = 0.203, p < 0.05), and the effect of cold BRQ on cross-buying decisions (|?ß| = 0.154, p < 0.05), were higher for group 2. The effect of hot BRQ on trip decision-making (|?ß| = 0.134, p < 0.05) was higher for group 1. Interestingly, no differences were found pertaining to the associations between involvement and consumption, brand involvement and contribution, consumption and cold BRQ, consumption and hot BRQ, contribution and cold BRQ, contribution and hot BRQ, creation and cold BRQ, cold BRQ and trip decision-making, and hot BRQ and cross-buying. Fig. 2, Fig. 3 present the findings of the structural models of groups 1 and 2."
94,"In the U.S sample, 50.2% of the respondents were female, 43.4% in the age group of 26?41 years, 40.9% had earned a four-year degree, and 84.3% indicated that the trip was domestic. In the Chinese sample, 56.1% were female, 36.9% in the age group of 26?41, 65.6% had earned a four-year degree, and 97.3% indicated that the trip was domestic. Table 1 shows the demographic profiles of participants in both countries."
95,"Establishing validity and reliability of the measures is essential prior to testing the structural model (Hair Jr et al., 2017; Henseler et al., 2009). We assessed the measurement models for the pooled data and then group 1 (U.S.) and group 2 (China). As displayed in Table 3, constructs were internally consistent since Cronbach?s alpha values were > the 0.70 threshold (Nunnally & Bernstein, 1994). The convergent validity criterion was verified, with average variance extracted (AVE) values all above 0.50 (Fornell & Bookstein, 1982)."
96,"As shown in Table 4, discriminant validity was assessed by heterotrait-monotrait criterion (HTMT). The results indicated good discriminant validity as all HTMT values were below the threshold value of 0.90 (Voorhees et al., 2016). A multicollinearity test was employed via the value inflation factor (VIF). VIF values were less than the 5.0 threshold. Hence, multicollinearity was not an issue. The Harman's single-factor test was utilized to control the threat of common method variance. The results of the exploratory factor analysis (unrotated) showed that no single construct explained more than 44.9% of the observed variance. Common method variance did not seem a concern in this study since it was below 50?60% (Fuller et al., 2016)."
97,"In the second stage, the structural models for group 1 and group 2 were gauged using SmartPLS 3 (Ringle et al., 2015). To assess the structural model, the R² of the endogenous variables was computed for the model?s explanatory power (Hair Jr et al., 2017). As reported in Table 5, consumption achieved R² value of 38.6% (pooled data), 32% (group 1), and 36.2% (group 2); contribution 44.3% (pooled), 38.5% (group 1), and 42.5% (group 2); creation 40.7% (pooled), 29.3% (group 1), and 43.8% (group 2); cold BRQ 27.7% (pooled), 17.9% (group 1), and 49.8% (group 2); hot BRQ 51.1% (pooled), 39.4% (group 1), and 49% (group 2); trip decision-making 23.9% (pooled), 20.2% (group 1), and 13.9% (group 2); and cross-buying 48.9% (pooled), 37.2% (group 1), and 54.9% (group 2). To assess the strength of the hypothesized relations, a bootstrapping test based on 5000 subsamples was performed. As hypothesized, brand involvement had a positive effect on ?consumption,? ?contribution? and ?creation? (H1a, ß = 0.612, p < 0.05; H1b, ß = 0.645, p < 0.05; H1c, ß = 0.603, p < 0.05, respectively). Accordingly, H1a, H1b, and H1c were verified. Additionally, perceived anonymity had a positive effect on ?consumption,? ?contribution? and ?creation? (H2a, ß = 0.038, p < 0.05; H2b, ß = 0.081, p < 0.05; H2c, ß = 0.119, p < 0.05, respectively). Hence, H2a, H2b, and H2c were confirmed. As hypothesized, consumption had a positive effect on both (H3a) cold and (H3b) hot BRQ (ß = 0.272, p < 0.05; ß = 0.221, p < 0.05, respectively). Contribution had a positive effect on both (H4a) cold and (H4b) hot BRQ (ß = 0.129, p < 0.05; ß = 0.281, p < 0.05, respectively). Additionally, creation had a positive effect on both (H5a) cold and (H5b) hot BRQ (ß = 0.170, p < 0.05; ß = 0.269, p < 0.05, respectively). Therefore, H3a, H3b, H4a, H4b, H5a, and H5b were verified. Cold BRQ had a positive effect on trip-decision making (H6a) and cross-buying (H6b) (ß = 0.057, p < 0.05; ß = 0.411, p < 0.05, respectively). Finally, hot BRQ had a significant positive effect on trip-decision making (H7a) and cross-buying (H7b) (ß = 0.448, p < 0.05; ß = 0.350, p < 0.05, respectively). Thus, H6a, H6b, H7a, and H7b were confirmed."
98,"A multigroup analysis (MGA) was performed to compare the differences between the model for group 1 with the model for group 2. Differences in the path coefficients between the two data sets are shown in Table 6. In the first step, we tested the measurement invariance of composite models (MICOM) (Henseler et al., 2016). Thus, we followed the three-step process. These steps were computing the ?configural invariance,? ?compositional invariance? and ?the equality of means and variances.? . As displayed in Table 5, the full measurement invariance was partially achieved in the comparison between group 1 and group 2 as step (1) was fully established, and step (2) and step (3) were partially established. The following step was related to the employment of a multigroup analysis to test the path coefficients in both groups. Parametric and nonparametric procedures (PLS-MGA and permutation) were applied to investigate group differences (Hair et al., 2018). As shown in Table 5, the findings indicated that significant differences existed in path coefficients between group 1 and group 2. In the relationship between perceived anonymity and ?consumption,? ?contribution? and ?creation? the effects were stronger for group 1 (|?ß| = 0.113, p < 0.05, |?ß| = 0.106, p < 0.05, |?ß| = 0.088, p < 0.05, respectively)."
99,"H1: The accuracy in predicting the communication effectiveness of FtF is higher than the accuracy in predicting the effectiveness of email. H1a : The difference in the accuracy of effectiveness predictions between FtF and email is moderated by the closeness between the requester and the target individual. In Study 1, we address RQA and test the related Hypotheses 1 and 1a as seen in the conceptual model displayed in Fig. 1. In this study we try to replicate previous research and reestablish that FtF is more effective than email for approaching potential resource providers. "
100," In Study 2, we build on the findings of Study 1 and empirically investigate the effect of perceived effectiveness and other (irrational) factors on media choice (RQM and Hypotheses 2, 3, and 4) as seen in the conceptual model displayed in Fig. 3.  H2 : Perceived differences in the effectiveness of the medium (FtF vs. email) has an impact on the media selection decision. H2a The impact stated in H2 is higher for strangers than for friends. H3 : Perceived differences in the awkwardness and embarrassment of the medium (FtF vs. email) has an impact on the media selection decision. H3a The impact stated in H3 is higher for strangers than for friends. H4 : Perceived differences in the convenience of the medium (FtF vs. email) has an impact on the media selection decision. H4a The impact stated in H4 is higher for strangers than for friends."
101,"The two goals of Study1 were to contrast the accuracy of effectiveness prediction (the difference between actual and predicted effectiveness) for email and FtF requests (H1) and to verify whether this overestimation is moderated by the closeness level between the requesters and the targets (H1a). To test these hypotheses, a new variable (Prediction accuracy) was calculated by subtracting Actual effectiveness from Predicted effectiveness. The data was submitted to a univariate ANOVA1 with factors of Media (email vs. FtF ? Between-subjects) and Closeness (Strangers vs. Friends ? Between-subjects), and Prediction accuracy as the DV. We looked at the main effect of Media which was highly significant [F (1, 109) = 22.105, p < 0.001,  = 0.169] showing that the magnitude of inaccuracy was significantly larger in email compared to FtF when Closeness levels are collapsed (H1 supported). However, neither the interaction [F (1, 109) = 0.321, p = 0.572,   = 0.003] nor the main effect of Closeness (F (1, 109) = 0.028, p = 0.867,   <0.001) was significant. Hence, the pattern of overestimation is not different among Friends and Strangers groups, that is the closeness between requesters and targets does not improve the accuracy of predicted effectiveness (H1a is not supported). We also asked participants to report their own feeling about the task (Table 1). We looked at the ANOVAs to see if there are any differences across conditions in terms of participants? feelings about the task. As expected, we found that the Friends group perceive requesting to be easier and feel less awkward approaching a target individual compared to the Strangers group. However, the two groups do not predict any difference in the embarrassment after being rejected. No significant interaction emerged between Media and Closeness nor any main effect for Media was observed (see Table 1)."
102,"We asked our participants one open-ended and several Likert scale questions (Appendix E) to justify their choice of medium. A content analysis of the open-ended question revealed that our Likert questions covered all the reasons provided by participants. Then, a factor analysis was conducted on the reasoning data leading to two factors and two single items (Table 2)."
103,"To test hypotheses 2 through 4, an omnibus logistic regression was conducted (Table 3). All the variables were mean-centred and logistic regression was conducted using the bootstrap method with 5000 resampling. Although we have already observed that a substantial portion of requesters chose email, which is the suboptimal medium according to Study1, the expectation was that their reasons would be different when approaching Strangers vs. Friends. Hence, in the following analysis, we included the interaction terms between Closeness and each of the reasons in Table 2. As seen in Table 3, two significant interactions emerged supporting H3a and H4a but not H2a. Please note that the main effects of factors involved in these significant interaction terms (i.e., Closeness, AwkEmbar, and Convenience) are not interpretable at this stage."
104,"However, we can conclude that Effectiveness does not play a significant role in media selection decision (H2 was not supported) as further confirmed in Table 4. To unpack the significant interaction terms of Closeness, data was split on Closeness levels (Friends vs. Strangers) and separate logistic regressions (bootstrap method with 5000 resampling) were performed for each subset of data (Table 4) with the decision outcome as the dependent variable and reasoning items explained above as independent variables. Although no predictor reached the significance level for the Friends group (H3 and H4 not supported for Friends), the biggest coefficient emerged for Effectiveness. On the other hand, email was more attractive for the Strangers group due to less awkwardness, less embarrassment, and the convenience of an email request (H3 and H4 supported for Strangers). A minority within this group picked the more effective medium for the right reason as shown by the marginally significant coefficient of Effectiveness."
105,"First, we contrasted the media decision by Male and Female participants by running a logistic regression (bootstrap with 5000 resampling) analysis with Closeness, Gender, and the interaction term of the two variables as the IVs and, the media decision outcome as the dependent variable. As shown in Table G1, no significant interaction effect emerged nor any main effect of Gender on media selection was detected."
106,"Second, we looked at the gender effect of reasoning by running an ANOVA analysis which was explained in the main text. Lastly, we excluded the female participants from the data set and ran another logistic regression. The significant interaction terms in Table 3 were also significant in this analysis."
107,"We asked our participants to contrast requesters? perspectives when they are approached FtF and via email. Two separate factor analyses were conducted on FtF and email perspective-taking measures with similar emerging factors as shown in Table A1. Cronbach?s alphas are reported separately for FtF questions and email questions. Separate repeated measure ANOVAs, Closeness (Cls vs. Str ? between-subject) × perspective-taking index (FtF vs. eml ? within-subject), were conducted for each of the three emerged indices and none of them resulted in a significant interaction between Closeness and the perspective-taking index. It suggests that Participants in both Closeness conditions predicted the same magnitude of change in targets? perspectives when moving from FtF to email. As shown in the last column of Table H1, participants, regardless of their closeness level to the target (main effect of media), acknowledged differences between FtF and email requests for some of the indices and Single measures. We tested whether requesters consider any of the significant indices/measures when predicting the effectiveness of each medium. Index 3 was positively correlated with requesters? prediction of both FtF effectiveness (r(106)= 0.401, p < .0001) and Email effectiveness (r(107)= 0.398, p < 0.001). Index 1 was correlated with Email prediction only (r(106)= 0.246, p = 0.011) but it wasn?t strong and vanished when data was split on the Closeness factor. Single1 was correlated both with Email (r(48)= 0.366, p = 0.010) and FtF (r(48)= 0.475, p = <0.001) predictions but Single2 was only correlated with FtF predictions (r(48)= 0.539, p = <0.001). These significant correlations offer possible causes that make requesters to perceive FtF as a more effective channel for reaching out to targets. However, as shown in Study 1, requesters? perception is not even nearly close to the real extent of the difference between the two channels."
108,"H1 : There is a negative relationship between the transparency in SHAs? decision and recommendation algorithms and perceived creepiness. H2 : There is a negative relationship between the tangibility of SHAs and perceived creepiness. H3 : Perceived creepiness has a positive relationship with resistance to SHAs. H4 : Resistance has a negative relationship with the intention to use SHAs. H5 : Resistance has a negative relationship with perceived usefulness. H6 : Resistance has a negative relationship with perceived ease of use. H7 : Perceived usefulness has a positive relationship with intention to use SHAs.  H8: Perceived ease of use has a positive relationship with intention to use SHAs.Fig. 1 summarizes the derived conceptual model of this study, which will be tested in our subsequent studies."
109,"Fig. 3 displays the final consensus mental model. The final consensus mental model consisted of six constructs: four inhibitors and two enablers. As displayed in Fig. 3, participants? responses to SHAs were shaped by the interplay of the enablers perceived usefulness and perceived ease of use and the inhibitors perceived creepiness, privacy concerns, perceived loss of control, and perceived immaturity of technology. While most of these constructs have been extensively studied in research on technology acceptance and resistance, perceived creepiness emerged as a novel inhibitor for the context of SHAs (see e.g., Chouk and Mani, 2019, Mani and Chouk, 2019, Lee, 2020, or Pal et al., 2021 who examine the inhibiting role of constructs analogous to privacy concerns, loss of control, and perceived immaturity of technology in the context of smart technology). Moreover, the centrality measures indicate that perceived creepiness has the highest centrality of all the identified inhibitors. Thus, these findings offer initial evidence and a glimpse into the possibility that individuals who decide not to utilize SHAs may be influenced by a certain sense of creepiness with them."
110,"Next, we selected those items with high and distinct loadings on this one factor (= .80). In addition, we sorted out items that did not clearly load on the feeling of creepiness factor. The final item set included seven items (see Table 1)."
111,"Next, we examined the effect of perceived creepiness on resistance. This analysis showed that the creepier participants perceived the SHA, the higher their resistance to using the SHA, supporting H3 (OLS regression including all conditions: B = 0.57, SE =.03, p < .001, d = 1.11). Considering the downstream effects of resistance, we find that all regression paths were significant at p < .001, except for the relationship between ease of use and usage intention (p = .316). In support of H4, resistance exerted a direct negative effect on usage intentions (B = -0.22, SE =.05) and negatively affected perceived usefulness (H5; B = -0.60, SE =.03) and ease of use (H6; B = -0.20, SE =.03). In line with H7, perceived usefulness exerted a positive effect on usage intention (B = 0.71, SE =.03). However, since ease of use did not significantly affect usage intention (B = -0.03, SE =.03), H8 had to be rejected. Finally, we tested the significance of the indirect effects of transparency and tangibility on usage intention through the proposed mediators ? creepiness, resistance, perceived usefulness, and perceived ease of use. Except for the path through perceived ease of use, all indirect effects reached significance at 95% CI. Table 2 provides a detailed overview of all results."
112,"Anchoring on the challenge-hindrance stressor framework, we apply the JD-R model to propose hypotheses development in the research model (Fig. 1). Specifically, we discuss that THS has a positive effect on workarounds, whereas TCS has a negative effect on workarounds. We also propose that support structures and trait resilience weaken the positive effect of THS on workarounds and strengthen the negative effects of TCS on workarounds."
113,"The results of the hypotheses testing about the research model were conducted by AMOS 28.0. We mean-centered the independent, moderator, and dependent variables to further decrease the possibility of multicollinearity. Fig. 2 showed the results of the main structural model. The R2 value for the dependent variable (workarounds) is 0.39. THS positively influenced workarounds (ß = 0.27, p < 0.01), whereas TCS negatively influenced workarounds (ß = -0.16, p < 0.05), supporting H1."
114,"Table 2 showed the demographic information. Specifically, we ranked respondents by the interval between responses to the two rounds of questionnaires, then we compared the demographic information of the early and late 25% of respondents (Armstrong and Overton, 1977, Sivo et al., 2006). The results (Table 2) showed no significant differences in gender, age, education, and usage experience. Therefore, in line with prior literature (Benlian, 2020, Ke et al., 2021, Laumer et al., 2017), we find that, even with a relatively low response rate, the comparative analysis revealed that nonresponse bias was not problematic for the present research."
115,"The longitudinal two-wave design with a time lag between the independent and dependent variables ensured that common method bias (CMB) was alleviated (Sykes, 2015). We also applied the Harmon single-factor test (Podsakoff et al., 2003). We found 10 factors with eigenvalues over 1, and the first factor accounted for 26.75%, lower than the threshold of 40%. In addition, we conducted a fit comparison between the one-factor model and the measurement model (Flynn et al., 2010). The one-factor model fit (?2/df=11231.312/560 =20.06, RMSEA=0.226, SRMR=0.332, CFI=0.182, TLI=0.131) indicated a worse result than the fit of our measurement model (?2/df=1849.975/539 =3.43, RMSEA=0.0072, SRMR=0.051, CFI=0.900, TLI=0.918). Finally, we adopted the marker variable technique to further examine CMB (Malhotra et al., 2006). In particular, the selected marker variable should be unrelated to any other variable in the measurement model (Acharya et al., 2022). We chose the vision of continuity as the marker variable. It refers to the organizational vision of maintaining continuity amidst internal and external changes (Venus et al., 2019), which was an irrelevant variable with two items. Table 3 showed that the correlations between the marker variable and other latent variables were irrelevant. Then, we utilized the lowest positive correlation (r = 0.02) to adjust the correlations among constructs. The results indicated that the difference between unadjusted correlations and adjusted correlations was not significant. Hence, CMB might not be a concern."
116,"We strategically selected 20 respondents from four subsidiaries of the electric company, all of whom had completed both phases of the questionnaires (as indicated in Table 4). Due to the constraints imposed by the COVID-19 pandemic, we conducted telephone interviews as the most practical and safe means of data collection. Each interview, on average, had a duration of approximately 10 min. Within the timeframe, we allocated around 7 min for specific questions tailored to each participant, while the remaining time was dedicated to providing a comprehensive introduction to the research background. While the interviews were relatively short, they were designed to be concise and focused, ensuring that participants? responses provided in-depth insights into their experiences and perspectives related to ES use, particularly in the context of TDS and support structures."
117,"Fig. 1 provides an overview of the selection process and outcomes at each stage, specifically the identification, screening, eligibility, and inclusion of articles in the study. From the identification step, 5470 article records were retrieved. We removed 1177 duplicated records. In the next step, we review 4293 records, following an in-depth evaluation of 371 full-text articles. Finally, 120 articles met the inclusion criteria and were included in the final analysis. Following that, an extraction of detailed data from the included articles was carried out (see Appendix, Table A1)."
118,"For the thematic analysis, we sought to construct a high-level framework as a template to organize emerging themes. Based on a hybrid view, integration of variance, and process thinking recommendations (Burton-Jones et al., 2015, de Guinea and Webster, 2017, Langley, 2007, Maitlis and Lawrence, 2007), for a systematic examination of the health data breaches, we developed a model (see Fig. 2). This model distinguishes between facilitators (the conditions that clear the paths for potential data breaches), data breach types, and impacts (an effect or influence that a data breach incident has on individuals and organizations/businesses). In developing our model and identifying antecedents, facilitators, and impacts, we were inspired by recent methodological developments in the IS field (Burton-Jones et al., 2015, de Guinea and Webster, 2017) and the notion of hybrid models. In our model, facilitators and data breach types (enablers) denote a process approach (see, for example, Maitlis and Lawrence, 2007), while studying impacts (consequences) pertains to variance approaches. As stated by de Guinea and Webster (2017, p. 159), a hybrid model is an ideal candidate for ?investigating events that trigger certain changes in states or outcomes?."
119,Graphical Abstract
120, Fig. 1 depicts the proposed R2M2 model.
121,"According to the CKC model shown in Fig. 2 (NIST, 2014b), an attacker initially carries out reconnaissance to assess the potential benefits in terms of ransom (Oz et al., 2022) that they could gain from targeting an organization (NIST, 2022, Souppaya and Scarfone, 2013). Larger organizations, although better equipped with security technologies (Angst et al., 2017, Dalton et al., 1980, McLeod and Dolezel, 2018, Schlackl et al., 2022), would be severely impacted by a ransomware attack, as their IT-enabled supply chain may come to a standstill. To maintain seamless operations, a large organization will prefer to pay the ransom at times (Carroll & Stater, 2008). Therefore, this study proposes the following hypotheses: H1a: Larger organizations are more prone to ransomware attacks."
122,"This study used data from the University of Queensland, Australia, to explore the cyber resilience of organizations (Tsen et al., 2020). The dataset consisted of various features of 1473 organizations belonging to the critical and non-critical sectors in Australia, the USA, Canada, and Japan and their associated breaches due to ransomware attacks from 2004 to 2022. The dataset included firms with varying digital intensity, organizational size, network segmentation, EVSS, and CSR. We referred to Statista for data on the average financial losses (Lj) (Statista, 2022) from 2004 to 2022, which are derived from cyber-crime cases reported by the Internet Crime Complaint Center (IC3), which is a part of the FBI. Table 5 summarizes the statistics of the data."
123,Table 6 shows the relationship between the dependent and independent variables.
124,"We used generalized linear models such as LR (Son et al., 2020) in the RRA module to test the hypotheses. Based on the chi-squared test, the goodness of fit of the model was revealed to be significant (p < 0.001) with a small deviance of 66.68. Table 8 reports the parameters of the M1 (LR) model that were significant at the 1% level. Substituting the values from Table 8 into Equation (1), Eq. (3) is determined, as follows (3) Table 8 and Eq. (3) indicate that larger organizations are 6.786 times more likely to face R attacks than NR attacks (p = 0.001), thereby supporting hypothesis H1a. Moreover, if organizations belong to a critical industry, their chances of facing ransomware attacks increase by 5.229 times compared to NR attacks (p = 0.001). This finding supports hypothesis H1b. Similarly, for each unit increase in digital intensity, the odds of R attacks are 13.573 times greater than those of NR attacks (p = 0.001). This result aligns with hypothesis H2a. However, if the network is segmented, the probability of occurrence of R attacks decreases to 0.042 times that of NR attacks (p = 0.001), thereby supporting hypotheses H2b. Table 8 also indicates that for each unit increase in vulnerabilities in the organizational environment, the odds of ransomware attacks increase by 2.734 times compared to other cyberattacks (p = 0.098). This finding supports hypothesis H3. In contrast, if information security governance is properly implemented in an organization, for an increase in every cybersecurity role, the probability of ransomware attacks decreases to 0.421 times that of non-ransomware attacks (p = 0.100), thereby supporting hypothesis H4."
125,"Next, in the RRA module, we used the M1, M2, and M3 models to classify attacks as R or NR. The performances of the three models were measured and compared on the test dataset using the accuracy, precision, recall, and F1-score, as shown in Table 9. It is evident from Table 9 that M1 was better than M2 and M3 because its accuracy, precision, recall, and F1-score were better than those of the other two models. Hence, this study further elaborates on the results of M1 (the LR model). Figs. 3(a) and 3(b) show that M1 could correctly classify or predict 27 out of 29 R and 293 out of 314 NR data points of the test dataset."
126,"Based on the method by Nickerson et al. (2013), combined with the steps of selected sub-methods, we compiled an integrated research design (cf. Fig. 1). While the research method is based on Nickerson et al. (2013), its application is inspired by Cledou et al. (2018) as they developed their taxonomy. This section outlines methodological steps, and section 4 describes the application of these steps, following the phases listed in Fig. 1: Planning & Data Collection, Taxonomy Construction, and Taxonomy Evaluation.2"
127,"As described in section 3, we informed the taxonomy development from both a theoretical perspective and an empirical perspective. The Taxonomy Construction itself was performed in four iterations (cf. Fig. 2). The first iteration followed the conceptual-to-empirical approach. That is, we derived dimensions and characteristics from the 23 identified articles and examined 30 of the 210 digital innovation projects from the database to assess their empirical evidence."
128,"In the final iteration, the objective ending conditions described in Table 1 are fulfilled, so the final taxonomy is complete, empirically relevant, stable, and unambiguous. However, within the iterations of the Taxonomy Construction process, we made several adjustments to the taxonomy (i.e., additions, deletions, and relabeling of dimensions and characteristics). Changes along the development process were made when the research team concluded that the taxonomy?s explanatory character, conciseness, and robustness would increase given a particular adjustment. We also revised the taxonomy when analyzing the projects revealed that certain dimensions and/or characteristics had little empirical relevance, are invariant across the sample, or are not mutually exclusive (i.e., characteristics were hardly distinguishable from one another). The progression of adjustments in these iterations is shown in Fig. 4."
129,"While the OIPT from Galbraith (1973) offers a powerful mechanistic model to derive strategies to reduce uncertainty, the original theory had several limitations, which is why it has been further advanced over the past decades by several scholars. According to Haußmann et al. (2012), major limitations were, among others, the lack of considering individual information restrictions among stakeholders (e.g., Zmud, 1979), interpersonal characteristics (e.g., Bensaou & Venkatraman, 1995; Burke et al., 2001), inter-organizational relations (e.g., Fairbank et al., 2006). Consequently, subsequent studies (Burke et al., 2001; cf., Cooper & Wolfe, 2005; Daft & Lengel, 1986) successively advance the original theoretical framework from Galbraith, 1973, Galbraith, 1974 and incorporate external environment, interdepartmental relations, and technology as sources of uncertainty and equivocality. One relevant resulting advancement is the recognition that task uncertainty is not the only constraint to be reduced but also equivocality, which is defined as ?ambiguity, the existence of multiple and conflicting interpretations about an organizational situation? (Daft & Lengel, 1986, p. 556). Overall, Haußmann et al. (2012) propose an adapted framework of the OIPT, considering the limitations of previous scholars and incorporating helpful advancements (see Fig. 1)."
130,"Based on the management factors and the management practices, we iteratively developed the AIAMA model. Since our first model concept considers the AI management factors as the main source of task uncertainty and equivocality leading to information processing issues among stakeholders (i.e., guiding concept 1), we rely on the management factors from Fig. 2 as our model?s main building blocks. Building the basis of our AIAMA model, we adopted the management factors and transformed them into our model (i.e., describing what to manage), as presented in Fig. 4. Each factor consists of several dynamic constructs that can evolve via system changes. In the model, the constructs serve as abstract concepts inductively summarizing observations from reality into researchable objects and explaining the factors of AI application management (Bhattacherjee, 2012, Cronbach and Meehl, 1955)."
131,Coding scheme based on literature analysis
132,Coding scheme based on interview analysis
133,We illustrate the AIAMA model application by the management of outdated medical practices. Outdated medical practices may result from updated medical guidelines by healthcare authorities. We exemplify the model application by the case of a hospital having to react to changing medical guidelines as it affects their AI application.
134,"Fig. 1 shows our research framework. Our study used SDT as the main theory with which to develop a research framework and formulate hypotheses. Past online game research has used SDT to explain player satisfaction (Sepehr & Head, 2018) and loyalty (Teng et al., 2022a). Our study is novel because no previous studies have examined the impacts of our three proposed game design elements. Our study is valuable because our findings can guide game makers to effectively boost player satisfaction by selecting the best elements to improve."
135,"We used LISREL v.8.53 software, which is a commonly used tool (Shareef et al., 2017, Shareef et al., 2020, Tsai and Bagozzi, 2014), to perform structural equation modeling analysis, which we then used to test our hypotheses. We set the significance at the typical.05 level. Fig. 2 shows the analytical results. Most of the study hypotheses are supported, with some exceptions. First, game creatability is not related to competence satisfaction, thus H1a is not supported. The reason for this may be that game creatability empowers players to unleash their creative potential but may also influence players? focus during gameplay. Specifically, players may prioritize showcasing their creativity rather than honing their playing skills, which is the key theoretical factor in competence satisfaction (Reer et al., 2022). Moreover, game creatability induces players to develop their own storylines. From a practical standpoint, different story endings are not always satisfactory. Players sometimes fail to reach their goal, which is unlikely to result in much competence satisfaction (Deci & Ryan, 2000). These reasons would explain the nonsignificant relationship between game creatability and competence satisfaction."
136,"We further implemented two improvements. First, we omitted the skill level from the list of control variables because, in our theoretical model (Fig. 2), it was the only control variable that was significantly related to usage but not to continuance. Its removal simplifies the list of control variables. Second, we added a third item to assess game creatability: ?I play this online game to create my own characters,? and a third item to assess game achievability: ?I am confident I can achieve the goals of making in-game progress in this online game.? Both scales showed a higher level of reliability (a = .82 and .87), providing future scholars with three-item scales to assess the two concepts. Fig. 3 illustrates the results."
137,"Consistent with the literature (Islam et al., 2022, Sun et al., 2020, Sun et al., 2021), Table 2 lists the loadings and cross-loadings of the measurement items. The items loaded in our assumed factors provide preliminary evidence of our data validity."
138,"All the Cronbach?s a values are.76 or larger, indicating sufficient reliability (Nunnally & Bernstein, 1994). All the composite reliability (CR) values are.81 or larger, and all the average variance extracted (AVE) values are.55 or larger. These results indicate acceptable reliability (Bagozzi & Yi, 1988). As shown in Appendix A, all the indicator loadings are.65 or higher, suggesting good convergent validity (Hair et al., 1998). As shown in Table 3, all the positive square roots of the AVE values exceed the associated correlations, indicating discriminant validity (Fornell & Larcker, 1981). To offer enhanced evidence of discriminant validity, we tested and found that all the 95% confidence intervals of the correlations are smaller than all the positive square roots of the AVE values. Psychometric properties may include reliability, validity, and model fit performance. Our measurement model has sufficiently good performance in model fit, i.e., CFI= .97, IFI= .97, NNFI= .96, SRMR= .05 (Bagozzi, 2010)."
139,"We obtained 546 complete responses through a two-wave data collection process. The data indicate that most of our participants were male (86.7%). This proportion is similar to that of the local player population, in which 83% of players are male (GNN, 2016). Most participants were aged = 30 years (82.1%), had a college/university level education or higher (91.4%), and had an income = NT $600,000.00 (79.5%). Most participants had played the focal game for = 5 years (91.1%), showed weekly gameplay hours of < 21 h (87.7%), and had a skill level of gold or below (91.7%). Table 4 shows the demographic profile of the participants. However, the total numbers shown in Table 4 are not always equal to the total collected sample size due to some missing values in the profile data."
140,"As shown in Fig. 2 and Table 5, our structural model explains significant variances in the endogenous constructs: 57% in competence satisfaction, 49% in autonomy satisfaction, 26% in relatedness satisfaction, 49% in game continuance, and 11% in game usage. We suppose that 11% may be adequate, as game usage may be easily affected by schoolwork or workplace and family responsibilities. Moreover, the well-known phenomenon of the intentionsingle bondbehavior gap predicts a high discount in transforming intention to behavior (Fennis et al., 2011)."
141,"We performed the usual bootstrapping process, i.e., 5000 resamplings at the typical significance level of.05 (Nusair et al., 2024). Although not all the path coefficients in our structural model have significant coefficients, the bootstrapping results indicate that all the mediations are significant. This is reasonable, as the bootstrapping method is designed to test the interactions of the path coefficients. Therefore, a single large path coefficient can result in a significant interaction among path coefficients. All the mediation coefficients are significant, justifying the importance of the chosen mediators in our model. Moreover, our model shows that most (but not all) paths have significant coefficients, giving game makers useful insights into game achievability and game immersibility, but not focusing on game creatability."
142,"After the measurement model?s fit and validity were established, we moved to the path testing of our research model. The latent constructs were maintained in Amos instead of creating imputed sum constructs. The variances explained (R2) by the model in the dependent variables were 40 % for perceived organizational value of XR, 46 % for expected employee resistance to XR, and 44 % for organizational XR adoption intention. The overall hypothesis testing results can be seen in Fig. 2."
143,"The analysis proceeded by (1) identifying and grouping relevant quotes relating to the examined variables and their relationships, (2) deriving descriptive informant-driven first-level concepts based on these quotes, and (3) developing more theoretical second-order themes (i.e., adoption manifestations and conditions affecting the relationships between the variables) from these initial descriptive concepts. The outcome of this analysis is presented in the form of a data structure (Fig. 3). The findings are also finally integrated with the earlier quantitative results (Fig. 4). Further evidence and quotes from the interviews justifying the development of the themes can be found in the data tables (see Appendix D)."
144,"In total, we received 213 complete responses, which were screened in multiple steps, resulting in 206 valid responses. We excluded 1 response from South Africa, as it was from outside of Europe, and removed 6 responses for inattentive responding (< 0.5 sd in the responses) or for responding at a speed that would be impossible to do attentively (< 5 min). The sample was then examined for missing data. We found two missing values for USE_3 (see Appendix C, Table C1), which were imputed with the median value of the item. Finally, we screened the remaining responses that failed the attention-trap question. We found sufficient variance in their answers, and the respondents took a sufficiently long enough time (> 10 min) to complete the survey. Demographic information about the respondents (n = 206) can be found in Table 1. As can be seen from the table, the survey responses were collected from top management (61), middle management (65), lower management (51), and experts (29). Table 2 depicts information about the companies of the respondents."
145,"In total, we received 213 complete responses, which were screened in multiple steps, resulting in 206 valid responses. We excluded 1 response from South Africa, as it was from outside of Europe, and removed 6 responses for inattentive responding (< 0.5 sd in the responses) or for responding at a speed that would be impossible to do attentively (< 5 min). The sample was then examined for missing data. We found two missing values for USE_3 (see Appendix C, Table C1), which were imputed with the median value of the item. Finally, we screened the remaining responses that failed the attention-trap question. We found sufficient variance in their answers, and the respondents took a sufficiently long enough time (> 10 min) to complete the survey. Demographic information about the respondents (n = 206) can be found in Table 1. As can be seen from the table, the survey responses were collected from top management (61), middle management (65), lower management (51), and experts (29). Table 2 depicts information about the companies of the respondents."
146,"We then proceeded with the CFA by evaluating the fully correlated measurement model. As the first step in assessing the indicator and construct validities, we examined the standardized item loadings for the constructs. All loadings were statistically significant (p < 0.001) and above the recommended 0.707 threshold (Fornell & Larcker, 1981), except for one RESI item, which had a loading of 0.689. Thus, this item was dropped from subsequent analyses. The constructs, items, and their means, standard deviations, and standardized factor loadings are presented in Appendix C (Table C1). Next, we used the Master Validity plug-in of Gaskin et al. (2019) to analyze the discriminant validity of our measurement model (Table 3). According to Fornell and Larcker (1981), the composite reliability (CR) value of all constructs should be above 0.7, the average variance extracted (AVE) should be above 0.5 and larger than the maximum shared variance (MSV), and the square root of each AVE should be larger than all other correlations with the other variables. The CR, AVE, MSV, and square root of the AVE (bolded in diagonal) are reported in Table 3. As can be seen, our data fit all the aforementioned criteria, indicating sufficient convergent and discriminant validity for our model."
147,"We opted to evaluate the model fit for the measurement and path models using the CFI, SRMR, and RMSEA measures. This is in line with the recommendations of Hair et al. (2014), who suggested that model fit should be evaluated with at least one absolute fit measure (e.g., SRMR and RMSEA) and one incremental fit index (e.g., CFI). The suggested cutoffs for these fit measures are = 0.95 for CFI, = 0.08 for SRMR, and = 0.08 for RMSEA, along with > 0.05 for its PClose (Hair et al., 2014). As shown in Table 4, the measurement model?s values were excellent for all of these fit indices. The model?s normed chi-square (?2/df) was 1.515, which falls within the suggested range of 1?3 (Hair et al., 2014). The chi-square test was statistically significant (p < 0.05), indicating poor fit with the data; however, this is common with complex models and larger sample sizes (Schermelleh-Engel et al., 2003). Moreover, Hair et al. (2014) recommend that this measure should not be examined independently but in the context of other model fit measures. As all the other model fit indices were excellent, we can conclude that the measurement model fit the data well. After we moved on to the hypothesis-testing phase, we also evaluated the path model?s fit. Again, the measures were still excellent except for CFI (0.946), which was still close to excellent fit, but within acceptable range (= 0.9) nonetheless."
148,"Four out of twelve of the paths were significant at the p < 0.001 level, two at the p < 0.01 level, and one at the p < 0.05 level. Seven out of the twelve hypotheses were thus supported (see Table 5). For the statistically significant paths, the effect directions (positive or negative) were as hypothesized in the research model (Fig. 1). Thus, the research model had overall empirical support."
149,"Besides testing for the direct effects of each antecedent, we also carried out additional mediation analysis by testing for the indirect effects of the six TOE-based antecedents on organizational XR adoption intention via the perceived organizational value of XR and expected employee resistance to XR constructs. In addition, we tested whether the effect of expected employee resistance to XR on organizational XR adoption intention was mediated via the perceived organizational value of XR. This analysis was carried out using the latent mediation estimand and the Indirect Effects plugin created by Gaskin et al. (2020). The statistically significant paths are presented in Table 6. Mimetic pressure?s effect on organizational XR adoption intention was strongly mediated via the perceived organizational value of XR (ß = 0.206; p < 0.001). Organizational support (ß = 0.088; p < 0.01), employee technology use skills (ß = 0.069; p < 0.05), and trialability (ß = 0.047; p < 0.05) had a positive mediated effect on organizational XR adoption intention via expected employee resistance to XR. Other indirect effects were statistically insignificant (p > 0.05)."
150,"We can differentiate between potentialities (encompassing both risks and utilities) seen as hypotheticals, and their actual outcomes (harms and benefits). Numerous aspects underpin both categories (refer to Fig. 1). Pinning down causality from potentialities to outcomes is notoriously challenging, as causality can never be proven, but only rejected (Pearl, 2009) (for reasons similar to those widely accepted for hypothesis-testing (Popper, 2002)). Not every risk results in harm, and not every harm traces back to a known risk. Our study, aiming to assess the frequency of ?good? and ?bad? recommendations from algorithms, primarily addresses the potentialities of risks and utilities presented on the left-side of Fig. 1."
151,"Table 1 displays socially accepted risk profiles (for sources, see S.I.2. Comparison data). In food safety, the U.S. Food and Drug Administration (FDA) maintains a Food Defect Levels Handbook, which specifies that it accepts around 7% of defect samples (mainly mold and insect-infestations). About 15% of U.S. citizens contract foodborne illnesses annually, while severe harm is much less common (some 3000 die each year). The U.S. Food Safety and Inspection Service (FSIS) accepts 7.5% of salmonella-positive chicken carcasses and ground beef samples, with harm levels below 0.5% and around 420 U.S. deaths annually. General consumer goods have a different profile. Tracking the number of faulty products sold per year, the U.S. Dept. of Commerce only recalls 0.1%. Their accumulation and frequent use hurts 2%- 4% of U.S. citizens annually. In between these extremes is the risk profile of sports. In soccer, 1 in 32 ball possessions leads to a potentially dangerous foul (3.1%), and 1.5% of U.S. players end up injured. Another extreme is the risk profile of cigarettes. While each cigarette is risky and severe harm levels are notably higher (some 10% die from lung cancer or cardiovascular disease), it is surprising to many that these trackable risks are not life-threatening to some 90% of smokers."
152,"As our analysis aims at bridging measures from computer science, information science, medicine, and the psychological and social sciences, we prioritized expected values and simple conditional probabilities over higher-order meta-analytic statistics (Higgins et al., 2019, Petticrew and Roberts, 2008, Uman, 2011). We still achieve the meta-analytic goal of systematically synthesizing independent studies to calculate an overall effect (Egger and Smith, 1997, Shorten and Shorten, 2013). Table 2 presents the simple framework that conditions algorithmic recommendation output on different kinds of input. While all included studies (N = 151) report the percentage of ?bad? recommendations (first column: X%, Y%, or Z%), we only obtain data on ?good? recommendations for 62 studies (see Table 3). This means that we can distinguish between ?bad? and ?not bad? recommendations for all 151 audits (which is what we will do for most of our analyses), and distinguish between ?good?, ?other/neutral?, and ?bad? for a subgroup of studies (see section ?Recommending utility content?)."
153,"As our analysis aims at bridging measures from computer science, information science, medicine, and the psychological and social sciences, we prioritized expected values and simple conditional probabilities over higher-order meta-analytic statistics (Higgins et al., 2019, Petticrew and Roberts, 2008, Uman, 2011). We still achieve the meta-analytic goal of systematically synthesizing independent studies to calculate an overall effect (Egger and Smith, 1997, Shorten and Shorten, 2013). Table 2 presents the simple framework that conditions algorithmic recommendation output on different kinds of input. While all included studies (N = 151) report the percentage of ?bad? recommendations (first column: X%, Y%, or Z%), we only obtain data on ?good? recommendations for 62 studies (see Table 3). This means that we can distinguish between ?bad? and ?not bad? recommendations for all 151 audits (which is what we will do for most of our analyses), and distinguish between ?good?, ?other/neutral?, and ?bad? for a subgroup of studies (see section ?Recommending utility content?)."
154,"To date, the agency theory has been mostly applied to explain the intentions and motivations for BCT adoption in the supply chain, and consequently, has been categorized as a theory in the antecedents group (Zhu et al., 2022). Treiblmaier (2018) postulated BCT-induced transformations in the principal-agent relationship as follows (see Fig. 1). Without blockchain (Fig. 1, left), the agent is the only one which has access to information from external sources, such as customers, suppliers, or partners. The agent, guided by its own self-interests, processes this information, and then acts on behalf of the principal. This asymmetry of information requires the principal's trust in the agent. By integrating the blockchain into this relationship (Fig. 1, right), information flows become transparent and accessible to the principal, reducing the principal's required trust in the agent. Additionally, smart contracts can mitigate issues of information asymmetry through better control of contract execution and supervision (Chang & Chen, 2020). As a result, the interactions and relations among supply chain participants may be improved."
155,"We can argue that BCT, by virtue of its underlying characteristics, can reduce the asymmetry of information as well as opportunistic behaviors of agents. This result is in line with prior works in the research stream on BCT in SCM (Grosse et al., 2021, Treiblmaier, 2018, Zhu et al., 2022) as well as the one related to BCT adoption in IS (Upadhyay, 2020). However, as our findings do not concur with the blockchain-induced transformations postulated by Treiblmaier (2018), especially when it comes to trust (see Fig. 2), we initiate a discussion about the impacts on principal-agent relationships. Our study indicates that BCT only minimize the need for trust of the principal towards the agents. This result is consistent with Brookbanks and Parry (2022), who highlighted that BCT does not remove the importance of trust in established supply chains. This can be explained by the complexity of relationships between supply chain members, which implies that agents' behaviors cannot be fully automatized. In line with Tönnissen and Teuteberg (2020), our results do not lead to any examples of disintermediation as a whole, but key activities of agents have been disintermediated. This suggests that BCT are more complements than substitutes of inter-organizational trust in established supply chains. Moreover, the need for trust in SCM could have shape the extensive use of permissioned blockchains (Vadgama & Tasca, 2021), which place trust in known participants of the network rather than into the technology itself. This brings to the fore the trustless myth of blockchain (Meyers & Keymolen, 2023). Paradoxically, we note that the control mechanisms inherent in BCT can decrease the agent's trust in the principal (Fig. 2). We explain in the following this finding by connecting the unique features of BCT to information asymmetry and opportunistic behaviors."
156,"First, we found 920 articles from the three databases based on search keywords, and after extracting only those from journals and conference papers, we were left with 563 articles. Second, we removed the duplicated studies, and researchers reviewed each journal, and conference proceedings that were recognized as potentially relevant and eliminated the others, therefore, 442 articles were removed. At the end of the next phase, the papers that did not fit the criteria were extracted, and 68 articles were obtained. Based on the PRISMA statement, Fig. 1 describes the methodology used to conduct this systematic review. PRISMA stands for ""preferred reporting items for systematic literature review and meta-analysis"" (Moher et al., 2009)."
157,"This results in a CSD loop, as illustrated in Fig. 1, where individuals engage with the mobile device to distract themselves from the real world or unwelcome emotional states, but where the act of using the mobile device can cause further negative outcomes (e.g. anxiety). However, increased appetite for distraction is insufficiently explained by rationalist cognitive processing theories commonly applied by researchers explaining drivers, process mechanisms, and outcomes."
158,"From the initial 880 articles, Covidence removed duplicates (n = 273), after which two of the researchers screened the abstracts of the remaining articles (n = 607) to assess suitability. During this process the abstracts were examined based on the eligibility criteria as outlined in Fig. 2. From this process 101 articles were determined as eligible from the abstract and these articles were read in full by two researchers. The full text review showed 77 articles did not meet the eligibility criteria, resulting in the final sample of 23 articles (see Fig. 3)."
159,"To assist in better understanding this phenomenon, we draw on Cognitive-Affective Personality System (CAPS) theory (Mischel & Shoda, 1995), as a unifying theory to clarify our conjectures and develop research propositions that are encapsulated in our theoretical CSD framework (Fig. 5). In this framework we outline that the dynamic input of both external environmental stimuli and internally generated psychological states act as the key antecedents of CSD which lead to behavioral and psychological consequences for the customer. Socio-cultural and individual factors are embedded to condition these impacts. Although presented separately in our framework, the environmental stimuli and psychological states are considered interdependent components of a dynamic system."
160,"Drawing from the research questions, a range of keywords were identified for the database searches, including ?mobile phone,? ?smart phone,? ?augmented reality,? ?distraction,? and ?multi-tasking.? In line with prior studies (Agarwal et al., 2019, Borges et al., 2021, Yan et al., 2021), Web of Science (WoS) and Scopus were identified as appropriate databases to obtain relevant and reliable journal articles across disciplines (see Table 1). To determine the eligibility of articles, inclusion and exclusion criteria were created in line with PRISMA guidelines (Massaro et al., 2016, Moher et al., 2009, Tranfield et al., 2003), see Fig. 2. Inclusion criteria included i) journal articles investigated mobile or AR technologies, and distraction or multi-tasking, involving customers (i.e., consumption context), ii) journal articles published in English, iii) journal articles published in high-ranking outlets (Scimago Q1 or Q2), iv) journal articles published since 2016 (to ensure the technology studied is up to date), v) journal articles empirical in nature, and vi) full-text versions accessible. The full exclusion criteria included i) publications merely mentioning distraction or multi-tasking, ii) publications where the focus was not on a customer-related experience, iii) publications in other than high-ranking journal articles, and iv) non-empirical research articles."
161,"To test the model's hypotheses, a bootstrapping method, using SmartPLS with 5000 subsamples, was used (Hair et al., 2011). R² values (coefficient of determination) are a measure of the predictive ability of a structural model. R² values higher than 0.25, as a rule of thumb, indicate a model has moderate explanatory power (Hair et al., 2011). The results of the tests of the hypotheses and the R² values are shown in Fig. 2."
162,"Three focus groups were conducted (between 5 and 8 participants per session) to examine specific issues (Krueger, 2014). The sessions lasted between 60 and 90 min. The participants were recruited in Spain, following a non-probabilistic, purposive approach. The composition of a focus group should have a certain degree of homogeneity to avoid huge differences in opinion emerging, but it should also be diverse enough to promote discussion and generate useful information (Phillippi & Lauderdale, 2018). As prior knowledge of cultural events can influence participants? perceptions and evaluation of an experience (Lobuono et al., 2016), we selected people with similar levels of knowledge about the cultural event under consideration, but with different characteristics in terms of age, gender, and willingness to adopt new technologies. The focus groups were run until the saturation criterion was met (Malterud et al., 2016). The composition of the focus groups is shown in Table 1."
163,"Analyses of the reliability and convergent validity of the scales were conducted using SmartPLS 4.0 software. The factorial loadings of the indicators exceeded the minimum recommended level of 0.70 (except one, see Appendix B; Hair et al., 2011). The composite reliability of the constructs and the average variance extracted (AVE) values were also higher than the recommended minimum levels (Hair et al., 2011) (see Appendix B). Discriminant validity was assessed based on the criteria of Fornell and Larcker (1981) and heterotrait-monotrait ratios (Kline, 2011), with both approaches returning satisfactory values (see Table 3)."
164,"As the research aims to advance the current academic knowledge on an under-debated issue ? i.e., how AI systems are twisted with public organizational actions and agents ?, we act on two levels. First, not viewing ?the world with a blank slate? (van de Ven et al., 2015, p. 2), we adopted as a mode of inquiry abductive reasoning (Timmermans & Tavory, 2012), as this logic appears to be particularly suitable to shed light on the uncertain, dynamic, and interconnected phenomena (Sætre & Van De Ven, 2021). Thus, we built on the TOE framework (DePietro et al., 1990) to revise the empirical phenomenon (Timmermans & Tavory, 2012), highlighting the factors associated with the implementation of AI. This framework allows us to cast light on the complex system of actors, actions, and interactions in which the technological artifact is entwined (Barley, 2020, Majchrzak et al., 2016), shedding lights on previously undistinguishable features. Fig. 2 illustrates the coding structure for the first round of interviews."
165,"Moreover, going through the last step described by Timmermans and Tavory (2012) ? alternative casing ? the TACT (Majchrzak & Markus, 2013) has been adopted as additional theoretical lens to complement the previous findings and deeply understand how AI is entwined with organizational agents, enabling, or obstructing, novel actions (Treem & Leonardi, 2012). Finally, we observed the presence of recurrent patterns in affordances and constraints, based on the features of the cases analyzed. In other words, we observed if and which features of the cases (type of AI, status of the project, geographical extent, etc.) may influence the presence or absence of certain affordances and constraints. The initial relationships were then refined via replication logic ? frequently revising each case to compare and verify the occurrence of specific constructs, relationships, and logics. Fig. 3 depicts the coding scheme of the second round of data gathered."
166,"This dual perspective allows us to shape a conceptual framework (Fig. 4) that links social organizing to the specificities of AI (Faraj & Azad, 2012). The framework offers a novel perspective and enriches the current debate on the implementation of AI, shedding light on the relationships between the contexts observed and explaining the complex and multifaceted factors and features behind AI implementation. As reported in the framework, our data show that AI implementation requires several factors to be present in a PSO. These factors can be divided into common implementation factors, which AI shares with any type of technological implementation, and AI-related factors, which are instead specific to AI. These latter factors afford or constrain new actions for PSOs. Finally, these relationships are mutated by a set of case-specific features affecting the entire implementation process, such as the type of AI technology and the user of the AI system."
167,H1: The dissemination and influence of health information in mobile social media are positively correlated with the information's quality. H2: The dissemination and influence of mobile social media are positively connected with its interactiveness. H3: The dissemination and influence of mobile social media are positively correlated with its dependability. H4: The dissemination and influence of health information are positively correlated with the accuracy of its content. H5: The dissemination and influence of health information are correlated with the information's content value. The conceptual model of our study is presented in Fig. 1.
168,"(1) Filtering of indicators : To ensure the scientific rigor and validity of our evaluation system, we recruited 684 mobile social media users who met specific inclusion criteria. These criteria included: (1) having utilized mobile social media, (2) being concerned about health information, (3) taking part freely, (4) being under the age of 18, and (5) having strong reading and comprehension abilities independently. The questionnaires were displayed on the ?Wenjuanxing? online professional survey platform, convenience sampling was employed, and we utilized popular platforms such as QQ and WeChat to distribute the questionnaires. Users of general mobile social media for health information made up the distribution population. To prevent duplicate responses, each user was limited to completing the questionnaire only once. The flow of indicator screening is shown in Fig. 2:"
169,"First, based on a preliminary survey of 684 valid questionnaires on the importance rating of user perception indicators, the weights of the first- and second-level indicators were calculated using the factor analysis method, and the importance ranking of the second-level indicators was obtained. The ranking reflects the advantages of user subjective weighting in the ranking of the second-level indicators; Then, based on empirical research sample data, the entropy method is used to calculate the objective weights of secondary indicators, and the objective weights of secondary indicators are used as the basis for calculating the importance ratio between indicators; Finally, the G1 combination weighting method was used to calculate the combination weights of the mobile social media health information quality evaluation index system [41,69,70]. Implement combination weighting, as shown in Fig. 3."
170,"Based on the responses to the questionnaire on the importance of users' mobile social media health information quality evaluation indicators, factor analysis was conducted using SPSS, and the weights of each question item indicator and public factor were calculated using the variance interpretation rate. The entropy method was chosen to calculate the objective weights of mobile social media health information quality evaluation indicators based on empirical data of mobile social media (represented by WeChat official accounts), and the combination weights were calculated using the entropy-modified G1 combination assignment method (see Table 4 and Fig. 4)."
171,"In this study, exploratory factor analysis and confirmatory factor analysis were used to test the dimensions and items of the evaluation index system for structural validity, convergent validity and discriminant validity. The KMO value of the indicator system is 0.922 > 0.7, Bartlett's test: X2 is 2817.8, p < 0.001, which shows that the indicator system has good structural validity. Table 2 demonstrates the rotated factor loading coefficients after adjustment and deletion in this study. The four factors extracted were named by combining the connotation of each variable; factor 1 was the value of health information content, factor 2 was the reliability of mobile social media, factor 3 was the trustworthiness of health information content, and factor 4 was the interactivity of mobile social media. From the results of principal component extraction, it can be seen that the cumulative explained variance of the extracted 19 question items is 77.31 %, which indicates that the four factors extracted from the 19 question items have a better explanation for the original data."
172,"According to the four-factor model derived from the exploratory factor analysis, the validated factor analysis of the mobile social media health information quality evaluation scale was conducted on the survey questionnaire data using Amos23 software. The results showed (see Table 3) that the factor loadings of the dimensions of interactivity of mobile social media, reliability of mobile social media, the trustworthiness of health information content, and value of health information content ranged from 0.748 to 0.883, 0.781 to 0.908, 0.771 to 0.858, and 0.693 to 0.911, respectively, which were all greater than 0.6, indicating that each of their latent variables corresponding to the AVEs of each dimension were 0.6987, 0.7162, 0.6686, and 0.7118, all of which were greater than 0.6; the combined reliability CRs were 0.8737, 0.9095, 0.8896, and 0.9516, all of which were greater than 0.8, indicating ideal convergent validity."
173,"The final weight and the scores of the four dimensions of the WeChat official accounts' health information quality evaluation system were derived to give a quantitative evaluation of the current situation of WeChat official accounts' health information quality; see Table 5. The total average score of WeChat official accounts health information quality in this empirical study was 48.24, among which the score of the WeChat official accounts subject interaction dimension was 45.20, the score of the WeChat official accounts reliability dimension was 59.95, the score of the content credibility dimension was 28.32, and the score of the information content value dimension was 53.55. It is clear that the quality of health information on WeChat official accounts is generally low, especially the lowest score of content credibility, and the future improvement of health information quality on WeChat official accounts should be promoted from the aspect of content credibility."
174,Fig. 1 shows the proposed conceptual model for the research hypotheses.
175,"The choice of the number of hidden layers depends on the complexity of the problem. ANN models with one hidden layer (shallow ANNs) are sufficient to model any continuous function, whereas ANNs with two hidden layers can be used to model even discontinuous functions [111]. ANN models with two or more hidden layers are called deep ANN models, as they enable deep learning and modelling of more complex relationships ([98]; Kalinic et al., 2021), but they also require more data for training and testing. Although Kalinic et al. (2021) proved that, in the case of relatively simple research models, the introduction of a second hidden layer does not deliver any improvement in terms of accuracy, in our case, the high number of predictors and nonlinear relationships led us to select the deep learning ANN approach (Alharbi and Sohaib, 2020; [98]). This provided two hidden layers in the neural network and, thus, a greater degree of precision. We determined the number of neurons in hidden layers using simulation software?SPSS v20 [95,112]?and we used sigmoid as an activation function in both hidden and output layers [[50], [96]]. The deep learning ANN model is presented in Fig. 2."
176,"To minimise the drop-out rate, we included information about the purpose of the research in the survey, together with a statement guaranteeing the anonymity of the respondents. In addition, we offered some small incentives, such as mugs and umbrellas featuring the university logo, and, at the end of the survey, awarded these to randomly selected participants. To reduce the occurrence of missing values, the participants were required to give their responses to all questions/statements before they could progress to the next page/end of the survey (otherwise, a notification would pop up). Regarding the sample characteristics, 51.5 % were women, 76 % of the total were under 45 years of age, 46.5 % had studied at the higher education level and 63.8 % were in employed work. Table 4 presents the sample characteristics."
177,"Prior to conducting further data analyses, we assessed the multivariate assumptions of normality, linearity, multicollinearity and homoscedasticity [98]. The results of the one-sample Kolmogorov-Smirnov test [99], presented in Table 5, indicate the absence of normal distribution [99], since all 2-tailed asymptotic significance values were 0.000?that is, less than 0.05 [82,96]. Hence, we opted for PLS-SEM in this study because it has been shown to be robust under conditions of non-normality [100]."
178,"We performed an ANOVA to test the linearity of the relationships between variables [100,82], the results of which are presented in Table 6. The results show that there are linear relationships between the dependent (use intention) and independent variables, since all p-values are below 0.05. However, seven out of the eight relationships reveal a statistically significant deviation from linearity, which justified the use of the ANN model?a nonlinear artificial intelligence technique that reflects the structure and operation of the human brain. The only exception was the relationship between price value and use intention, albeit its p-value (0.054) was very close to the significance threshold (0.05)."
179,"Multicollinearity is a problem of high correlation between independent variables [101]. The results of the multicollinearity test performed on our model (see Table 7) indicate that there were no issues of multicollinearity, since all the Variance Inflation Factor (VIF) values were in the range of 1.065?3.562 (i.e., less than 10), and the tolerances were all higher than 0.10 [93,99]."
180,"We evaluated the measurement model by analysing its reliability and convergent and discriminant validity. The reliability analysis included three indicators of internal consistency: Cronbach's alpha (CA; [102]), the Rho coefficient and composite reliability (CR; [103]). The values for all three tests were above the recommended minimum value of 0.7. We assessed CR using average variance extracted (AVE). The AVE indicates the amount of variance a variable obtains from its indicators relative to the amount of variance caused by measurement error. All the AVE values were above the recommended minimum value of 0.5 [104]. Table 8 lists these values for each variable, along with the mean of each item and the outer loadings (i.e., the loads estimated for the relationships in reflective measurement models)."
181,"In this study, the presence of nonresponse bias was detected. To address this issue, we conducted a multigroup analysis, following the approach outlined by Hair et al. [105]. We compared the group of respondents who completed the survey promptly (within the first 5 days of issue) with the group of respondents who completed it later. The results indicated that there were no statistically significant differences between these two groups in terms of all variables (p > 0.05). Consequently, it can be concluded that the potential impact of nonresponse bias on the sample data is likely to be minimal or negligible [106]. Next, we assessed discriminant validity by comparing the squared AVE with the intercorrelation scores. Discriminant validity is achieved if the squared AVE of a variable is greater than the intercorrelation with other variables [107]. We further checked discriminant validity by applying the heterotrait-monotrait (HTMT) ratio. Henseler et al. [108] suggest that a HTMT ratio score above 0.90 indicates a discriminant validity issue. The HTMT ratio scores were all below the threshold, indicating that discriminant validity was achieved (see Table 9)."
182,"First, we tested the research hypotheses by comparative analysis of the coefficients obtained by OLS, using IBM SPSS v20 as a simulation tool. The results, presented in Table 10, confirm that none of the initial eight hypotheses derived from the extended UTAUT2 model could be rejected?that is, that all eight predictors have a statistically significant influence on the dependent variable (use intention). The most influential predictors according to the OLS findings are performance expectancy (ßPE?UI=0.321, p-value=0.000), effort expectancy (ßEE?UI=0.177, p-value=0.000) and facilitating conditions (ßFC?UI=0.150, p-value=0.000), followed by hedonic motivation (ßHM?UI=0.120, p-value=0.000), subjective norms (ßSN?UI=0.100, p-value=0.000) and habit (ßHAB?UI=0.112, p-value=0.000). The least influential predictors as per OLS are risk (ßPRISK?UI= -0.084, p-value=0.000) and price value (ßPRI-VAL?UI=0.065, p-value=0.006). We assessed the quality of the OLS model using the values of adjusted R2 (which was 0.724) and normalised root mean squared error (RMSE), which was 0.1292. Both values are acceptable, meaning that OLS can be accepted as a valid baseline model."
183,"We also assessed the predictive ability of the model by determining the squared multiple correlation coefficient (R2). The R2 value for use intention was 0.708, meaning that it explains a high proportion of the variance of the model. Furthermore, we examined the standardised root mean square residual (SRMR) value [108] to test the difference between the observed correlation and the predicted correlation as an indicator of model fit. A value of less than 0.08 is considered acceptable. The model proposed in this study yielded a value below this threshold (0.046). We also evaluated effect size (f2) after reviewing research from Chin [109], who indicated that f2 values of 0.02?0.15, 0.15?0.35 and 0.35 or higher suggest that an independent or exogenous latent variable has a small, moderate or large effect, respectively, on a dependent latent variable. The relationship between the variables in the present study was found to exert a significant effect, and the lowest value with regard to f2 pertained to the relationship between perceived value and use intention. Finally, we assessed the predictive relevance of the model using Stone-Geisser's Q2 value. According to Chin [109], a model demonstrates good predictive relevance when its Q2 value is greater than zero. Thus, the present value can be considered adequate. Table 10 summarises all of these results."
184,"One of the potential problems associated with ANNs is overfitting [86], which occurs when the model ?memorises? data from the training sample and loses the ability to generalise when used with previously unseen data. To avoid this problem, we performed 10-fold cross validation [[95], [115]]. A common measure of the prediction accuracy of ANN models is RMSE [91,99] (Table 11). The low RMSE values presented in Table 11 indicate good reliability and high prediction accuracy for the proposed model [101,50]. Finally, we further evaluated the performance of the ANN model by determining its goodness-of-fit coefficient R2 [116,100], using the following formula: where   is the variance of the desired output. The value R2 = 0.957 indicates that the ANN acceptance model explains 95.7 % of the variance of use intention (model output), which is a significant improvement on the PLS-SEM results."
185,"Finally, we performed a sensitivity analysis of the ANN model to determine the importance of each predictor. The importance of a predictor measures the significance of the changes in the output caused by changes in different predictors [62]. The normalised importance is calculated by dividing the importance values of each predictor by the largest importance value [91]. Values for the relative and normalised importance of the ANN model are presented in Table 12."
186,"The most significant predictor of use intention is PE (average importance: 0.256), followed by EE (0.175), FC (0.134) and HM (0.104), which is in line with SEM-PLS findings. Next, the ANN model predicts that SN (0.099) has a more significant impact than HAB (0.089), which differs from SEM-PLS results. Finally, the two least influential predictors were PRISK (0.087) and PRI-VAL (0.057), which was also predicted by SEM-PLS findings. These minor differences between the ANN and SEM-PLS findings can be explained by the higher prediction accuracy of the ANN model and its capacity to consider any nonlinear relationships among the variables [[50], [62]]. A detailed comparison of OLS, SEM-PLS and ANN findings is presented in Table 13 [117]."
187,"Table 14 presents a comparison of similar research studies related to m-payment that have employed the UTAUT2 model as a theoretical framework. As can be seen, the results are aligned with the recent proposals of Al-Okaily et al. [120] and Migliore et al. (2020), which reinforces the generalisability of the findings obtained."
188,"Laddering approaches may be hard or soft: hard laddering involves a structured questionnaire that includes open-ended questions to be filled out by participants (e.g., in large-scale surveys; [78]), while soft laddering uses one-on-one interviews that promote in-depth conversation and enable concept clarification [31]. Given that AR is new to many consumers, participants might have difficulty articulating abstract experiences because relevant terminologies (e.g., contextual embedding) might not yet be established in consumer language. This consideration was the core reason for choosing soft laddering in this study. Fig. 1 summarizes our laddering approach."
189,"Based on the implication matrix, we developed an HVM that graphically represents the most dominant MECs (Fig. 2). The HVM is displayed as a tree diagram with the hierarchical level of the elements (from left to right), their relationships (arrows), and the strength of the links (arrow thickness). We found five dominant pathways in the HVM: the first and second?assortment and contextualization?facilitated a higher sense of perceived aesthetics and inspiration, respectively, which were important for users to achieve better choice-making, value for money, and perceived product fit. These benefits are critical for satisfying the underlying values of lifestyle, status and social influence, and security and safety. Third, shareability also facilitated inspiration, thereby reducing ?fear of missing out? (colloquially abbreviated as FOMO) and adding to the sense of safety. The fourth important route originated from portability and led to saving time and increasing productivity, thus satisfying economic values. Finally, the fifth theme emerged from reality congruence: consumers experiencing better value for money and a sense of accomplishment."
190,"To validate the assignment of benefits and values into broader categories, we applied an established procedure from the qualitative literature: a sorting task.1 Card sorting is a qualitative research method used to group, label, and describe information based on feedback from customers or users. Through an international research agency, we surveyed 411 adults (male = 292, female = 119 average age of 37 years) who had bought something using AR apps within the last three months and who were familiar with AR technology. Since sorting tasks require high cognitive effort among respondents, respondents sorted either benefits (n = 208) or values (n = 203). We asked about the benefit of using a ?typical? AR shopping app, such as IKEA or Sephora (which were mentioned frequently in the main study). The question provided four options representing broader categories to which the benefits belonged, and respondents were asked to select the most appropriate category for each benefit. We applied the same procedure for status, achievement, lifestyle, economy, and safety (SALES). Since respondents assigned the constructs to the proposed categories (see Appendixes E and F for details), the results validated our classification (See Appendixes E & F). Fig. 3 summarizes the taxonomies into sensory, efficiency, assessment, and discovery benefits (SEAD) and the values into SALES."
191,"The comprehensive model also extends prior work from a value perspective, specifically extant research grounded in uses and gratifications or technology acceptance research, which has revealed the importance of utilitarian benefits?a construct that covers how ?useful? or ?practical? consumers consider an AR app (e.g., [[47], [137]]). Our study extends these findings through the additional explanation of why some apps are more or less useful than others, for example, by proposing time savings as a specific benefit, thereby contributing significantly to the IS literature. The same appears for hedonic benefits, such as aesthetics and inspiration [15]. From a broader perspective, the authors deduce two subframeworks, SEAD (benefits) and SALES (values), which group certain variables into broader categories. As shown in Fig. 4, the framework can be used in a parsimonious way to explain how AR marketing impacts value. These benefit and value categories may serve as generic sets of variables for researchers to include in their theories. Overall, this research enhances existing understandings of AR technology use behavior and provides a framework to guide future research in this area."
192,"While the potential existence of injustice in the recommendations provided by algorithms has been known for some time, the literature has only recently raised concerns about the harmful effects of algorithmic injustice against particular groups or individuals [7]. The data revolution has been driven by advanced analytical tools, resulting in the widespread ?datafication? of the ?information civilization? or ?surveillance society? [7]. Central to this trend is the growing enhancement in data, which encourages firms to utilize these data in their decision-making processes [88,89]. Algorithms are used to identify patterns or correlations within datasets, serving as indicators to categorize a subject as a member of a particular group [90]. These classifications are created based on ?probabilistic assumptions? [91] that ?deindividualize? the subjects [92]. For example, a firm may promote its employees based on their gender rather than their qualifications, resulting in discriminatory decisions against disadvantaged groups. This highlights the potential discriminatory implications of implicit assumptions during programming and the use of biased data to train algorithms [93]. In a hiring situation, algorithms that are used to predict job performance may rely heavily on factors such as gender and race, without adequately considering the applicants? actual performance [94]. Such discrimination is often unexpected and unintentional, as most decision makers assume algorithms to be inherently objective. Nonetheless, decision making based on algorithms may lead to using socially unjust recommendations, even if the computing process is well-intentioned [95]. Therefore, according to the obedience to authority perspective, decision makers are influenced by information or advice provided by an authority (in this case, algorithms). Consequently, if the recommendation is unjust, the user's decision may also be unjust, potentially resulting in discriminatory decisions. We hypothesize as follows (Fig. 1): H1: Algorithmic injustice in DA tools is positively associated with making discriminatory decisions. H2: Making a discriminatory decision based on an unjust algorithmic recommendation does not result in guilt. H3. Displacement of responsibility strengthens the effect of algorithmic injustice on making a discriminatory decision. H4. Displacement of responsibility moderates the effect of making a discriminatory decision when using unjust algorithmic recommendations on guilt, such that for high levels of displacement of responsibility, making discriminatory decisions does not result in guilt. H5. Trust in DA outcomes strengthens the effect of algorithmic injustice on making a discriminatory decision. H6. Trust in DA outcomes moderates the effect of making a discriminatory decision when using unjust algorithmic recommendations on guilt, such that for high levels of trust, making discriminatory decisions does not result in guilt."
193,"In the experiment, participants were presented with a recommendation provided by DA and were asked to make a decision regarding the promotion of a candidate to a sales manager position. The decision specifically focused on choosing between a female candidate and a male candidate. This scenario aimed to simulate the use of algorithms in real-world promotion decisions, which have significant implications for individuals? lives and careers [108]. To ensure that the sample met the requirements of our study, we included a screening question asking participants about their roles in their respective firms. Participants who did not have managerial roles were excluded from the analysis. This decision was based on the understanding that managers are typically the primary users of DA tools and are responsible for accepting or rejecting the recommendations provided by such tools. The characteristics of the final sample are presented in Table 2."
194,"SPSS 26 was used to conduct the analyses. Cronbach's alphas, composite reliabilities, correlations, descriptive statistics, and the square roots of average variance extracted (diagonal values) are provided in Table 3. The item loadings associated with the constructs are presented in Table 4. To evaluate the presence of common method bias, we performed a marker-variable analysis [110]. We used extraversion as the marker variable, as it is theoretically not related to the variables in the research model. The average correlation between the marker variable and the main variables in the research model was 0.018, suggesting that common method bias was unlikely to exist in the data. To examine the manipulation check for ?just recommendation? versus ?unjust recommendation against women,? we used ANOVA. The results showed that participants in the ?just recommendation? treatment group reported a mean score of 4.35 (SD = 1.53), whereas participants in the ?unjust recommendation? treatment group reported a mean score of 5.31 (SD = 1.45) for the manipulation check question assessing the perception of unjust treatment toward the female candidate. The difference between the two groups was statistically significant (P < 0.05). Therefore, the manipulation of algorithmic injustice was successful."
195,"SPSS 26 was used to conduct the analyses. Cronbach's alphas, composite reliabilities, correlations, descriptive statistics, and the square roots of average variance extracted (diagonal values) are provided in Table 3. The item loadings associated with the constructs are presented in Table 4. To evaluate the presence of common method bias, we performed a marker-variable analysis [110]. We used extraversion as the marker variable, as it is theoretically not related to the variables in the research model. The average correlation between the marker variable and the main variables in the research model was 0.018, suggesting that common method bias was unlikely to exist in the data. To examine the manipulation check for ?just recommendation? versus ?unjust recommendation against women,? we used ANOVA. The results showed that participants in the ?just recommendation? treatment group reported a mean score of 4.35 (SD = 1.53), whereas participants in the ?unjust recommendation? treatment group reported a mean score of 5.31 (SD = 1.45) for the manipulation check question assessing the perception of unjust treatment toward the female candidate. The difference between the two groups was statistically significant (P < 0.05). Therefore, the manipulation of algorithmic injustice was successful."
196,"We performed a hierarchical logistic regression analysis in SPSS 26 to examine (1) the impact of algorithmic injustice on making a discriminatory decision and (2) the moderating role of displacement of responsibility and trust in DA outcomes on that association. The results, presented in Table 5, indicated that the impact of algorithmic injustice on discrimination (H1) was statistically significant (P < 0.001). While the moderating effect of displacement of responsibility on the relationship between algorithmic injustice and discrimination (H3) was not significant, trust in DA outcomes showed a significant moderating effect on the impact of algorithmic injustice on discrimination (P < 0.05) (H5). The findings also demonstrated that the control variables did not have a significant impact on making algorithmically informed, discriminatory decisions. Next, we ran an analysis of covariance (ANCOVA) to examine the effect of discrimination on the perception of guilt (H2). The results indicated that the effect was not significant (P = 0.19), supporting H2. We then added two interaction terms to the ANCOVA model to test the moderating effect of displacement of responsibility on the relation between discrimination and guilt (H4) as well as the moderating impact of trust in DA outcomes on that association (H6). The results showed that while H4 was not significant (P = 0.87), H6 was marginally significant (P = 0.08). In summary, while H1, H2, and H5 were supported, H3 and H4 were not supported, and H6 was partially supported."
197,"This study employs the theoretical framing of situational skepticism and signaling theory to develop a model explaining the emergence of consumer skepticism toward web seals (refer to Fig. 2 and Table 2). We argue that consumers rely on their topic and agent knowledge structures to elaborate on a web seal's unreliability (Section 3.1.1) and the seal authority's motives and credibility (Section 3.1.2), resulting in disbelief in the web seal and mistrust in the seal authority."
198,"We used the Amos software to assess the significance of the structural path estimates in our theoretical model. Fig. 4 presents the model testing results. The model explains 74.9 % of the variance in mistrust in the IS provider, 61.1 % in perceived assurance, 56.5 % in inferences of manipulative intent, 51.5 % in disbelief in web seal, and 57.3 % in mistrust in seal authority."
199,"Second, we performed a confirmatory factor analysis (CFA). We examined the reliability, convergent validity, and discriminant validity of the latent reflective constructs. The composite reliability (CR) was above the recommended 0.70 threshold [91]. The average variance extracted (AVE) for all constructs exceeded the suggested 0.50 threshold [28], thereby demonstrating good reliability and internal consistency (Table 4), except for our control variable, social desirability (AVE = 0.475). Because social desirability slightly falls below the 0.50 threshold but has a CR of 0.729, we kept social desirability in our model (cf. [28]). All indicators loaded significantly on their latent constructs, and standardized loadings exceeded the required minimum of 0.700, indicating good convergent validity, except for social desirability (Appendix D). In addition, we tested the discriminant validity of the constructs. Since the square root of the AVE of each construct exceeded the squared interconstruct correlations, each construct explained more variance in its indicators than it shared with other constructs (Table 4). "
200,"In addition, all heterotrait?monotrait (HTMT) ratios of correlations (Table 5) were below the 0.85 threshold [36], suggesting no discriminant validity problems. We also examined variance inflation factor (VIF) values to test for multicollinearity in our data. The highest VIF value was between inferences of manipulative intent and mistrust in seal authority (i.e., 3.146), falling below a threshold of 5.0, suggesting that our data are not subject to a severe multicollinearity issue [62]. To assess model fit, we used four metrics [70]: the ?²/degrees of freedom (df) ratio, the root mean squared error of approximation (RMSEA), the comparative fit index (CFI), and the Tucker?Lewis index (TLI). Common thresholds for acceptable model fit are ?²/df < 3, RMSEA < 0.80, CFI and TLI > 0.90 [32,42]. The CFA model yielded an acceptable model fit (?²/df = 2.329; RMSEA = 0.042; CFI = 0.943; TLI = 0.939)."
201,This study set out to identify and empirically test the antecedents and consequences of skepticism toward web seals. We conducted an online experiment to test the proposed hypotheses in a cloud service market context. Our results support the harmful effects of skepticism toward consumers? perception of IS providers (Table 6) and particularly emphasize the central role of seal authority incredibility in the nomological net. This study uncovers skepticism as a critical boundary condition for the effectiveness of web seals because skepticism can lead to the opposite effect of what is intended with them.
202,"The entire sequence of events of our model is described in Fig. 1. In the first period, both firms set their prices and consumers choose the product that maximizes their utility. After the purchase, the sales volume information and ratings information are generated. In the second period, the disclosed information is visible to consumers, thereby influencing the purchase. The disclosure of the two types of information generates three scenarios, namely sales information only, ratings information only, and both types of information."
203,"The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table 1 for details)."
204,"We have the following findings, which are consistent with previous ones:1 Firm H sets a higher price in the first period than in the second period when  . Firm L always sets a higher price in the second period than in the first period. 2 Firm H always sets a higher price than firm L in the first period but a lower price in the second period. 3 The profit of firm H is higher than that of firm L ( ) only when the price effect is weak?that is,  . As the price effect strengthens, the profit difference decreases ( ). Here, . 4 As the brand reputation gap increases, the profit difference increases ( ) except when, and this effect is mitigated as the price effect increases ( ). 5 The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table C1 for details)."
205,"We have the following major findings, which are basically consistent with previous ones: 1 Firm H always sets a higher price than firm L in the first period but a lower price in the second period. 2 The profit of firm H is higher than that of firm L ( ) only when the price effect is weak?that is,  . Here,  3 The impact of the brand reputation gap on firms? profits is non-monotonic. Specifically, there exists a win-win region within which both firms? profits increase when the brand reputation gap increases (see Table D1 for details)."
206,"Our BI system success model is depicted in Fig. 1. The model defines system quality, information quality, and service quality as important attributes in an individual user's evaluation of a system leading to actual use of the system and to user satisfaction [5,19,28,126,132]. System quality refers to the usability, availability, reliability, and response time of the system as experienced by its users, whereas information quality reflects the content and format of the system's outputs to ensure they are useful, relevant, and understandable to users in their decision making [29,40]. Service quality reflects the support users receive when using the IS, such as the ability for a support team to respond reliably and timeously to user queries [29]. Recent work has also drawn attention to the importance of data quality in IS success [23,43]. Data quality reflects the completeness and integrity of the underlying data captured and stored in a system [80,116], and is specifically relevant in the BI context [60,117]. User satisfaction is the affective response of the user toward the system, such as whether it is found enjoyable or frustrating to use [11,103]. Use and user satisfaction then have consequences for user performance [59,90,120], such as perceived contribution to individual problem solving, decision making, and productivity [53,58,92,99,109]. However, the measurement of use has been oversimplified in many IS success studies [90], and we therefore distinguish between routine and advanced system use. Fig. 1 also depicts our incorporation of user self-efficacy, along with the direct and moderating effects of task complexity on the links between BI use and performance."
207,The dimensionality of the constructs was confirmed through an initial exploratory principal components analysis with varimax rotation. The results (Table 4) did not suggest that any items needed to be dropped as all factor loadings were above 0.60 and average variance extracted values were all above 0.50 (Table 4). 
208," Moreover, as per Harman's one factor test, the first factor did not account for a majority of variance, only 14.5 %, suggesting no concerns over common method bias [67,94]. In addition, we considered the marker variable approach to test for common method bias [79]. We selected self-reported experience in management accounting as a marker because it is likely to be subject to a similar disproportionate response or acquiescence bias as other variables, such as levels of BI use and performance. However, it is not expected to be theoretically or statistically related to other model variables. When the marker variable was included as an additional determinant of the dependent variables, the significance of path coefficients did not change, providing further assurance that common method bias was not substantial [79]. Cronbach's alpha was used to measure internal consistency of the multi-item scales and found to be greater than 0.70 for all constructs. This provides support for convergent validity and reliability of our scales. To confirm discriminant validity, we compared inter-construct correlations with the square root of AVE of each construct. The square roots of AVE of each construct are presented along the diagonal of Table 5. These are shown as larger than the inter-construct correlations (i.e., constructs share more variance with their own items than with other constructs in the model). In addition, the HTMT ratios were calculated, and results indicate that the ratios are less than 0.85 [52]. Therefore, discriminant validity was confirmed."
209,"A Mann-Whitney comparison of users in the high advanced use subgroup with those in the low advanced use subgroup reveals significant differences across all model variables, except self-efficacy (Table 6). The mean rank for users classified as high advanced users was higher on all four system attributes, and in BI contribution to performance. These users also ranked significantly higher in task complexity."
210,The structural model was tested using AMOS with results reported in Table 7 indicating 17 out of 22 hypothesized paths were directly supported by the model being tested.
211,"System quality (p < 0.001), data quality (p < 0.01), information quality (p < 0.001), and service quality (p < 0.05) were all found to have positive significance effects on routine use. Furthermore, system quality (p < 0.001), data quality (p < 0.05), information quality (p < 0.001), and service quality (p < 0.01) were all found to have positive significance influence on advanced use, with effect sizes larger for advanced use than routine use, thus supporting H1a, H1b, H3a, H3b, H5b, H5c, H7a, and H7b. To further confirm the effects, we ran additional multiple regressions with bootstrap resampling to determine the overlap, if any, among confidence intervals. As shown in Table 8, we confirm no overlapping confidence intervals for effects of DQ and IQ providing added support to H3b and H5c. Although there is some overlap in confidence intervals for the effects of SQ and SQa, the overlap is not more than 50 % of the confidence interval range, thus giving us confidence to support H1b and H7b that their effects are greater on advanced use than on routine use. We also considered data quality to have additional indirect effects on BI use through effects on information quality. The effect of data quality on information quality is significant, supporting H5a."
212,"We considered the model's goodness of fit, the significance of the path coefficients, and the sign of the path to reach conclusions about moderation [48]. The results (Table 9) indicate that the complexity of the management accountant's tasks increases their opportunity to use the BI system innovatively to support their management accounting function (p < 0.001). Task complexity also has a moderating effect on routine use and performance (p < 0.05). The relationship between use and performance is moderated by task complexity, but the moderating effect is weaker for routine use, supporting both H13a and H13b. This confirms that the more complex the tasks of management accountants, the stronger will be the effect of advanced use on performance. This supports the importance of using the advanced features of the BI system to improve performance under conditions of greater task complexity."
213,Fig. 1 illustrates the conceptual integration of this new phase within the existing body of theory.
214,"Our study further reveals what concrete contextual manifestations make up these seven bias-inducing factors within the underlying particular context of AI-supported car manufacturing. Throughout the study's course, we captured these contextual manifestations in the form of second-order research elements. Fig. 2 presents a holistic overview over both the first-order psychological factors from the original status quo bias theory and their occurrence in our research as well as their second-order contextual manifestations."
215,"In the third step, which refers to Level 2c in the approach of [19] , the interviews were then coded in two different rounds within a parallel deductive and inductive approach. Within the former, we used our first-order research elements (i.e., the psychological factors from the status quo bias theory) for confirmatory coding of the raw case protocol. After that, in the second coding round, inductive coding was used on the statements extracted within the deductive first round to identify the concrete contextual manifestations of those bias-inducing factors. For instance, the sentence ?I think most people have seen the movie ?The Terminator?, right? I mean, the underlying idea as such was well-intentioned, but if it gets out of hand, if you lose control at some point, you're going to have a huge problem? (Interviewee 24, translated) was labeled with the code ?anchoring effects? in the first round and then with the contextual element ?Portrayal of AI in Hollywood movie (?The Terminator?)? in the second round. These contextual manifestations function as the second-order research elements in this study and the presentation of its results (cf. Fig. 2 and Table 3 in the results section)."
216,"During this step, insiders were grouped into categories, as shown in Fig. 2. "
217,"Moreover, we introduce a new kind of insider threat, Category C3 (Disempowered). Before the advent of generative AI, this was not an insider threat category, but now that hackers have these tools, the traditional rule-based approach is no longer sufficient (Fig. 5)."
218,"The analysis showed that the BU and vendor teams bypassed formal communication channels and exchanged information directly, hiding information from the SSU and the mediated system (see Fig. 1). Under these circumstances, rather than applying dogmatic rules as would be expected from a highly structured mediated system, the broker was astute in recognizing the often conflicting needs of the distributed system and adapted their own brokering actions accordingly. Our analysis shows that brokering actions tended to be based on four main conditions: exchange risk, information load, brokering capacity, and political influence. These are explored in more detail below."
219,"Departing from these assumptions, our findings show that because of workarounds, the formal broker needed to adapt its brokering practice and consider alternative practices that would address the exchange of hidden information within the information exchange network. As another level of abstraction, we next synthesize our findings from this case into three formal brokering practices, as depicted in Fig. 2."
220,"Our proposed PCGT model (see Fig. 3) depicts its three assumptions: the bidirectional reduction of privacy concerns, two types of privacy trade-offs, and anchoring effects. The model also includes a combination of rational and behavioral privacy decisions based on the three types of sustainability and two SSCC applications of monetary incentives. Thus, to demonstrate how consumers disclose their personal information, specifically that for SSCCs, the PCGT model describes not just the effects of the three types of sustainability on disclosure intentions but also a mediation effect and six moderated?mediation effects. Next, we discuss the effect of each type of sustainability on SSCC consumers? disclosure intentions and purchase intentions. Information disclosure acts as a mediator between privacy concerns and purchase decisions concerning SSCCs. Accordingly, we assume that privacy concerns will negatively affect information disclosure [4,107]. In parallel, we predict that SSCC consumers? purchase intentions will be affected more by disclosure intentions than privacy concerns because the willingness to use personal information for location-aware marketing positively influences purchase intentions [113]. Due to the unique privacy issues raised by SSCCs?that is, the large amount of personal information disclosed and its accessibility to multiple stakeholders?we posit that privacy concerns similarly affect consumers? disclosure intentions and purchase intentions regarding SSCCs (see Fig. 3). Thus, we propose the following hypotheses: H1. SSCC consumers? privacy concerns will negatively influence their disclosure intentions. H2. SSCC consumers? privacy concerns will negatively influence their purchase intentions. H3. SSCC consumers? disclosure intentions will positively influence their purchase intentions. H4. SSCC consumers? disclosure intentions will partially mediate the relationship between privacy concerns and purchase intentions."
221,"To understand why consumers share their information in response to vendors? sustainability-related advertisements and government policies concerning SSCCs, it is important to consider their rational and nonrational behaviors. We explicate consumers? privacy decisions from two perspectives, as summarized in Table 2 and Fig. 4: (1) from a PCM perspective, rational decisions, which involve personal benefits and monetary incentives, and (2) from a TBR perspective, behavioral decisions, which involve anchoring effects on sustainability."
222,"The instructions, manipulations, and surveys were delivered online through the Qualtrics XM? survey and marketing-research platform. Eligible participants were required to read and sign the institutional review board's consent form before deciding whether to participate in the survey. Those who consented were asked to provide demographic information and answer presurvey questions about relevant aspects of their backgrounds, including data breach experiences, general privacy concerns, technical knowledge, and sustainability preferences. Next, the participants read the experiment instructions (see Fig. 5). "
223,"We analyzed the relationship between privacy concerns, government subsidies for SSCCs, consumers? disclosure intentions, and purchase intentions when adopting SSCCs. Government subsidies significantly affected consumers? disclosure intentions and purchase intentions when adopting SSCCs (H7 and H8 supported). Table 10 and Fig. 7 show the results of the effect of government subsidies. The mediation effects of disclosure intentions between privacy concerns and purchase intentions when adopting SSCCs had the same results as the previous output because privacy concerns negatively influenced disclosure intentions (H1 supported) that positively affected purchase intentions (H3 supported). Privacy concerns did not directly affect consumers? purchase intentions (H2 not supported)."
224," Fig. B.1 shows the moderated?mediation model and its index, -0.09, which was calculated by SPSS Process 3.5 with 5000 bootstrap samples. Because the moderated?mediation effects of the fatal-accident rate had a negative relationship with privacy concerns, the index value was negative. The zero did not fall in the interval between BootLLCI and BootULCI, which indicates that the fatal-accident rate of SSCCs played a significant role as a moderated mediator. We found that the conditional direct effects of the fatal-accident rate of SSCCs were one of two moderated mediations in the relationship between consumers? privacy concerns and purchase intentions. Because the effect of SSCC privacy concerns on purchase intentions occurred through consumers? information-disclosure intentions and interacted with the fatal-accident rate, we called the moderated?mediation effect ?conditional indirect effects.?"
225,"The environmental performance of SSCCs, the second sustainability type, conditionally influenced SSCC privacy concerns on disclosure intentions and privacy concerns on purchase intentions, respectively (H6a and H6b supported). The moderated?mediation index of the environmental performance of SSCCs was 0.08, calculation of SPSS Process 3.5 with 5000 bootstrap samples (see Fig. B.2). Because the interval between BootLLCI and BootULCI did not include the zero point, we concluded that the moderated?mediation effects were significant. "
226,"The third sustainability type, SSCCs? fuel and system efficiency, also conditionally affected SSCC privacy concerns on disclosure intentions and purchase intentions, respectively (H6a and H6b supported). The moderated?mediation index of the fuel and system efficiency of SSCCs was -0.07, and both BootLLCI and BootULCI were negative values, which means the moderated?mediation effects were significant based on SPSS Process 3.5 with 5000 bootstrap samples (Fig. B.3). "
227,"After the participants were given their specific manipulations, they answered manipulation-check questions. These questions enabled us to determine whether participants remembered and understood the manipulations they were given. Table 4 shows the number of samples, means, and standard deviations for each variable. We provided manipulation checks in the following two ways."
228,"We conducted preliminary tests to assess the reliability and validity of the responses. The measurement model analyses involved the reliabilities for each correlation alpha (CRA), called Cronbach's alpha (a). Table 5 shows that all the scores were over the threshold of 0.7 [20,24]. Based on correlation matrix analyses, there were no critical issues regarding convergent validity and discriminant validity (see Fig. B.1). Because all average variances extracted (AVEs) were greater than 0.5, there were no convergent validity issues. The AVE square roots were greater than interconstruct correlations, which also indicated discriminant validity. The total number of violations was less than one-half of the potential comparisons [17]. We also tested the variance inflation factor to examine for potential multicollinearity. Based on the recommended value of 5, it was good in all cases except FE (FA = 2.2, EV = 2.1, FE = 5.2, GS. = 4.8, PC = 1.1) [40]. We tested for and ruled out common method bias using a marker variable and common latent factors in AMOS [72]. Finally, the results of confirmatory factor analysis (CFA) for sustainability demonstrated strong model fit statistics; the root mean square error of approximation (RMSEA) value was 0.043, which is lower than 0.07 [93]; the comparative fit index (CFI) value was 0.985, which is greater than 0.90; the Tucker?Lewis index value (TLI) was 0.982, also greater than 0.90."
229,"As Table 6 shows, to analyze the mediation effects of disclosure intentions between privacy concerns and purchase intentions, we used two tools: SPSS Process and AMOS. The effects of SSCC privacy concerns on disclosure intentions (t(454) = -8.82, p < 0) and disclosure intentions on SSCC purchase intentions (t(454) = 15.16, p < 0) were significant (see Table B.2 and Table B.3). We concluded that there were indirect effects of disclosure intentions between privacy concerns and purchase intentions because the interval between the lower level of confidence interval (BootLLCI) and the upper level of confidence interval (BootULCI) did not include zero (see Table B.3). The results of the model fit (GFI = 0.98, AGFI = 0.96, CFI = 0.99, RMSEA = 0.03) indicated that there was good model fit between the proposed model and the data, such that the data does not require re-specification (see Table B.4)."
230,"We analyzed the relationship between privacy concerns, government subsidies for SSCCs, consumers? disclosure intentions, and purchase intentions when adopting SSCCs. Government subsidies significantly affected consumers? disclosure intentions and purchase intentions when adopting SSCCs (H7 and H8 supported). Table 10 and Fig. 7 show the results of the effect of government subsidies. The mediation effects of disclosure intentions between privacy concerns and purchase intentions when adopting SSCCs had the same results as the previous output because privacy concerns negatively influenced disclosure intentions (H1 supported) that positively affected purchase intentions (H3 supported). Privacy concerns did not directly affect consumers? purchase intentions (H2 not supported)."
231,"Further to the main independent variables, we provide a set of campaign-specific factors to control the model, consistent with Vismara [43] and Nguyen et al. [37]. For instance, we use size of the management team to capture ta project's human capital, while the dummy variable patent indicates the existence of a patent in the project documents and is used as a proxy for projects? intellectual capital. The variable active campaign encompasses parallel projects that raise funds at the same time, which potentially lead to less daily crowdfunding investment in target projects. Some empirical findings indicate that parallel projects diminish support from investors in equity crowdfunding [43] and lenders within lending platforms [18] as well as backers in reward-based crowdfunding [51]. Table 1 provides detailed definitions and descriptive statistics for all variables used. Regarding the number of key statistics in Table 1, on the average, projects attract about 5 investors and raise nearly £10,000 daily. While the daily average number of investors in equity crowdfunding is comparable to that in other types of crowdfunding markets, such as reward-based crowdfunding [45] or lending-based crowdfunding [24,49], their daily fundraising volume is much higher. This indicates the important and potential role of equity crowdfunding in providing capital to young entrepreneurs. Our sample statistics are, to a large extent, consistent with samples of equity crowdfunding projects from other papers [23,42,43]."
232,We provide a correlation matrix among independent variables in Table 2. The correlation coefficients satisfy the condition of no multicollinearity in the model. We have no pair of variables that are highly correlated.
233,"Table 3 reports the results from our different panel regressions on the presence of herding dynamics in equity crowdfunding. Model 1 shows the results from curvilinear regression (specification 2), while models 2 and 3 report the outcomes of the linear regression in the first and last periods of the crowdfunding campaigns (specification 1). As the dependent variable of the daily number of investors is a non-negative integer, we first use random-effect negative binominal panel regressions in models 1?3 to control for overdispersion. Furthermore, following Xiao [45], we replicate models 1?3 in models 4?6 using random-effect OLS regressions with natural log of daily number of investors as the dependent variable. To confirm the robustness of the results, we also run model 4?6 using fixed-effect regressions. The results appear to be consistent."
234,"To illustrate the dynamics of these information sources throughout the funding process, Table 4 presents different statistics (mean, median, max, min) on the number of discussions and of Facebook and Twitter posts at the end of the first stage and the last stage of the funding cycle. Further to Table 4, Graph 1 shows the average growth rate of number of posts from these sources. The growth rate for each project is calculated as where, NDFTPLS is the number of discussions, Facebook, and Twitter posts at the end of the last stage, and NDFTPFS is the number of discussions, Facebook, and Twitter posts at the end of the first stage."
235,"We ran a series of different tests to confirm the robustness of our analysis. First, we constructed an interaction variable between the logarithm forms of lag investors and days available, which measures the number of days remaining in a funding campaign. We replicate the main analysis using this new interaction term in negative binominal and OLS specifications. Models 1 and 2 of Table 6 show that the interaction term is negative and statistically significant, suggesting that herding momentum is more prevalent toward the final days of the funding campaign (i.e., the number of days available is getting smaller). These results are consistent with our previous findings."
236,"We also replicate our main analysis using an alternative measure for herding, which is the momentum of daily funding amount and total prior funding amount, as used in the prior literature [24,45]. These measures are considered to be good alternative proxies, as information on funding amounts is publicly available in crowdfunding platforms, so as investors can use it in their funding decision-making. Indeed, as discussed in Zhang and Liu [49], investors may herd to solve two key questions of whether they should invest or not and if so, how much they should contribute. Table 7 replicates our main analysis from Table 3 using funding amount momentum as an alternative measure of herding behavior. The results from the robustness checks are largely consistent with the main finding that herding only occurs in the last stages of those funding campaigns. The results from replicating Table 3 using an alternative measure of herding are also robust."
237,"Finally, we extend our main analysis to another important UK equity platform?Seedrs. We replicate our main tests in a sample of 80 projects, listed in Seedrs during 2017?2018, as a method for assessing the validity of our main findings with out-of-sample data. Using a smaller set of control variables than for the Crowdcube projects, our results, illustrated in Table 8, suggest similar herding dynamics among Seedrs projects, with herding momentum appearing strongly in the final stage of the funding campaigns, and confirm our original findings."
238,"Building on the preceding hypotheses, our research model displayed in Fig. 1 explains users? intentions to continue using museum social media official accounts and brand pages, considering the extent to which they perceive certain gratifications (i.e., informational value, entertainment value and social value) as fulfilled through this use. The model also explores certain socio-demographic specificities related to users (i.e., age, gender), as research identifies them as significant in shaping different patterns of social media usage [121,[130], [131], [132], [133], [134], [135]]. To be coherent with the procedure and results of our previous study 2, the population targeted is the digital natives. We respond by doing so to the call of many studies for more research about the ways young people choose to interact with museums and how to attract them [122,136]. H4: Attitude toward digitally supported museum visits is positively related to a museum's social media continuance intention. H5a: Attitude toward digitally supported museum visits moderates the effect of informational value on social media continuance intention. H5b: Attitude toward digitally supported museum visits moderates the effect of entertainment value on social media continuance intention. H5c: Attitude toward digitally supported museum visits moderates the effect of social value on social media continuance intention.H3: Social value is positively related to a museum's social media continuance intention. H2: Entertainment value is positively related to a museum's social media continuance intention. H1: Informational value is positively related to a museum's social media continuance intention."
239,"As shown in Fig. 2, our research design allows us to address a variety of views and perceptions that belong to the two groups of actors who co-create the museum experience: museum professionals and visitors. Study 1 confirmed the enabling potential of ICT which are mainly employed from receptive and interactive mediation perspectives [19,20]. Social media technologies are specifically recognized as powerful transformative tools. Characterized by immediacy, interactivity and massive reach, they can improve visibility and accessibility to audiences and foster collaborative mediation patterns [67]."
240,"By applying the retroduction procedure [29,43] and the inductive methodology for the analysis of qualitative data [48], we identified multiple categories, including the self- and others-oriented affordances that potentially explain differences in users? Instagram experiences. The categories presented in the previous subsections were related to one another and formed an affordances-actualizations-outcomes model, as illustrated in Fig. 1. The proposed graphical model (Fig. 1) reflects the layered critical realism ontology. The core categories ?SNS Affordances? and ?Context Characteristics? are represented by ovals with dashed borders. The dashed borders symbolize the real layer of structures that include material objects as well as social and conceptual entities (thoughts, goals, and intentions). "
241,"Therefore, as the next step in the candidate mechanism and model validation, we applied quantitative methods and tested the relationships between the actualization of identified affordance groups (self-oriented and others-oriented) and outcomes. Based on the proposed model and, in particular, the nature of user motives behind self- and others-oriented affordances, we put forth and tested the following hypotheses (Fig. 2)."
242,"The items for the actualization of others- and self-oriented affordances (OAA and SAA, respectively) were developed in an iterative process based on the interview transcripts and often included the exact wording of the participants. Items for measuring the actualizations of affordances were developed for each user's goal-feature combination [57] and assessed on a 5-point Likert scale (1 = strongly disagree; 5 = strongly agree). If participants had trouble understanding an item, they could indicate this by choosing the option ?I don't understand the statement.? In the questionnaire, the items for the actualization of others- and self-oriented affordances were mixed and displayed in random order. Table 4 contains the final survey items with descriptive measures."
243,"Given that the measurement models of our predictors?OAA and SAA?are formative, specific criteria for assessing formative measurement models need to be applied. Some researchers propose that item weights that are significant at the 0.05 level and greater than 0.1 demonstrate high relevance for the formative construct [59]. Not all of our indicators met this criterion (Table 5). However, a simulation study by He [67] shows that it might be misleading and overly simplistic to assess indicator validity based on item weights only. He [67] argues that the contribution of individual items to the formative construct might be obscured by shared variance between indicators. This variance affects neither the predictive power nor the reliability of the construct. Despite the diverging views, there is an agreement not to omit items in formative models purely on statistical grounds. Dropping items alters the meaning of the formative construct and therefore is not recommended [58,59,63,67]. Hence, we kept all items, as each one covered a specific aspect of the constructs that emerged in our qualitative study. Another criterion for indicator validity is variance inflation factors (VIF) below 10 [59,68]. All VIFs of our items were below 1.6, which indicates that multicollinearity is not a problem (Table 5)."
244,"Intermittent discontinuance in STSF is a complex psychological and behavioral process, and understanding this process has positive implications for the operation and management of social media. On the one hand, it is related to the operational goals of social commerce and social service [10]. Generally, the relationship between brands and users is characterized by weak ties [13]. However, STSF can provide a carrier of strong social relations to help brand service providers transform weak ties into strong ones, thereby realizing better social marketing and value co-creation. On the other hand, it can help to uncover the black box of intermittent discontinuance, which makes timely identification, monitoring, and improvement in service quality and user experience convenient for a platform. Fig. 1 illustrates the details of this process."
245,"Negative and positive expectancy violations of the current STSF produce push and pull effects, respectively. Positive expectancy violations of the current STSF and negative expectancy violations of the competitive STSF together produce a mooring effect. The push and pull effects act as discontinuance factors for the current STSF, whereas the mooring effect acts as a continuance factor; these act together to produce intermittent discontinuance. Fig. 2 illustrates the mechanism of intermittent discontinuance of STSF under the EVT and PPM models."
246,"H1: Information overload positively affects STSF usage fatigue. H2: Social overload positively affects STSF usage fatigue. H3: System feature overload positively affects STSF usage fatigue. H4: Usage fatigue positively affects intermittent discontinuance in STSF. H5: Privacy concerns positively influence intermittent discontinuance in STSF. H6: Alternative attractiveness positively influences intermittent discontinuance in STSF. H7: Sunk costs positively influence a user's transition fatigue in STSF. H8: Transition costs positively influence a user's transition fatigue in STSF. H9: Transition fatigue negatively influences intermittent discontinuance in STSF. H10: Transition fatigue has a mediating effect on the relationship between sunk costs and intermittent discontinuance in STSF. H11: Transition fatigue has a mediating effect on the relationship between transition costs and intermittent discontinuances in STSF. Furthermore, we control for three demographic variables, including sex, education, and age. Other control variables include duration of use and the number of official accounts followed, contacts, and groups. Based on the above analysis, a conceptual model was developed, as shown in Fig. 3."
247,"The results of the structural model and hypothesis tests are presented in Fig. 4 and Table 7. As shown in Fig. 4, the corresponding R2 values for intermittent discontinuance, usage fatigue, and transition fatigue are 0.298, 0.332, and 0.651, respectively, indicating that the structural model has a good fit [75]."
248," Table 2 provides the sample profile information. There were 47 % male and 53 % female participants. More than 80 % of the participants were aged between 18 and 25 years, which is consistent with the statistics of the China Internet Network Information Center (CNNIC) [64] on social media users in China."
249,"The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem."
250,"The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem."
251,"The measurement models of reflective and formative variables should be tested differently [69]. For the reflective variables, we examined Cronbach's alpha and composite reliability (CR) for the reliability test, average variance extraction (AVE) values for the convergent validity test, and the square root of the AVE and the correlation coefficient between variables for the discriminant validity test. First, it can be seen from Table 3 that Cronbach's alpha and CR of all variables are more than 0.7, indicating good reliability of the questionnaire [70]. The results show that the AVE of every variable is above 0.5, indicating good convergent validity of the questionnaire [70]. Finally, as shown in Table 4, the square root value of the AVE of all variables is greater than the correlation coefficient between the variable and all other variables, indicating that the scale has good discriminant validity [70]. For the collinearity test, the variance inflation factor (VIF) statistics were used to detect the multicollinearity problem between structures. Inner VIF values range from 1.088 to 3.163, as shown in Table 5; outer VIF values range from 1.669 to 6.565, as shown in Table 3. The general statistical theory believes that the tolerance VIF value can be considered 10 [71]. The range of outer and inner VIF values of this model is far less than their recommended tolerance limits, so there is no serious multicollinearity problem."
252,"VIF values and item weights were examined to assess the reliability and validity of formative variables [69]. The reliability of the variable is considered adequate for VIF values lower than 3.00. If the item weights are significant at the statistical level, that is, if the t-values are greater than 1.960, then the validity test is satisfied. According to the results in Table 6, the formative variable (i.e., intermittent discontinuance) has good reliability and validity."
253,"The results of the structural model and hypothesis tests are presented in Fig. 4 and Table 7. As shown in Fig. 4, the corresponding R2 values for intermittent discontinuance, usage fatigue, and transition fatigue are 0.298, 0.332, and 0.651, respectively, indicating that the structural model has a good fit [75]. Table 7 indicates that all hypotheses are supported except for H3. Information (ß=0.450, t = 8.940, P<0.001) and system feature (ß=0.197, t = 3.671, P<0.001) overload positively and significantly affect usage fatigue, supporting H1 and H2. "
254,"We conducted a mediation analysis based on the guidelines of Nitzl and Roldan [77] and Zhao et al. [78]. As shown in Table 8, the direct effect of sunk and transition costs is significantly positive, while the indirect effect of transition fatigue is significantly negative. When the direct and indirect effects are both significant and point in opposite directions, the type of mediation is competitive mediation [78]. Therefore, transition fatigue exerts a competitive mediation effect, which weakens the impact of transition and sunk costs on intermittent discontinuance."
255,"Necessity analysis, which is used to identify whether the existence of a variable can be considered a necessary condition for a result, must be tested before analyzing sufficient conditional combinations [82]. Table 9 shows that both the consistency and coverage levels of each variable are below the recommended threshold of 0.9 for necessity analysis [80], indicating that the condition variables could not completely explain the outcome variable. Thus, no single condition was necessary for intermittent discontinuance. In conclusion, further analysis of the conditional configuration combinations is required."
256,"FsQCA3.0 is used to construct a 2K row truth table, where k is the number of antecedents. The recommended consistency measurement threshold is 0.8 or 0.9 [79]. This study chose 0.9 as a cut-off point to ensure a high degree of consistency at this stage of analysis. Fiss [83] recommended that the frequency threshold should be three when the samples exceed 150. This study set the number of acceptable cases to four. The final results for the configurations leading to high and low intermittent discontinuance are listed, respectively, in Tables 10 and 11."
257,"FsQCA3.0 is used to construct a 2K row truth table, where k is the number of antecedents. The recommended consistency measurement threshold is 0.8 or 0.9 [79]. This study chose 0.9 as a cut-off point to ensure a high degree of consistency at this stage of analysis. Fiss [83] recommended that the frequency threshold should be three when the samples exceed 150. This study set the number of acceptable cases to four. The final results for the configurations leading to high and low intermittent discontinuance are listed, respectively, in Tables 10 and 11."
258,"We examine an NFT transactions database sourced from Nadini et al. [30]. It spans from June 2017 to April 2021, containing over 6 million transactions from Ethereum and WAX networks. Table 1 summarizes the dataset. We aggregate the data on a monthly basis and run a battery of statistical tests. The underlying data have been sourced from five different NFT places: OpenSea, Atomic, Decentraland, Cryptokitties and Godsunchained. All the NFTs under consideration belong to the categories games, collectibles, metaverse, and art. The dataset is comprehensive and provides information about the date and time of transaction, accounts of the transacting parties, and transaction amount in USD as well as in Cryptocurrency units. Thus, the dataset is robust, statistically pliable, and representative of the general NFT market. The dataset is summarized in Table 1. The cryptocurrency price data of transactions is then exploited in this study to quantitatively assess the extent of the fraudulent practices prevalent in the NFT markets. All three statistical tests used in this study, Benford's test, Student's t-test for clustering and Power-law fitting, are performed on this parameter only. Fischer's test is then performed on the p-values obtained from the previous tests to address the concerns of type-1 error and p-hacking in the analysis. The present work performs analysis of monthly data to ascertain that wash trading is being thoroughly practiced in the NFT markets."
259,"Meanwhile, MAD conformity deals with the average divergence from an actual digit distribution. As per Nigrini and Miller [63], . We use the same author's decision criteria where values between 0.0012 and 0.0018 are acceptable, between 0.0018 and 0.0022 are marginally acceptable, and beyond 0.0022 are labeled as nonconformity. Results from Table 2 show that and scores from Chi-squared and Mantissa Arc tests necessitate rejection of the null hypothesis for all months. As a robustness check, we run a Distortion Factor Model, which is capable of revealing overstatement and understatement in data. We report the magnitude of the distortion of actual values against what Benford's Law expects. Conformity tests on mean absolute deviation (MAD) register positive results for only two months: September and October 2020. Otherwise, overall evidence overwhelmingly suggests violation of Benford's Law."
260,"Next, we investigate whether clustering takes place in NFT sales prices. Since human traders use round numbers as a mental heuristic to save time, such a test is well-suited to identify potential automatic trades, which often manifest in clusters. For each period, we divide the data into two groups: one with prices that are exact multiples of 1000 base units and another with prices within a 500 unit radius from multiples of 1000 base units. The base for the analysis is 4?10 units. The results of the clustering test in Table 3 show that, for 33 out of the 42 time periods analyzed, ~40 % of the trades are clustered around rounded values. This suggests that the majority of trades might be computer-generated."
261,"Our third investigation uses power laws to estimate the fat tails of the NFT prices. If detected, they could indicate the presence of herding behavior [65]. Prior works also attribute fat tails to inadequate information available to economic agents to value an asset [66]. Li et al. [67] have shown that for financial assets the tail exponents lie in the Pareto-Levy range, i.e.,where is the power law exponent. The results of the Power fitting law in Table 4 show that power exponents, on 21 out of 42 occasions, do not lie in the Pareto-Levy range, suggesting abnormal trading practices."
262,"The upshot of the three tests described above is this: the trading patterns suggest high probability of abnormal and computer-generated trading both symptoms of wash trading and price manipulation. However, since we apply multiple tests on the same dataset, concerns over type-1 error and p-hacking may surface. Hence, we perform a multiple hypothesis test using Fischer's method with the null hypothesis that NFT trades are consistent with universal patterns in traditional financial markets. The combined results in Table 5 show a rejection of the null hypothesis for all months alleviating the aforementioned concerns. Our results contradict traditional asset stylized facts. For instance, Corazza et al. [68] show that prices of S&P 500 stocks generally follow Benford's Law but not during extreme events; e.g. the September 11 attack in 2001 or the crash during the Global Financial Crisis. Within the digital asset sphere, our results are comparable to Cong et al.?s [58] report on cryptocurrencies traded on unregulated exchanges, but similarities do not extend to regulated exchanges. Overall, our results appear consistent with the prevalent reputation of NFTs."
263,"The second phase of this investigation studies both the first and second significant digits. As stated before, a comma or zero before the first natural number is ignored [61]. Benford's Law is used to attempt to detect anomalies in the datasets of the non-fungible tokens. Via this approach, an effort is made to inspect whether price manipulation is occurring in the non-fungible tokens markets. Most of the requirements that are stated above are met by the datasets in this research. However, for the volume data, the requirement of a large dataset is somewhat difficult. The benchmark for the required number of transactions for the price data was set at 1000. However, in the literature there is a discrepancy that a number of researchers use as the lower bound for the criteria of a large dataset. For example, Riccioni and Cerqueti [56] used 100 as the lower bound. Meanwhile, Vicic and To?ic [29] stated that the rule is at least 50?100 transactions, while they state that often thousands of observations are used. Some papers argue for 500 as the lower bound for a large dataset (Tables 6 and 7)."
264,"The second phase of this investigation studies both the first and second significant digits. As stated before, a comma or zero before the first natural number is ignored [61]. Benford's Law is used to attempt to detect anomalies in the datasets of the non-fungible tokens. Via this approach, an effort is made to inspect whether price manipulation is occurring in the non-fungible tokens markets. Most of the requirements that are stated above are met by the datasets in this research. However, for the volume data, the requirement of a large dataset is somewhat difficult. The benchmark for the required number of transactions for the price data was set at 1000. However, in the literature there is a discrepancy that a number of researchers use as the lower bound for the criteria of a large dataset. For example, Riccioni and Cerqueti [56] used 100 as the lower bound. Meanwhile, Vicic and To?ic [29] stated that the rule is at least 50?100 transactions, while they state that often thousands of observations are used. Some papers argue for 500 as the lower bound for a large dataset (Tables 6 and 7)."
265,"These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11)."
266,"These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11)."
267,"These categories are also mentioned in the descriptive data overview in Table 8, which stems from the Nadini dataset. In the selection of NFTs, there are 13 Art, 9 Collectibles, 20 Games, 4 Metaverses, 2 Others and 2 Utility NFTs. Some popular examples of NFTs in the Art category are CryptoKitties and CryptoPunks. Both CryptoKitties and CryptoPunks have thousands of different characters. Every cat or Punk is unique. CryptoKitties can even be used to breed a new CryptoKittie [74]. Bitverse, MLB Champion and Sorare are good examples of the category Collectibles. Bitverse is a digital universe where sets of cards can be collected. There are four types of cards, ranging from common to legendary (Bitverse Comics, 2021). MLB Champions and Sorare are quite similar NFTs. MLB Champions are virtual baseball players, while Sorare consists of player cards. In both MLB Champions and Sorare, the NFTs can be used to build teams and compete in a virtual game (Non-Fungible Corporation, n.d.). Some well-known NFTs, Axie Infinity, Gods Unchained and My Crypto Heroes, fall in the category of Games. Axie Infinity is a large gaming universe where cute pets, called Axies, live. The Axie universe is called Lunacia, which is the home for the Axies and the land there is tokenized. Axies? owners can collect, breed and battle with their pets (Non-Fungible Corporation, n.d.). Gods Unchained consists of six unique gods, who compete with each other in a virtual card game (Non-Fungible Corporation, n.d.). With the My Crypto Heroes NFTs, battles can be fought using all kinds of equipment (Non-Fungible Corporation, n.d.). Decentraland and Cryptovoxels are famous NFTs in the metaverse tribe. Users can choose what types of content they want to publish on their LAND, which is the NFT of Decentraland. The content can be static 3D scenes; however, it can also consist of any kind of interactive experience (Non-Fungible Corporation, n.d.). In the CryptoVoxels world, digital property can be bought and decorated with custom-designed monochrome blocks (Non-Fungible Corporation, n.d.). The two NFTs that are categorized as Others are CryptoAssault and Footbattle Card. CryptoAssault is one of the NFTs of which doubt can exist whether it is placed in the correct category. CryptoAssault is a 3D world in which territory can be battled for (Non-Fungible Corporation, n.d.). For Footbattle the same applies, as it is described as a footbattle management simulation (Non-Fungible Corporation, n.d.). The Utility NFTs are Unstoppable Domains and Urbit ID. Unstoppable Domains is an NFT that can be used to appoint simple names to cryptocurrency payments (Non-Fungible Corporation, n.d.). In contrast, Urbit ID can be used to send and receive cryptocurrency payments (Non-Fungible Corporation, n.d.). While there are NFTs in each of the six categories, it is very clear that the categories of Art, Collectibles and Games are the best represented in this subset (Table 9, Table 10, Table 11)."
268,"The results in the following tables are split into separate tables for the first and second digits and for the price and volume data. In total there are 8 tables with results. Conformity to Benford's Law is judged according to the Mean Absolute Deviation score while Pearson's chi-squared test can be unreliable for large datasets. In Table 12 the results are shown for the price data of the selection of 50 NFTs. For 16 out of the 50 NFTs the first digit distribution conforms to Benford's Law according to the Mean Absolute Deviation. Two of these conforming NFTs are also confirmed by a significant chi-squared statistic. It is noteworthy that, among others, the NFT collections Axie Infinity, CryptoKitties and Gods Unchained score an abnormally high chi-squared statistic."
269,"In Table 13 the results for the second digit distribution of the price data of the NFT selection are presented. Something outstanding here is that 22 out of 30 conforming NFTs score in close conformity. Eight out of the 30 NFTs that conform to Benford's Law also achieve a significant chi-squared statistic. Once again, the collections of Axie Infinity, CryptoKitties and Gods Unchained have high chi-squared statistics. For 0xuniverse and CyberKongz, these high values are observed as well."
270,"Table 14 reports the results of the first digit distribution of the volume data of the 50 NFTs. Eight NFTs score acceptable conformity, five score marginally acceptable conformity and the remaining 37 NFTs nonconformity. Eleven of 13 conforming NFTs also show significant chi-squared scores. However, out of the 37 nonconforming NFTs, six nonetheless have a significant chi-squared statistic, which would indicate conformity. Either way, the MAD score is deemed as the leading test statistic; therefore these NFTs remain classified as nonconforming."
271,"Table 16 moves on to the results of the yearly aggregated data, starting with the first digit distribution of the yearly aggregated price data. The years 2017 through 2019 scored acceptable conformity, 2020 scored close conformity and the first 117 days of 2021 scored nonconformity. Nonetheless, with a MAD score of 0.01567, it is still close to marginally acceptable conformity. The transactions that are included in the years that do conform to Benford's Law amount to 44.3 % of the total number of transactions (6062,744) that are examined in the results section. All years have a high chi-squared statistic. Regardless, the year 2021 has an extraordinarily high chi-squared statistic, namely over 140,000."
272,"Table 17 demonstrates the second digit distribution of the yearly aggregated price data of the complete Nadini dataset. All of the years 2017 through 2021 obtained close conformity to Benford's Law. Once more, all of the years show a high chi-squared statistic"
273,"A pattern can be observed in Tables 18 and 19. In total 16 NFTs conform to the first digit distribution according to Benford's Law. Fifteen out of these 16 NFTs also conform to the second digit distribution according to Benford's Law. Therefore, this might be a consistent pattern that can also be found in other transaction data. Economically this pattern seems to make sense as well. If price manipulation would take place, it would be logical that both digits or at least the first do(es) not conform to Benford's Law. It would not make sense if the first digit distribution does conform to Benford's Law while the second digit distribution does not conform to Benford's Law. However, this pattern is not as strong for the volume data. The findings indicate that merely seven NFTs out of the 13 that conform to Benford's first digit distribution also conform to Benford's second digit distribution. Additionally, it can be observed that the 50 NFTs that were selected conform more often to Benford's Law for the price data than for the volume data. There were four NFTs that conformed to Benford's Law in all four tables, namely Crypto Space Commanders, CryptoKitites, Decentraland and Gods Unchained. They conformed for both the price and the volume data to the first- and second-digit distributions of Benford's Law. No pattern is found regarding the different categories of the NFTs. The categories are dispersed relatively in the same manner in the results as in the NFT selection. It can be observed that there are more NFTs in Art, Collectible and Games that appear in the conforming results; however, these NFTs are also represented way more in the NFT selection. Therefore, no relative difference can be observed."
274,"Table 19 shows the first- and second-digit distribution of the yearly aggregated volume data of the entire market history as recorded by nonfungible.com. For both the first- and second-digit distribution, all years (2017 through 2022) score nonconformity according to the MAD statistic. However, for the first-digit distribution, the chi-squared statistic reports a significant value for the years 2017 and 2022. For the second-digit distribution, the same applies for all of the years. Again, this is probably due to the small N. A pattern can be observed in Tables 18 and 19. In total 16 NFTs conform to the first digit distribution according to Benford's Law. Fifteen out of these 16 NFTs also conform to the second digit distribution according to Benford's Law. Therefore, this might be a consistent pattern that can also be found in other transaction data. Economically this pattern seems to make sense as well. If price manipulation would take place, it would be logical that both digits or at least the first do(es) not conform to Benford's Law. It would not make sense if the first digit distribution does conform to Benford's Law while the second digit distribution does not conform to Benford's Law. However, this pattern is not as strong for the volume data. The findings indicate that merely seven NFTs out of the 13 that conform to Benford's first digit distribution also conform to Benford's second digit distribution. Additionally, it can be observed that the 50 NFTs that were selected conform more often to Benford's Law for the price data than for the volume data. There were four NFTs that conformed to Benford's Law in all four tables, namely Crypto Space Commanders, CryptoKitites, Decentraland and Gods Unchained. They conformed for both the price and the volume data to the first- and second-digit distributions of Benford's Law. No pattern is found regarding the different categories of the NFTs. The categories are dispersed relatively in the same manner in the results as in the NFT selection. It can be observed that there are more NFTs in Art, Collectible and Games that appear in the conforming results; however, these NFTs are also represented way more in the NFT selection. Therefore, no relative difference can be observed."
275,"HNGOs in Germany typically have a three-level operational structure based on the German federal structure, with a national office, intermediate offices, and regional units (see Figure 1). The intermediate offices sit between the regional and national levels and may correspond to a German state or administrative region. This organizational set-up reflects Kemp et al.?s [12] differentiation of a (1) strategic level (e.g., developing a vision, formulating long-term goals), (2) tactical level (e.g., networking, agenda building), and (3) operational level (e.g., implementing, experimenting, building)."
276,"The results of this study highlight the emergence of a self-producing/constructing information governance system reflected in an ?organizationally closed but structurally open? manner through self-reference to its own organizing principles. We see that social media information governance for accruing social capital has been managed in a self-sustaining manner by the HNGOs. Figure 2 summarizes this idea, as well as the focus areas of social media use and accruing social capital at the strategic, tactical, and operational levels."
277,"We conducted nine semi-structured interviews with experts at the regional level, six at the intermediate level, and three at the national office level; these experts came from five different HNGOs. Most of our interviewees working at regional units were volunteers, as this is where most volunteers engage with HNGOs; our paid staff interviewees were mostly active in intermediate and national units (headquarters), where HNGO permanent operations are situated. Hence, our sample reflects these organizational structures. The interviews were conducted in German via Skype or by telephone in 2019 and 2020. All interviews were between 30 minutes and 2 1/2 hours in length, depending on how detailed responses were; interviews conducted later in the study tended to be longer as we refined our interview guide and protocols. Table 2 is an overview of the interview sample."
278,"Contribution intentions were regressed on crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, and their interaction (see Table 2 for the stepwise regression results). As expected, the results revealed a significant interaction between crowdfunding platform and prosocial motivation (B = 2.24, SE = 0.72, p < 0.01). As prosocial motivation is a continuous measure, the analyses were repeated using a spotlight analysis at one standard deviation below and above the mean (see Fig. 1; [63]). The analysis revealed a significant negative simple effect of crowdfunding platform type for participants low in prosocial motivation (B = -2.31, SE = 0.74, p < 0.01), indicating they were willing to contribute less money in the reward (vs. donation) condition. Conversely, there was no effect of crowdfunding platform type for those with high prosocial motivation (B = 0.93, SE = 0.71, p = 0.20)."
279,"Given that this study sampled the general population on MTurk rather than a more homogeneous undergraduate sample as in Study 1, we controlled for participants? gender, age, education level, and income. Contribution intentions were regressed on prosocial nature of the project description (1 = high; 0 = low), crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, their three-way interaction, and all lower-order interactions, while controlling for gender, age, education level, and income level (see Table 3 for the stepwise regression results). The results showed significant effects of the interaction between prosocial nature of the project description and crowdfunding platform type (B = -3.64, SE = 1.45, p = 0.01), the interaction between prosocial nature of the project description and prosocial motivation (B = -0.39, SE = 0.19, p = 0.04), and the three-way interaction between prosocial nature of the project description, crowdfunding platform type, and prosocial motivation (B = 0.56, SE = 0.26, p = 0.03). As prosocial motivation is a continuous measure, the analyses were repeated using a spotlight analysis at one standard deviation below and above the mean to probe the three-way interaction [63]. "
280,"Similar to Study 2, given that this study sampled a more general population on Prolific rather than a more homogeneous undergraduate sample, we controlled for participants? gender, age, education level, and income. We also controlled for nationality given that we collected data from both American and Chinese participants. Contribution intentions were regressed on prosocial nature of the project description (1 = high; 0 = low), crowdfunding platform type (1 = reward; 0 = donation), prosocial motivation, their three-way interaction, all lower-order interactions, and control variables (see Table 4 for the stepwise regression results). Results showed significant effects of the interaction between prosocial nature of the project description and prosocial motivation (B = -0.74, SE = 0.34, p = 0.03) and the interaction between crowdfunding platform type and prosocial motivation (B = -0.94, SE = 0.31, p < 0.01). There were also marginally significant effects of the interaction between prosocial nature of the project description and crowdfunding platform type (B = -5.09, SE = 2.62, p = 0.05) and importantly, the three-way interaction between prosocial nature of the project description, crowdfunding platform type, and prosocial motivation (B = 0.78, SE = 0.45, p = 0.08)."
281,"To address these research questions, we leveraged on a longitudinal dataset that allowed us to observe the effects of the marketing choices (price and flexible policies as functional attributes) and economic returns (occupation rates and revenues per active nights, as in line with Airbnb literature; [17]) at a single Airbnb property level in the city of Rome (i.e., the largest touristic submarket in Italy). The first evidence that emerged from these data is that the sharp market contraction due to the pandemic shock affected demand much more than supply: the negative change in revenues has in fact been about five times larger than the exit rates (see Table 1), thus depicting a novel market condition on the Airbnb platform. This circumstance suggests that competition must have increased substantially and, consequently, forced (at least some) entrepreneurs to react to the shock with renewed activism."
282,"With the aim of describing the impact of the shock in our sample, Table 2 reports the descriptive statistics of the two dependant variables for the considered three years, that is, 2018, 2019, and 2020. In line with Ghebreyesus [18], the descriptive statistics are computed for the months of March to Decemeber of each year. Consistent with the magnitude of the shock, Table 2 reports that the mean values of occupation rate and revenues per active nights declined significantly in 2020, compared to 2018 and 2019, years that were instead rather stable."
283,"Table 3 provides the number of properties (and the relative shares in brackets) that adopted a given cancellation policy (moderate, flexible, or strict) in 2018, 2019, and 2020, and also shows the year-over-year transitions. Table 3a shows data pertaining to the benchmark situation (i.e., 2018 to 2019), while Table 3b shows data for the Covid-19 shocked situation (i.e., 2019 to 2020)."
284,"Table 4 provides a comparison of the pricing adjustements between 2018 and 2019 and between 2019 and 2020, and it also shows the main descriptive indicators for the absolute price variation (i.e., the absolute value of PV(i,m)) as well as the results of a mean comparison test (i.e., the T-test). Fig. 2, which is complementary to Table 4, plots the distribution of PVy(i,m) in 2019 and in 2020."
285,"Table 5 shows the estimates of the econometric model that was used to predict the impact of the pricing adjustments, the impact of the flexible cancellation policies, and the effect of joint adoptions on the occupation rate. It is worth noting that the IV diagnostic statistics (i.e., Kleibergen-Paap weak identification tests) fully confirmed the validity of the instrument for our empirical setting.16 Models M1 and M2 include control variables only, Model M3 includes control variables and PV2020(i,m), Model M4 includes control variables and ModFlexPolicy(i), Model M5 includes control variables, PV2020(I,m), and ModFlexPolicy(i), while Model M6 includes all the variables as well as the interaction between PV2020(i,m) and ModFlexPolicy(i). Table A4 in the Online Appendix provides the estimation results when a pooled OLS estimation was employed, without instrumenting PV2020(i,m), and rather similar results can be observed."
286,"Table 6 shows the estimates of the econometric model when RevPAN in 2020 was considered as the dependant variable and tested on the sample of strict cancellation policy adopters in 2019. Again in this case, it is worth noticing that the Kleibergen-Paap weak identification tests fully confirmed the validity of the instrument for our empirical setting.18 Models M1 to M6 are defined as in Section 5.2.1. As for the analyses on the OccR, Table A9 in the Online Appendix provides the estimation results obtained when employing a pooled OLS estimation without instrumenting PV2020(i,m), and they show rather similar results."
287,"Guided by the theoretical lenses of adaptive structuration and sensemaking theories, the findings from this study have presented a conceptualization of why and how social media use and application evolve (see Fig. 2). Fig. 2 summarizes this processual framework's building blocks and their relationships that emerged from the analysis of the interview data. Social media features as created by designers and perceived by potential users play a central role in this process, which unfolds on the individual and firm levels. Salient characteristics that are likely to be perceived differently generate the triggering conditions as a stage of pre-attention, which may in turn initiate later stages of meaning-making of social media. This meaning-making takes place not only on the individual cognitive level, but also at the group level as part of a mutual exchange. However, the sensemaking of social media and its related use responses mainly occur via the cognitive interpretations of leader-managers, or responsible executives who commonly play the role of ?influential individuals? [93] within SMEs."
288,"Sample companies were purposely selected to obtain information-rich data that were able to contribute to a deeper understanding of the perceptions, concerns, and behavior related to social media use [109]. The interviewees were all key informants at their respective companies, i.e., owner- or business-managers, or executives responsible for their firms? social media management (see Table 1). A snowball sampling procedure [122] was employed, initiated via the first author's acquaintances. The use of natural social ties for accessing the ?elite informants? [119,122] counterbalanced the often prevailing asymmetry in the power relationship between interviewer and interviewee. To represent the heterogeneity of the SME area, the interview participants varied along several dimensions, such as gender, education, social media experience levels, and age. The participants? ages spanned from 23 to 71 years, with a job tenure ranging from 1 year to 30 years and more. This long period of service for a company applied to the owner-managers and founders of a hotel (SME 9), a men's shoe fashion wholesale (SME 15), and a firm consulting entrepreneurial couples (SME 18). Valuable insights into the research topic also resulted from the respondents' various social media worlds and accounts thereof complementing each other. Particularly, young participants aged under 35 years, acting in car and fashion retail or online marketing (SME 7, 8, 14, and 16), seemed to feel superior in knowledge and experience with social media to members of other age groups within and beyond their companies. Apart from the 42-year-old, socially and politically highly engaged owner-manager of a book retail shop (SME 13), all firm leaders were male. Most of the female participants held positions of a business, marketing, or social media manager."
289,"Although previous investigations [22] found that regular social media activity remains challenging for firms regardless of their size, this study showed differences in companies? behavioral consistencies. As innovation adoption literature in general, and especially in the SME domain, mainly restricts its focus to mere adoption processes, which fail to account for a comprehensive, sustainable use [152], a better understanding of social media usage in terms of (dis)continuity [45] is needed. Initially, executives are equally exposed to a broad bundle of structural, i.e., material and informational, properties of social media [9]. Nevertheless, these features open up different, context-related journeys on which SME managers may embark in conditions triggering perceptions and behavior in terms of this technology as a whole. From a structuration perspective [40], our findings suggest that these structural properties of social media possibly fail to cause triggers unleashing a sensemaking and engagement process. Here, the encounter with few, negatively connotated features may not produce triggers, such as perceiving obligation or novelty [100] for driving sensemaking and steady use behavior. This tendency effect is reinforced by environmental influences, such as necessary protection against competitors or lacking regulatory and market requirements (for a tendential relation between perception of social media features and steadiness of use behavior derived from interview data, see Table 3)."
290,"Table 2 reports the key descriptive statistics and correlation matrix for the variables applied in this study, with Bonferroni-adjusted significance levels below 0.01. Multicollinearity does not represent a problem for any of the variables as the Variance Inflation Factor (VIF) is largely below the suggested threshold of ten [110]."
291,"Table 3 shows the distribution of the strategic role of IT in industry in our sample of 1769 firms (and 17,690 observations) operating in 382 four-digit industries from 2011 to 2020."
292,"Models 1a ? 12a in Table 4 report the comparative analysis among subsamples (i.e. Automate, Informate, Physical-Transform, and Digital-Transform) used to assess the best alignment between business strategy and the strategic role of IT in industry on labour productivity growth (Models 1a, 4a, 7a, 10a) and its value components ? output growth (Models 2a, 5a, 8a, 11a) and input reduction (Models 3a, 6a, 9a, 12a) ? in Automate, Informate, Physical-Transform and Digital-Transform industries, respectively."
293,"As a robustness check, we also used a dynamic panel data estimation to overcome any possible endogeneity issues arising from reverse causality due to the prior performance of a firm. The results are shown in Table 6 and are consistent with the results of the comparative analysis of the subsamples (Table 4) and with the results on the moderating effect of the strategic role of IT in industry (Table 5)."
294,"H1 Smartphone utilitarian gratifications will reduce consumer state anxiety during shopping journeys. H2 Smartphone hedonic gratifications will reduce consumer state anxiety during shopping journeys. H3 Smartphone social gratifications will reduce consumer state anxiety during shopping journeys. H4 Consumer state anxiety mediates the relationships between smartphone U&G and in-store purchase intention. Given the hypothesized relationships above, we propose a visualized conceptual framework as follows (Fig. 1)."
295,"Reliability was tested using Cronbach's Alpha value (a) (see Table 4) with all variables exceeding the threshold of 0.7. Moreover, to avoid common method bias, we employed Harman's one-factor test [138], which reported about 25.2% of variances explained by extracting only one factor, meeting the thumb value within 50% of the variance among all variables. This implies that there was no bias when applying respondents? answers in the same questionnaire for both independent and dependent variables. A reflective measurement model was inspected by conducting confirmatory factor analysis (CFA) via AMOS 26. Given the adequate sample size of 349 responses, the model fitness indices were as follows: ?2 = 656.355, degree of freedom (df) = 409, CMIN/DF= 1.605 (<3), p < 0.001, GFI= 0.903 (>0.9), TLI= 0.944 (>0.9), CFI= 0.950 (>0.9), RMSEA= 0.042 (<0.07). Hence, all the GOF indices met acceptable requirements, indicating that the measurement model achieved a good fit. As shown in Table 4, each item (Con1, Con2) and the first-order indicators (CON, OSO) significantly relate to the second-order latent constructs, the so-called utilitarian gratifications. Similarly, satisfying results were achieved with respect to hedonic and social gratifications and consumer state anxiety. In addition, the dependent variable (in-store purchase intention) was measured through a single item by probing the extent to which consumers were willing to purchase products after using their smartphones in-store. Single item has been accepted in existing studies as respondents can easily interpret the question [139,140]. This variable is further examined in the SEM and mediation analysis."
296,"Apart from achieving scale reliability, examining construct validity is suggested to embrace both convergent and discriminant validity tests. Convergent validity is assessed according to three aspects. First, all factor loadings should be statistically significant, with a standardized parameter of 0.5 or higher [141]. According to the output in Table 4, all indicators (CON and OSO) are significantly related to the latent constructs, falling between 0.534 (product information seeking via branded mobile apps) and 0.927 (consumer engagement via online brand communities). Second, the average variance extracted (AVE) is considered as the mean variance extracted for the items loading on a construct and is a conclusive index of convergence [142], with a suggested adequate convergence of over 0.5. The third attribute evaluates composite reliability (CR), an acceptable value of good reliability, suggested as being higher than 0.7. Table 5 demonstrates the convergent and discriminant validity performance."
297,"When performing the structural model, smartphones? utilitarian, hedonic, and social gratifications were independent variables, the mediator being named as consumer state anxiety, and purchase intention was the dependent variable. In a similar vein, the model's fitness indices were exhibited first: ?2 = 860.589, degree of freedom (df) = 532, CMIN/DF= 1.618 (<3), p < 0.001, GFI= 0.910 (>0.9), TLI= 0.928 (>0.9), CFI= 0.936 (>0.9), RMSEA= 0.042 (<0.07). These indicators support a valid and reliable structural model leading to hypotheses testing (Table 6)."
298,"Mediation analysis was performed to test the fourth hypothesis via Hayes? PROCESS Macro [144]. The advantages of conducting mediation analysis beyond SEM are threefold in this study. First, the PROCESS can test moderator and mediator effects in one model and suggest conditional outcomes. Second, SEM inspects the entire model while PROCESS can perform each equation separately [145]. Third, PROCESS incorporates bootstrapping methods that further recommend reliable results by evaluating extra information. Table 7 presents the mediation analysis results, including total, indirect, and direct effects of the models."
299,"Fig. 1 provides an overview of the research methodology as a process consisting of three stages; define and design, prepare, collect and analyze, analyze and conclude, which was used in this study. It was abductive in nature going back and forth between the empirical world, the cases, the framework, and theory by matching, redirecting, and directing between those cases [54]. The definition and design phase started simultaneously with the literature overview and empirical frame of reference by looking out for interesting cases and matching them with theory, in this case, the TOE framework. Cases were identified and selected by how far they have come with their cloud sourcing. The data collection was designed to be qualitative and consisted of interviews, observations, and text analyses. Then the next phase of the research method commenced with preparation, data collection, and analysis of data. This phase took about a year and a half and was iterative, and abductive with redirections and matching to literature conducted in the first phase of the research methodology, framework, and theory development. The analysis and initial findings were documented in case reports before the last phase of the research methodology. In the last phase analyze and conclude, we once again reviewed thoroughly the literature, the empirical world, the theory, the cases, and the framework in order to draw cross-case conclusions. This directed us to modify the theory and suggest a matching framework (TOMPE) that complements the TOE with a newly identified factor ? namely the management process barrier (MP). We have further developed the barriers to continuance use of cloud computing."
300,"The research data amounted to 83.5 h of audio recordings. These were transcribed verbatim due to the specifics of the oil and gas jargon and to avoid obstructing preconceptions [87]. To ?recontextualize? the extensive research data, transcripts were further analyzed using the qualitative data analysis software NVivo 20. Applying the Gioia methodology as an analytical device [88], we structured the research data along with multiple data-anchored first-order concepts (those meaningful to the interviewees) that were further structured into theory-anchored second-order themes (induced by the authors). Our data analysis further abductively unpacked three aggregate dimensions related to the research question?pressures promoting adoption of hyped technologies, segmented perception of hyped technologies, and trust-building mechanisms (Fig. 1). We offered labels of the aggregate dimensions either by encapsulating dimensions at a higher level of abstraction or by referring to established literature describing these themes [85]. To increase the rigor of this qualitative research, we seek to provide transparency to our data structuring process. "
301,"Although several studies have examined hyped technologies [[25], [26], [27],90], the role of the hyped status in the implementation of and trust in hyped technologies such as digital twins is far from consensual [39,40]. Fig. 2 synopsizes our main findings about the application of hyped technologies as a multilevel process. This model illustrates the critical role of hype and trust building in the diffusion of technologies. As argued below, this model shows the two main contributions of this study: (1) how the hyped status of technologies materializes in multilevel perception segmentation in the organization(s), and (2) how local interpreting actors maneuver this segmentation by building trust in hyped technologies."
302,"Fig. 1 presents our proposed research model. This argument, as illustrated in the theoretical hypothesis 1 (H1), invites us to expect and theorize that: Note: CV: Control variables. The control variables are not included simultaneously in the same model. This graphical representation summarizes the control variables included in the base empirical analysis, the test of robustness, and the post-hoc empirical analyses. H1: There is a positive relationship between remote work firms' initiatives in t1 and remote work firms' initiatives in t2, which will ultimately provide a competitive advantage to leaders over agile companies. H2: There is a positive relationship between remote work firms' initiatives in t2 and remote work firms' initiatives in t3, which will ultimately provide a competitive advantage to leaders and agile companies over survival companies."
303,"The constructs of interest were measured using archival data. We measured remote work firm's initiatives through the natural logarithm of the number of remote work firm's initiatives mentions in news published about these initiatives per firm in t1, t2, and t3, with information collected from MyNews database (https://www.mynews.es/). This database includes news from more than 700 online and print editions of the national, regional, and international press. We performed a structured content analysis following the well-established protocol used in leading prior studies (e.g., [43,44,47]). Based on the review of relevant academic literature, managerial reports, and news on IT-enabled remote work, a preliminary list of keywords on remote work firms? initiatives was created. One of the authors discussed this list with three IT executives and four HR (business) executives. According to their recommendations and by carefully using their feedback, 20 keywords were selected and used for the corporate news coding protocol (Table 4). We considered these tools to be crucial to deploy remote work initiatives.4 Two of the authors conducted the news coding protocol. For each period, the two coders carefully read the news where the keyword appeared to check that the news referred to any remote work firm's initiative. We collected and read 2778 news in t1, 7819 news in t2, and 6353 news in t3 where the keywords appeared (Table 4). Each news contained different keywords referring to different initiatives on remote work. In that case, we computed as one each of these initiatives on remote work. After being read and coded, 111, 1137, and 847 remote work firm's initiatives mentions were found in t1, t2, and t3, respectively."
304,"We empirically tested the proposed research model running a partial least squares path modeling (PLS-PM), a variance-based structural equation modeling (SEM) technique [46,50] that is suitable to test the proposed model for two reasons. First, PLS is a full-fledged estimator that enables the empirical test of conceptual models in both confirmatory and explanatory IS research (as in this study) by using an overall evaluation of the fit of the saturated and estimated models [46]. Second, all the constructs of the proposed research model were conceptualized and operationalized as composite constructs, and this estimator is suitable to test composite models [48,51]. Moreover, PLS has been used extensively in the IS research area (e.g., [[52], [53], [54], [55], [56], [57]]). We used the statistical software package Advanced Analysis for Composites (ADANCO) 2.1. Professional (e.g., [58,59]) because it provides consistent estimates, and it has been designed for confirmatory research, as our study [60]. We ran a bootstrapping of 4999 subsamples. The proposed model was specified as a composite model, and all the constructs were estimated using mode B [46]. Table 5 presents the results of the empirical analysis. "
305,"Table A1 (in the appendix) presents the correlation matrix. We find support for H1 (beta = 0.278, p < 0.01) and H2 (beta = 0.203, p < 0.01) which indicates the following. 1. Companies that designed and executed a leader remote work strategy have continued executing these initiatives, which has provided leaders with a competitive advantage in remote work over their competitors. 2. Leaders and agile companies which have previously worked deliberately (leaders) or emergently (agile) on these remote work initiatives can yield leaders and agile a competitive advantage over survival companies that work on remote work initiatives through improvisation. The R2 values for remote work firm's initiatives in t1, t2, and t3 were 0.587, 0.570, and 0.650, respectively. The adjusted R2 values for the remote work firm's initiatives in t1, t2, and t3 constructs were 0.579, 0.556, and 0.639, respectively. The effect size (f2) values for the supported hypotheses ranged from 0.096 to 0.168, which indicates the medium size of these effects [61]. In addition, we compared the empirical correlation matrix with the model-implied correlation matrix of the estimated model to estimate three discrepancies between these two matrixes [62]: standardized root-mean-squared residual (SRMR), unweighted least squares (ULS) discrepancy (dULS), and the geodesic discrepancy (dG) [46]. SRMR should be lower than 0.080, and all the HI95 values should be greater than the values of the three discrepancies [46]. This analysis suggests that neither model should be rejected based on an alpha level of 0.05 since all discrepancies are below the 95%-quantile of the bootstrap discrepancies, which indicates that with a 5% probability, our theoretical hypotheses and the research proposed model are correct. Related to the control variables, the effects of firm size were significant on remote work firm's initiatives in t1 (beta = 0.112??) and t3 (beta = 0.112*). The size of these effects was similar (0.030 and 0.034, respectively). The influence of the firm's RSE in remote work was significant and with a large effect size in the three periods. The beta coefficients ranged from 0.649??? (t2) to 0.748??? (t1). The effect sizes were large, ranging from 0.994 (t3) to 1.342 (t1). The results show that the largest influence of the firm's RSE in remote work on the deployment of pioneer remote work initiatives was in t1, which is consistent with the leader remote work strategy."
306,"This task is challenging as the acceptance or rejection of upgrade offers relies on complex interactions among the offer, the customer, the booking, and the destination (as shown in Fig. 1). Our premise was based on the constructs as shown in Fig. 1, taken from discussions with experts on revenue, customer experience, and e-commerce within the company, using semi-structured interviews [12]. These dimensions provide a theoretical and practical paradigm for PREM [13,14] concerning the customer and the product. Prior work has also shown the influence of these three constructs in the airline industry [15], with each construct having subattributes such as departure date and price of the booking. Prior work has also shown that whether the customer is, for example, a leisure or business traveler affects the offer acceptance [16] and also the destination [16,17]. Customer perceptions of the airline service have also been noted as important for continued bookings [17]. While booking is often the only primary preference known about the customer [18], it can also involve subattributes such as the quality of the airline website [19]. As discussed later, our premise of the constructs of customer, booking, and destination can be seen to affect the decision to accept or reject the upgrade offer."
307,"Our collaboration airline company has more than 100 flights to more than 100 destinations on all major continents. The company serves more than 25 million customers annually, and our data covers a significant proportion of these customers over multiple years. The company uses a hub-and-spoke model (where flight routes are organized as a series of routes connected by a single hub airport), has international flights, competes on service and revenue, faces governance issues common in the industry, has a frequent flyer program, and is a member of a major airline alliance. As such, it represents a typical major airline company that offers bookings to multiple destinations to an international customer base. Fig. 2 illustrates the company's rule-based process for upselling offers via email marketing channels. In Fig. 2, the customer books a ticket in economy class. The company determines if the customer is eligible to receive an upgrade offer, which is sent via an email message containing the booking details and the upgrade offer price. Upon receiving the offer, the customer either accepts or ignores the proposed offer."
308," As we empirically show in the experiments, the multistage approach provides better results than a single-stage end-to-end approach using multiobjective optimization with a constraints-based approach. The overall PREM process and components are presented in Fig. 4. As an overview of Fig. 4, the three most critical problems are noise, sparsity, and imbalance. We tackle each of these problems through two components ? feature embeddings and cost-sensitive classification."
309,"The dataset used for PREM development consists of more than 64 million trip booking records of customers traveling between 2017 and 2019. The dataset contains information concerning the price of the upgrade offer, which customers were sent upgrade offers, and which customers did or did not accept the update offers. This data is valuable for analyzing customer upselling and price elasticity dynamics in a major and competitive industry, and findings have implications for other travel-related domains. More than 14 million customers received upgrade offers (with a reach rate of 22%), and over 194,000 customers accepted the upgrade (with a conversion rate of 1.43%), as shown in Table 1."
310,"We did evaluate other encoding approaches, including dimensionality reduction techniques such as PCA. As discussed below in Table 2 (Results of feature embedding analysis), our embedding algorithm outperforms other approaches. While we did not invent this embedding algorithm, we are the first, to our knowledge, to apply it to airline data to tackle noise and sparsity issues. This applied study draws from computer science to solve a problem with real-world impact."
311,"A key hyperparameter is embedding in size, which determines the vector outputted size by the autoencoder, representing the trip information as a fixed-length vector. We evaluate its impact in the next experiment. When using the autoencoder, one could set the size of the latent representation to an arbitrary number. Traditionally, this size is often set as a power of two. Table 3 shows the results of PREM for different embedding sizes. We can see that an embedding size of 256 represents a sweet spot. Reducing the size results in lesser accuracy and revenue capture, while increasing it could potentially result in overfitting the model."
312,"From Table 4, we can see that a DL model using embeddings provides the best results. However, one could also use other traditional classifiers, such as logistic regression, support vector machine, or random forest, and pay only a minor penalty in the F1 score and almost none in revenue capture. Using the original data without the embeddings shows a steep drop in the F1 score for both DL-based and non-DL-based methods. As shown in Table 4, we evaluated alternate approaches for handling imbalanced data [69], such as the Synthetic Minority Oversampling Technique (SMOTE), oversampling, and the use of generative models such as Generative Adversarial Networks (GANs). We used random forests as the downstream classifier. From Table 4, we can see that traditional approaches such as oversampling are outperformed by the GAN, where we generate synthetic data for the rare classes so that the training data is balanced. Our embedding-based approach outperforms each of these approaches."
313,"The personalized upgrade model uses a binary autoencoder to segment customer bookings into K segments so that similar customers have similar embeddings. Once the segmentation is obtained, PREM estimates the response rate for each upgrade offer bucket. We consider two other segmentation approaches that also result in K segments. The first is based on K-Means that cluster all bookings into K distinct clusters, with K = 7. Our other baseline is a decision tree that tries to partition bookings using the Gini criterion. As shown in Table 5, our approach gives the best results."
314,"Next, we evaluate the revenue maximizer component. Recalling that we use an integer programming approach to select the best customers and the corresponding offers, a natural alternative is to use a greedy baseline that works as follows. First, the greedy algorithm computes each customer's expected revenue for each offer. Then, the Revenue Maximizer picks the best among them. For example, if Customer A accepts an offer of $500 with a 0.5 probability and $1000 with a 0.2 probability, then the expected revenues are $250 and $200, respectively. So, the Revenue Maximizer identifies that the best offer for Customer A is $500, with expected revenue of $250. Suppose there is another customer, Customer B, whose expected revenue is $300, and the model needs to select one customer. Here, the greedy algorithm will pick Customer B. As shown in Table 6, the revenue maximization of PREM (ILP) outperforms the greedy algorithm baseline."
315,"We propose a novel multistage approach based on feature development, the offer acceptance model, a personalized upgrade model, and a revenue maximizer. It is worth investigating if one needs such a multistage model in the first place. Specifically, two questions are of interest: (a) What would have happened if PREM used a single-stage classifier? and (b) What would have happened if PREM skipped the offer cost classification component or the revenue maximizer? The results presented in Table 7 address these questions. As shown in Table 7, the proposed PREM approach containing separate and sequential stages provides the best results. If one squeezes all of these stages into a single stage (i.e., if one trains a single classifier that takes all the bookings of a flight as input and returns the list of users and upgrade offers as output), then that gives the worst result. Splitting tasks into the different stages of an ML system clearly improves the performance in this context. Instead of the classifier trying to handle multiple objectives, each classifier in the PREM approach is targeted and focused on a single task, resulting in superior overall performance. Table 7 also indicates that all three components contribute positively. If one skips the revenue maximizer, then the accuracy drops marginally, but there is a steeper drop in revenue capture. Similarly, if one skips the offer classification and runs the upgrade offer determination for every user on the flight, there is a steep drop in accuracy but a smaller drop in revenue capture."
316,Fig. 1 depicts the methodological approach followed.
317,"LMM requires data to be set up in the long format such that there were 17 rows per participant. We first examined the psychometric properties of the model by assessing the convergent and discriminant validity of conservatism and collectivism variables. As shown in Table 4, because of low factor loadings, CONS1, CONS7, and CONS8 were dropped from the conservative position scale, and COL5 was dropped from the collectivism scale. The items listed in Table 4 lead to distinct constructs that demonstrated excellent Cronbach's a (Hair Jr, Hult, Ringle, & Sarstedt, 2013). The conservatism scale had Cronbach's a = 0.851, and the collectivism scale had Cronbach's a = 0.849. Discriminant validity was further assessed by examining items? cross-loadings that were all smaller than their factor (of interest) loadings (Hair [113]). As such, the measurement model demonstrated sound psychometric properties. In addition, we gauged the degree of multicollinearity between items and constructs in our study by calculating variance inflation factors (VIFs). All VIF values were <3.3, indicating that multicollinearity was not a concern in this study. While the focus of this study is on individuals? conservative political beliefs and their espoused cultural beliefs, we also added the country variable to control for the participant's country of origin, which may play a part (given the data were collected in two different countries). However, the focus remains on individual-level beliefs ? both political and cultural."
318,"As shown in Table 5, both hypotheses were supported. With increasing degree of conservatism (ß = 0.65; p < 0.0001) and collectivism (ß = 0.21; p < 0.0001), individuals? fake news believability increased significantly, thereby providing support for both H1 and H2 and answering positively both associated research questions. Participants? gender (ß = -0.02) was found not significant. Age (ß = -0.004; P < .01) was significant such that with increasing age, the fake news believability decreased. Internet usage (ß = 0.03; P < 0.05) was also significant such that with increasing daily Internet usage, individuals? fake news believability increased. Country variable was found significant (ß = 0.36; P < 0 .001) such that American participants (mean = 3.12) in general were more likely than Indian participants (mean = 2.77) to lend credence to the fake news scenarios presented."
319,Figure 1 summarizes the hypotheses and relations discussed in the following sections.
320,Table 1 presents the descriptive statistics and Pearson correlation coefficients of all variables.
321,"Table 2 displays tests of the hypotheses. Models 1?3 and 4?6 use resource utilization (seat utilization) and product concentration (concentration) as the dependent variables, respectively. Models 1 and 4 are baseline models. They only include control variables. Models 2 and 5 include the independent variables for testing Hypotheses 1a and 1b. Models 3 and 6 include interaction terms between demand-side usage of O2O digital platforms and the extent of vertical and horizontal inter-firm relationships to test Hypotheses 2a, 2b, 3a, and 3b. All models tested are statistically significant. Wald's chi-square values are reported."
322,"Table A1 in the Appendix presents the results of robustness checks when the total number of all movie theaters is used as the measure of chain size and the new measure of concentration. The results indicate that the online ratio remains a strong predictor for operational decisions made by theaters. The study also finds that vertical relationship and chain size remain significant moderators. Hence, the results are robust to new formulations of critical variables. "
323,"Table A2 in the Appendix shows the results. Models 7 and 9 contain control variables, while Models 8 and 10 include independent variables. In Model 8, the results show that both seat utilization (ß = 0.592, p < 0.01) and concentration (ß = 0.191, p < 0.01) have a positive and significant impact on revenues. In Model 10, the results show that both seat utilization (ß = 0.418, p < 0.01) and concentration (ß = 0.131, p < 0.01) have a positive and significant impact on the audience."
324,Table A3 shows the profile of the respondents.
325,"Based on the above discussion, our theoretical framework is summarized in Fig. 1."
326,Table 1 summarizes the profiles of responding companies and respondents.
327,"All items in this paper were adapted from the tested scale and measured by a seven-point Likert scale, ranging from ?1? to ?7? (?1? = ?strongly disagree? and ?7 = ?strongly agree?)  as shown in Table 2. In particular  four items adapted from Bharadwaj et al. [6] were used to measure digitalization capabilities  and these items reflected the degree to which companies could access customer-related order-related production-related  and market-related data. """
328,The results in Table 3 thereby confirmed the effectiveness of discriminant validity.
329,"The estimated results based on the logic of stepwise regression are presented in Table 4. It can be seen that first, digitalization capabilities ( ) positively affect market capitalizing agility. Second, both digitalization capabilities ( ) and market capitalizing agility ( ) positively relate to operational adjustment agility. Third, without any mediators, digitalization capabilities ( ) pose a positive relationship with firm performance. Fourth, when considering the combined effect of digitalization capabilities, market capitalizing agility, and operational adjustment agility on firm performance, only market capitalizing agility ( ) and operational adjustment agility ( ) show significant coefficients, whereas the coefficient of digitalization capabilities is nonsignificant ( ). The above results thereby indicate that the full mediating role of market capitalizing agility and operational adjustment agility in influencing the relationship between digitalization capabilities and firm performance."
330,"Table 5 further presents the results based on the bootstrap method with 5000 samples and a 95% confidence interval (CI) [62]. In particular, when the 95% CI's upper and lower bounds do not include zero, such a path is statistically significant; in contrast, when the 95% CI's upper and lower bounds include zero, such a path is statistically nonsignificant. As Table 5 shows, the direct effect of digitalization capabilities on firm performance (direct effect = 0.052, SE = 0.078) is statistically nonsignificant; in contrast, the indirect effects of digitalization capabilities on firm performance through market capitalizing agility (indirect effect = 0.088, SE = 0.054), operational adjustment agility (indirect effect = 0.077, SE = 0.045), and the sequence of market capitalizing agility and operational adjustment agility (indirect effect = 0.046, SE = 0.023) all have significant coefficients. Hence, these results again support all of our hypotheses."
331,"Fig. 1 shows the implementation of the design (reported in Fig. C.1, Appendix C). The data flow (1) from the sensors (attached to the patient's body) to an app on a mobile device, (2) which sends the vital signs to the Zephyr/Medtronic cloud database, (3) which in turn are downloaded to our database, and (4) which is then uploaded to the green armband of the patient's avatar in the wepital. This way, the avatar embodies the patient's vital signs in real time."
332,"We conceptualized a theory-based model to assess the salient factors that contribute to the successful use of the wepital design from the perspective of patients, as shown in Fig. 4. The measure of success is patient satisfaction with the physician in the wepital. We continue to rely on the theory of affordances as the kernel theory in hypothesizing how design affordances impact satisfaction?the meta-requirement of the design."
333,"All but one fit indices for the estimated model were desirably above or below the corresponding threshold levels. The only exception was the standardized root mean square residual (SRMR), which was slightly above the threshold. This was due to the relatively small size of our sample. Asparauhov and Muthén [2] point out that for small samples, the SRMR above 0.08 commonly occurs and should not be a cause for concern.15 Fig. 5 reports the estimated model."
334,"As shown in Fig. 7, hospitals can extend their physical and staff capacity by adding patients? rooms at home (mypitals), virtual human supervisors in wepitals, and medical staff in docpitals. In times of extreme need, hospitals can immediately increase capacity by adding more rooms to their wepitals and staffing their wepitals with additional intelligent robots, virtual human providers, and medical staff in docpitals. Thus, hospitals will have the capacity and capability to better handle pandemics on short notice. More importantly, they will have the capability to extend their resources to remote parts of the country or even the globe."
335,"Fifth, in an advanced application, states can create an infrastructure that facilitates such movements. Each state or city (depending on the number of its hospitals and its population) can create a hub, called a wepital hub, on which its hospitals set up their wepitals. Each hub is supervised for security and compliance at the state level. The collection of wepital hubs creates the US Medical Care Internet, which should be regulated at the federal level for security and compliance (Fig. 8). State and federal governments can provide medical services to more people and handle medical crises with more options and resources. They can disseminate health information and educational materials in wepitals when people have the time and motivation to receive such information and education."
336,The Meta Design of Real Avatar in Wepital
337,"We checked the reliability of the constructs in three ways, as reported in Table I.1. We computed Cronbach alpha values, which were all above the 0.70 acceptable threshold [83]. Composite factor reliability (CFR) values were above the 0.70 acceptable threshold [104]. Average variance extracted (AVE) values were above the acceptable threshold 0.50 [104]. As an additional check on discriminant validity, we computed the square root of AVE for each construct and compared it with the construct's correlations with other constructs [37]. "
338,"For each construct, the square root of AVE was desirably greater than the correlation values (Table I.2). These checks provided support for the reliability and validity of the measured constructs."
339,"In our context, users need to actively apply and await approval to be administrators, i.e., administrator-users voluntarily take the administrative role. Hence, taking this role could reflect the process of social role taking, and therefore encourage the display of role-consistent behaviors (public space contribution, private space contribution, and OL). Fig. 1 illustrates our contextualized theoretical model."
340,"We employed LISREL v.8.80 software to implement structural equation modeling (SEM), which is a recommended method [144]. Our structural model acceptably fit the data, i.e., CFI=.95, IFI=.95, NFI=.94, RMSEA=.094, and SRMR=.039. The reasons that justify the RMSEA value were previously stated for the CFA results, and therefore are not repeated here. Notably, we permitted the LISREL software to freely estimate correlations between a few behavioral indicators. For example, the in-community level is partly determined by the number of posted articles in public areas so they should be related. This action is reasonable as they are reflective items of the same concept. The analytical results are shown in Fig. 2."
341,The participants had an average duration in the community of 8.70 years (standard deviation=5.33 years). Table 1 lists the demographic information of our participants.
342,"Consistent with the literature [1,117,[132], [133], [134]], we conducted an exploratory factor analysis (EFA) to test data validity. Table 3 lists our EFA results that do not show marked cross-loadings, that is, the EFA results offer preliminary support for our data validity. All factor loadings are larger than .82."
343,"Table 4 reports the correlations among our study concepts. These ranged from .09 and .59. Moreover, the square roots of the AVE values (on the diagonal of Table 4) exceeded the associated correlations, indicating sufficient discriminant validity [118,141]."
344,"All the hypotheses were supported. Community commitment is positively related to taking an administrator role (H1), which is further positively related to public space contribution, private space contribution, and OL (H2, H3, and H4). Taking an administrator role has a stronger impact on public space contributions than on private space contributions (H5) and on OL (H6). Table 5 lists the testing results of H5 and H6. Specifically, we followed Ray et al. [46] and therefore used the t tests to compare the influences of taking an administrator role on three positive community behaviors. More details can be found in Ray et al. [46]. Both H5 and H6 were supported."
345,"From an information value chain perspective, the specific impacts of quality data and IT-enabled sensing capability are expected to differ depending on their position in the chain, losing strength as they move to more distant elements (see Fig. 1). Proposing each data analytic resource?assets (i.e., quality data) and capabilities (i.e., IT-enabled sensing)?as having different effects (i.e., direct, partial, or fully mediated) on outcomes at the marketing unit and at the firm serves to add precision to our overall understanding of the value creation potential of data analytics."
346,"To test the significance and strength of the proposed relations, we used a one-tailed 5000 subsample BCA bootstrap [137]. Fig. 2 and Table 5 show the results obtained. As can be observed, quality data shows a very strong and positive relationship with IT-enabled data analytics sensing (ß = 0.689). Thus, hypothesis H1 is supported. Moreover, both quality data (ß = 0.340) and IT-enabled data analytics sensing (ß = 0.317) are positively and significantly related to marketing innovation. As the indirect effect of quality data on marketing innovation via IT-enabled data analytics sensing is positive and significant (ß1 × ß2 = 0.219), partial mediation applies. Hence, hypotheses H2a and H2b are supported."
347,"Thus, taking the initial sample size as a reference (i.e., 342 companies), the stratified sampling procedure ensured that different proportions of company types according to size (mid-sized vs. large-sized), industry (manufacturing vs. service), and technology intensity (high-techs vs. low-techs, as established by the OECD and Eurostat) were preserved as they exist in the population, thereby improving the precision and representativeness of the resulting sample. The final sample included 346 companies? 4 over the threshold of 342?that answered the provided questionnaire. Table 1 provides more details about the composition of the sample."
348,"As data was collected through a single method (i.e., survey), this presented the possibility of the occurrence of what is known as common-method bias [[115], [116]]. To determine the extent of the method variance in the dataset, we used the marker variable approach [117]. To that end, we included a two-item scale regarding competition intensity,2 based on Jaworski and Kohli [118]. Subsequent correlation analysis revealed that correlations between the marker variable and independent, mediating, and dependent variables were very low, the largest one being 0.191. Thus, it could be concluded that common method variance was not a likely problem in our dataset. Also, a full collinearity test specially conceived for PLS-SEM [119] was carried out. The above test includes both vertical (predictor?predictor) and lateral (predictor?criterion) collinearity analyses. According to Kock [119], if all the variance inflation factors (VIFs) resulting from a full collinearity test are equal to or lower than 3.3, the model can be considered free of common-method bias. The highest VIF in our model (see Table 2) was 2.023, well below the 3.3 threshold. Therefore, this provides further evidence for ruling out the potential for common-method bias."
349,"Once the quality of the measurement model was guaranteed and before evaluating the structural model, a collinearity test was carried out. This collinearity test was performed to rule out any potential bias in path coefficients due to critical levels of collinearity among the predictor constructs [137]. Analogous to the assessment of composite measurement models, VIF values should be lower than 3. Table 4 shows the results obtained. As can be observed, all VIFs are well below the established threshold, the highest one being 2.121. Therefore, collinearity in the structural model is not a problem in this research."
350,"To test the significance and strength of the proposed relations, we used a one-tailed 5000 subsample BCA bootstrap [137]. Fig. 2 and Table 5 show the results obtained. As can be observed, quality data shows a very strong and positive relationship with IT-enabled data analytics sensing (ß = 0.689). Thus, hypothesis H1 is supported. Moreover, both quality data (ß = 0.340) and IT-enabled data analytics sensing (ß = 0.317) are positively and significantly related to marketing innovation. As the indirect effect of quality data on marketing innovation via IT-enabled data analytics sensing is positive and significant (ß1 × ß2 = 0.219), partial mediation applies. Hence, hypotheses H2a and H2b are supported."
351,"The coefficient of determination (R2 value) of the mediating and dependent variables was also examined, which represents a measure of in-sample predictive power that also indicates explanatory power [122, 137]. As can be observed in Table 6, the amount of variance explained for IT-enabled data analytics sensing reached 47.4%, while for marketing innovation, it scored 38.3% and for market performance 34.1%. Moreover, changes in R2 when a specified exogenous construct is omitted from the model were analyzed by means of the so-called ??2 effect size? [137]. According to Hair et al. [137], for a construct to be relevant when explaining another variable, its effect size should reach the minimum threshold of 0.02. This was the case for both the independent and mediating variables, except for quality data vis-à-vis market performance."
352,"Fig. 1 provides an overview of the four types of approaches, the sensemaking value ascribed to the technology, and the associated innovation phase reflecting actions (i.e., whether actions are already taken toward adoption or not). The lightning bolts indicate discrepancies between sensemaking results and blockchain adoption activities. For the sake of simplicity, we will speak of types of blockchain approaches in the following and explain them without explicitly mentioning warrants again. The warrants are already textually merged with the description of four types and the concrete approaches of the organizations in the pre-adoption phase."
353,"Fig. 2 summarizes the results of our analyses. For the sake of clarity, the characteristics of approaches to blockchain are depicted as a spectrum, e.g., the assessment of blockchain's future value ranges between ?limited value? up to ?future value? of the technology, assuming that assessments of contingent and current value lie in between. Considering the differences between the four types of approaches in terms of how they evaluate the business value of blockchain and their sentiments and progress in the innovation process, more differentiated recommendations are needed that account for discrepancies between actions taken toward the technology and how organizations seek to make sense of blockchain."
354,"This data collection approach generated a diverse pool of key informants with diverse professional backgrounds and at various stages of their career. Descriptive characteristics of the key informants as well as the identification of key informants (abbreviated with the letter ?I? for informant and numbered consecutively) are summarized in Table 1. Each of the target organizations tasked a relatively small team of people with blockchain knowledge to (potentially) develop and test a prototype and, thus, interviewed key informants are assumed to be representative of their organization given the very small team size. "
355, Table 2 below provides an overview of our primary and secondary data.
356,"H1. The WHO's announcement of COVID-19 as a ?pandemic? generates a significant loss of valuation for SCF firms. H2. The WHO's announcement of COVID-19 as a ?pandemic? generates a lower valuation loss for blockchain-enabled SCF firms compared to other SCF firms. H3. Due to the WHO's announcement of COVID-19 as a ?pandemic,? a lower percentage of blockchain-enabled SCF firms incur valuation loss compared to other SCF firms. H4. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly higher trading volume for SCF firms. H5. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly lower trading volume for blockchain-enabled SCF firms than that for other SCF firms. H6. The WHO's announcement of COVID-19 as a ?pandemic? generates significantly different valuation loss for banking and non-banking SCF firms that are blockchain-enabled and for those that are not. H7a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more R&D intensive than other SCF firms. H7b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more capital intensive than other SCF firms. H7c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for SCF firms that adopt blockchain and are more labor intensive than other SCF firms. H8a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more R&D intensive than other blockchain-enabled SCF firms. H8b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more capital intensive than other blockchain-enabled SCF firms. H8c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are part of any consortium and are more labor intensive than other blockchain-enabled SCF firms. H9a. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more R&D intensive than other blockchain-enabled SCF firms. H9b. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more capital intensive than other blockchain-enabled SCF firms. H9c. The WHO's announcement of COVID-19 as a ?pandemic? will lead to a lower valuation loss for blockchain-enabled SCF firms that are at the implementation stage and are more labor intensive than other blockchain-enabled SCF firms. We summarize our hypotheses in the form of a conceptual framework and present it in Fig. 2."
357," Table 4 lists the pairwise correlation among all variables, variance inflation factor (VIF) for independent variables, and relevant descriptive statistics. A positive and significant correlation exists between the two main explanatory variables: Capex and Staffex. Among the control variables, CP and Size are highly associated with other variables. VIF scores for the independent variables indicate that the regression models used in the study are not affected by multi-collinearity. Additional daily data related to FF3F and C4F are obtained from Kenneth French's data library. To ensure that the impact of the WHO's announcement is free from any bias induced by extreme observations, we also compute the CAAR of the outlier-adjusted sample firms. The outlier-adjusted sample comprises all sample firms except those that fall in the top 10% or bottom 10% in terms of the CAR generated from day -30 to day 50."
358,"Table 5 presents some descriptive statistics related to the sample firms for the pre-event and the post-event windows (including the event day). The pre-event period denotes the time interval from 30 days to 1 day prior to the event, and the post-event period includes the time period from the event date to 50 days after it. From Table 5, it is evident that the average daily unadjusted dollar-denominated price reduces from the pre-event ($34.46) to the post-event period ($27.53). In the pre-event period, the average daily return percentage (annualized) is recorded as a minor loss (-0.62%). However, the return decreases even further in the post-event period (-2.03%). The volatility of the return increases in the post-event period (6.32%) compared to the pre-event period (3.06%). The logarithm of the average daily volume of trades also increases after the event. Also, the average daily market capitalization reduces slightly from the pre- to the post-event period. However, there is no change recorded for average shares outstanding between the two-time windows."
359,"To investigate the effect of the WHO's announcement on SCF firms, we determine the CAAR as per Eq. (2) and present our findings in Table 6. We estimate the CAAR for two sets of firms (i.e., all sample firms and outlier-adjusted sample firms) over different event windows. From Table 6, we observe that our sample firms incur a significant loss in valuation close to the event date. According to the MM, MMEGE, FF3F, and C4F models, the firms on an average earn -2.8%, -2.1%, -3.2%, and -2.8%, respectively, around the event window [-1, ]. These significant valuation losses are consistently observed throughout the entire event window. In the longest window of our study, (i.e., [-30, ]), sample firms experience a negative and significant CAAR of -19.3%, -23.8%, -28.0%, and -22.3% as per the MM, MMEGE, FF3F, and C4F models, respectively. Therefore, it seems that there is a permanent valuation loss for all SCF firms due to the announcement. This negative impact is not immediate as we do not find a significant drop in the [0, ] event window. However, the loss is quite prominent in the [0, ] window. This significant loss can also be observed in the [-15, ] event window, as estimated by the C4F model. Fig. 3 provides a visual depiction of the valuation loss of firms. The valuation loss is persistent even for a long post-event time period (i.e., [, ]). In contrast, none of the models depict any significant valuation loss for the sample firms in the pre-event time period (i.e., [-30, -1])."
360,"We perform a sub-sample analysis and explore the impact of the WHO's announcement on BlockFirms and Non-BlockFirms, separately. We compute the CAAR for these two groups of firms using the same MM, MMEGE, FF3F, and C4F models across all event windows and report the results in Table 7. Interestingly, there is not a single event window where BlockFims incur a significant loss of valuation. On the contrary, Non-BlockFirms exhibit a significant valuation loss across all event windows. It seems that valuation loss encountered by all SCF firms in the sample is predominantly driven by abnormal losses for Non-BlockFirms. BlockFirms, on the other hand, show enough investor confidence during the turbulent time period."
361,"This negative and significant CAAR for Non-BlockFirms can be potentially caused by either the higher magnitude of losses of a few sample firms or a significantly large number of sample firms that move into the loss-making domain due to the announcement. We explore the same using a binomial sign test. This test measures whether the percentage of firms earning negative returns on a particular day in a specific time window is significantly different from 50% or not. We compute the AAR earned by BlockFirms and Non-BlockFirms and the percentage of firms from each group earning losses on a particular day in the [-15, ] event window. We show the results obtained from the MM and C4F models in Table 8. The columns ?Mean (%)? and ?Negative (%)? represent the AARs and the percentage of loss-earning firms on each day, respectively. It yields two important insights. First, the C4F model shows a lower impact of the announcement than the MM model. The C4F model includes traditional asset pricing factors that play an important role in explaining the AAR. Second, there are few days, i.e., Days 1, 9, and 12, when a higher number of firms among BlockFirms earn negative returns. In contrast, Non-BlockFirms earn losses on 7 out of 15 days in the post-event period. Even Non-BlockFirms start experiencing valuation loss from one day before the event day, probably due to some information leakage or anticipation of panic. These findings support Hypothesis 3."
362,"To determine the impact of the WHO's announcement on the trading volume, we compute the CAAV around the event date using Eq. (5). Table 9 presents the estimated CAAV for all sample SCF firms as well as for the sub-samples BlockFirms and Non-BlockFirms across different event windows. From Table 9, it is evident that all sample SCF firms generate abnormally high trading volume around the event day and in the post-event period. The significant increase in trading volume is evident immediately after the event. In the event window [0, ], sample firms on the whole experience a CAAV of 1.335, and the CAAV increases up to 12.454 surrounding the event window (i.e., [-30, ]). This insight is consistent with the low return and high volume relationship in the bear market reported by Chen [77]. Thus, it statistically supports Hypothesis 4 of this study. However, such a significant increase in abnormal trading volume is guided by Non-BlockFirms. Non-BlockFirms experience a much higher trading volume compared to BlockFirms. While a significant increase in the trading volume for BlockFirms is observed only within the first two days of the event day, it is consistently higher in case of Non-BlockFirms for most of the event windows. This finding supports Hypothesis 5."
363,"To explore whether the firm characteristics in our sample of SCF firms play an important role in our findings, we perform a sub-sample analysis. More specifically, we divide both BlockFirms and Non-BlockFirms into two groups: banking and non-banking. We compute the ARs earned by these two groups of firms in different event windows using the Carhart 4-factor model (C4F). The outcomes of the analysis are documented in Table 10."
364,"To identify the predictive factors explaining the valuation loss for SCF firms, we perform a cross-sectional regression analysis, following the description in Section 4.3. The CAR estimated by the C4F model for the [-15, ] time window is used as the dependent variable in the regression. The results for models 1 to 4 (as specified in Section 4.3) are reported in Table 11. In model 1, Block_Dummy is found to be positive and significant. This indicates that Non-BlockFirms earn higher valuation loss compared to BlockFirms. Models 2 and 3 in Table 11 show that the impact of the interaction between Block_Dummy and R&D on CAR is significant. It is also observed that the interaction of Block_Dummy and Capex has a positive and significant relationship with CAR. However, there is no significant relationship between CAR and the interaction term Block_Dummy*Staffex. Therefore, we infer that BlockFirms that make a higher investment in R&D and capital expenditure suffer significantly less valuation loss due to the event. Thus, we find support for Hypothesis 7a and 7b, but not for Hypothesis 7c. The control variables show a consistent association with CAR across all four regression models."
365,"So far, we find that the adoption of blockchain enables SCF firms (i.e., BlockFirms) to protect against the erosion of firm value during the pandemic. Moreover, R&D and capital expenditure play an important role in this regard. Next, we aim to identify specific predictive factors that guide BlockFirms to protect the market value. We again run a set of cross-sectional regressions where we use CAR of BlockFirms estimated by the C4F model for the [-15, ] time window as the dependent variable. More specifically, we run models 5 to 10 (as specified in Section 4.3) to explain the impact of consortium and implementation dummies on the CAR of BlockFirms and report the results in Table 12. Results of models 5 and 6 reveal that the interaction terms Consortium*R&D and Implementation*R&D are positive and significant. While interacting capital expenditure (Capex) of BlockFirms with consortium and implementation dummies, we find a similar positive and significant association as depicted in the results of models 7 and 8. However, no significant association is observed in case of the interactions between staff expenditure (Staffex) and consortium and implementation dummies in models 9 and 10."
366," Fractions of top ratings in all categories are provided in Table 1 in Section 4. One can only speculate why such good ratings are usually given. AirBnB does not provide monetary incentives for reviews, which have been shown to lead to more positive reviews [114]. Possible explanations could be an ex post rationalization of the decision made by the guest (?I have chosen this accommodation, so it must be good?), or reciprocity between guests and hosts [30], [80]. On AirBnB, guests are also rated by hosts and might hope for a reciprocal favorable evaluation of themselves if they evaluated the host and the accommodation very positively."
367,"Table 2 presents the results of this analysis. The F values and their significance refer to the comparison to the previous model. Although adding both types of information to the baseline model leads to a significant improvement of fit, the effect of premises information is much stronger. Adding premises-specific variables to the baseline model increases the by almost 0.08, whereas adding the host information increases it by less than 0.02, and the F-test also clearly indicates a much stronger increase in model fit due to the premises information. Consequently, adding the host information to the model already containing the premises information leads only to a marginal improvement. Thus premises information has a considerably stronger influence on price than host information. This result is in accordance with other results from the literature. Although they did not specifically test for differences between premises-specific and host-specific information,Chen and Xie [24] also used nested regression models in which they first added premises-specific and then host-specific variables. In their model, adding host-specific variables increased the from 0.538 to 0.673. "
368,"Table 3 shows the results of this analysis. The results confirm those of the analysis of all premises types. Again, the effect of adding the premises-specific characteristics is considerably stronger than the effect of adding host characteristics."
369,The following tables provide an overview of descriptive statistics of the data set used. Table 4 summarizes data according to country.
370,Table 5 according to the type of premises.
371,Table 6 gives an overview of host characteristics.
372,Table 7 provides the regression coefficients for the three models using all types of premises.
373,Table 8 contains the regression coefficients of the full model (including both premises-specific and host-specific information) for six selected types of premises.
